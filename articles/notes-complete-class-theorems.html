<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- facebook sharing preview -->
  <meta property="og:url" content="http://zhat.io" />
  <meta property="og:image" content="http://zhat.io/assets/img/zhat.svg">

  <title>Notes: Complete Class Theorems</title>
  <meta name="description" content="">

  <!-- Google Fonts loaded here -->
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro|Open+Sans' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
      MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true,
        preview: "TeX"
      },
      "HTML-CSS": {
        fonts: ["TeX"],
        styles: {
          ".MathJax_Display": {"font-size": "125%"},
        }
      },
      // https://github.com/mathjax/MathJax/issues/1081#issuecomment-399878942
      TeX: {Augment: {
        Definitions: {macros: {xfrac: 'XFrac'}},
        Parse: {prototype: {
          XFrac: function (name) {
            var num = this.ParseArg(name);
            var den = this.ParseArg(name);
            this.Push(MathJax.ElementJax.mml.mfrac(num,den).With({bevelled: true}));
          }
        }}
      }}
    });
    </script>
  

  
    <script type="text/javascript">
      function toggle_adv() {
        var state = localStorage.getItem("advanced");
        if(state === null || state == "no") {
          localStorage.setItem("advanced", "yes");
        } else {
          localStorage.setItem("advanced", "no");
        }
      }

      function refresh_adv() {
        var state = localStorage.getItem("advanced");
        var elements = document.getElementsByClassName("advanced");
        if(state === null || state == "no") {
          for (let e of elements) e.classList.add("hidden");
        } else {
          for (let e of elements) e.classList.remove("hidden");
        }
      }

      // https://stackoverflow.com/a/24070373
      document.addEventListener("DOMContentLoaded", function() {
        // https://stackoverflow.com/a/31525463
        for (let e of document.querySelectorAll('.advanced.inner,.advanced-button')) {
          e.onclick = function() {
            toggle_adv();
            refresh_adv();
          }
        };

        for (let a of document.querySelectorAll('.advanced a')) {
          // https://stackoverflow.com/a/14526317
          a.onclick=function(e){ e.stopPropagation(); };
        }

        refresh_adv();
      });
    </script>
  

  <link rel="stylesheet" type="text/css" href="/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/css/print.css" media="print"> -->

  <link rel="canonical" href="pragmanym.github.io/zhat/articles/notes-complete-class-theorems">

  <link rel="alternate" type="application/rss+xml" title="Z-Hat" href="pragmanym.github.io/zhat/feed.xml" />

  <link rel="shortcut icon" type="image/png" href="/assets/img/favicon-32x32.png">
</head>

  <body class="full-width">
    <!--- Header and nav template site-wide -->
<header>
  <nav class="group">
  	<a href="/"><img class="badge" src="/assets/img/zhat.svg" alt="\hat{z}" onerror="this.onerror=null;this.src='/assets/img/zhat-large.png';"></a>
		
			
  	
			
  	
			
  	
			
		    
		      <a href="/css/print.css"></a>
		    
	    
  	
			
  	
	</nav>
</header>
    <article>
      <h1>Notes: Complete Class Theorems</h1>
<p class="subtitle">June 11, 2020</p>
<p>[<i><b>Disclaimer:</b>  I am not an expert on this topic, and I have likely just learned about this material for the first time. I don't guarantee the accuracy or correctness of anything. Consider this to be a page in my personal notebook. It is subject to change and revision over time. Equations in images are screen shots taken from study sources.</i>]</p>

<!--more-->

<ul class="toc" id="markdown-toc">
  <li><a href="#results-to-understand-in-hoff" id="markdown-toc-results-to-understand-in-hoff">Results to understand in Hoff</a></li>
  <li>
<a href="#notes" id="markdown-toc-notes">Notes</a>    <ul>
      <li><a href="#complete-class-theorem-i" id="markdown-toc-complete-class-theorem-i">Complete class theorem I</a></li>
      <li><a href="#complete-class-theorem-ii" id="markdown-toc-complete-class-theorem-ii">Complete class theorem II</a></li>
      <li><a href="#euclidean-parameter-spaces" id="markdown-toc-euclidean-parameter-spaces">Euclidean parameter spaces</a></li>
      <li><a href="#complete-class-theorem-iii" id="markdown-toc-complete-class-theorem-iii">Complete class theorem III</a></li>
    </ul>
  </li>
  <li>
<a href="#interpretation-and-implications" id="markdown-toc-interpretation-and-implications">Interpretation and implications</a>    <ul>
      <li><a href="#discussion" id="markdown-toc-discussion">Discussion</a></li>
    </ul>
  </li>
</ul>

<p><strong>Objective:</strong> I want to understand the complete class theorems because they are a common argument for Bayesian epistemology, a theory of knowledge that puts forward Bayesian posterior calculation as all you need. In order to properly evaluate whether “being Bayesian” is enough of a theoretical framework to build and explain intelligence, I need to understand arguments for Bayesian epistemology.</p>

<p>The argument boils down to:</p>
<blockquote>
  <p>If you agree with expected utility as your objective, then you have to be Bayesian.</p>
</blockquote>

<p>In a nutshell: An strategy is inadmissible if there exists another strategy that is as good in all situations and strictly better in at least one. If you want your strategy to be admissible, it should be equivalent to a Bayes estimator.</p>

<p>Complete class theorems: Only Bayes strategies are admissible, and admissible strategies are Bayes.</p>

<p>I’m mainly following <a href="https://www.stat.washington.edu/people/pdhoff/courses/581/LectureNotes/admiss.pdf" target="_blank" rel="noopener noreferrer">Admissibility and complete classes - Peter Hoff</a>.</p>

<p>Related study notes: <a href="https://docs.google.com/document/d/1fCseo1fsPwJfjnehauAzOr4bf1GHHRfRW6cHwNQTNu4/edit" target="_blank" rel="noopener noreferrer">Wald’s Complete Class Theorem(s) - study notes</a></p>

<h1 id="results-to-understand-in-hoff">
<a class="header-anchor" href="#results-to-understand-in-hoff">Results to understand in </a><a href="https://www.stat.washington.edu/people/pdhoff/courses/581/LectureNotes/admiss.pdf" target="_blank" rel="noopener noreferrer">Hoff</a>
</h1>

<p><strong>Section 1</strong>:<br>
<img src="https://i.imgur.com/KSZ6PVb.png" alt=""></p>

<p><strong>Section 2</strong>:<br>
<img src="https://i.imgur.com/F94ljVs.png" alt=""><br>
<img src="https://i.imgur.com/2QW8pcP.png" alt=""></p>

<p><strong>Section 3</strong>:<br>
<img src="https://i.imgur.com/U2npCDa.png" alt=""></p>

<p><strong>Section 4</strong>:<br>
<img src="https://i.imgur.com/XPqhZ4E.png" alt=""><br>
<img src="https://i.imgur.com/H8uda4H.png" alt=""><br>
<img src="https://i.imgur.com/fAvOCcu.png" alt=""></p>

<p><strong>Section 5</strong> covers similar results for infinite parameter spaces (so far results are for finite parameter spaces).</p>

<p><strong>Section 6</strong>:<br>
<img src="https://i.imgur.com/B9EDHSE.png" alt=""></p>

<p><img src="https://i.imgur.com/TQjLvpT.png" alt=""><br>
<img src="https://i.imgur.com/RRj8Mwi.png" alt=""><br>
<img src="https://i.imgur.com/XVFp6DY.png" alt=""></p>

<h1 id="notes"><a class="header-anchor" href="#notes">Notes</a></h1>

<script type="math/tex; mode=display">\newcommand{\bb}{\mathbb}
\newcommand{\mc}{\mathcal}
\newcommand{\d}{\delta}
\newcommand{\p}{\pi}
\newcommand{\t}{\theta}
\newcommand{\T}{\Theta}
\newcommand{\fa}{\forall}
\newcommand{\ex}{\exists}
\newcommand{\real}{\bb{R}}
\newcommand{\E}{\bb{E}}
\renewcommand{\D}[1]{\operatorname{d}\!{#1}}
\DeclareMathOperator*{\argmin}{argmin}</script>

<p>Let $(\mc{X}, \mc{A}, P_\t)$ be a probability space for all $\t \in \T$.<br>
$\mc{X}$ is the sample space.<br>
$\T$ is the parameter space.<br>
$\mc{P} = \{P_\t : \t \in \T\}$ is the <em>model</em>, i.e. the set of all probability measures specified by the parameter space.</p>

<p>We wish to estimate some unknown $g(\t)$ which depends in a known way on $\t$. The text does not tell us what type $g(\t)$ is, and it does not matter for the discussion since it will always be hidden behind our loss function. The text uses $g(\T)$ (the image of $g$) to denote the space of all such $g$, but I find it less confusing and more direct to use $G = g(\T)$.</p>

<p>A <strong>loss function</strong> is a function $L : \T \times G \to \real^+$ which is always 0 for equivalent inputs, i.e.<br>
<script type="math/tex">L(\t, g(\t)) = 0,\ \fa \t \in \T\,.</script><br>
Note that $L(\t_1, g(\t_2))$ may be 0 when $\t_1 \neq \t_2$.</p>

<p>A <strong>non-randomized estimator</strong> for $g(\t)$ is a function $\d : \mc{X} \to G$ s.t. $x \mapsto L(\t, \d(x))$ is a measurable function (of $x$) for all $\t \in \T$. A <a href="https://en.wikipedia.org/wiki/Measurable_function" target="_blank" rel="noopener noreferrer">function is measurable</a> if the preimage of any measurable set is measurable, i.e. it preserves measurability. Concretely in this case, $\{x : L(\t, \d(x)) \in B\} \in \mc{A}$ for all $B \in \mc{B}(\real)$, where $\mc{A}$ is our event space (set of all subsets of $\mc{X}$ which can be measured by $P_\t$), and $\mc{B}(\real)$ is the <a href="https://mathworld.wolfram.com/BorelSet.html" target="_blank" rel="noopener noreferrer">Borel $\sigma$-algebra</a> over the reals, which is a standard definition of measurable sets of reals (unions and intersections of closed and open intervals are measurable). Presumably $\d$ is non-randomized because it only depends on the ground truth $x$.</p>

<p>The <strong>risk function</strong> of estimator $\d$ is the expected loss:<br>
<script type="math/tex">R(\t, \d) = \E_{x \sim X}\left[L(\t, \d(x)) \mid \t\right] = \int_\mc{X} L(\t, \d(x))P_\t(x) \D{x}</script></p>

<p>A <strong>randomized estimator</strong> is a function $\d : \mc{X} \times [0, 1] \to G$ s.t. $(x, u) \mapsto L(\t, \d(x, u))$ is a measurable function (of $x$ and $u$) for all $\t \in \T$. Just like a non-randomized estimator, except it recieves noise from $U \sim \mathrm{uniform}([0, 1])$ as input. Non-randomized estimators are a special case (ignores the random input). Conversely, a randomized estimator can be viewed as a distribution over non-randomized estimators (which are parametrized by $u \in [0, 1]$).</p>

<p>The risk function then integrates over $u$:<br>
<script type="math/tex">R(\t, \d) = \E_{x \sim X, u \sim U}\left[L(\t, \d(x, u)) \mid \t\right] = \int_0^1 \int_\mc{X} L(\t, \d(x, u))P_\t(x) \D{x} \D{u}</script></p>

<p>An estimator $\d_1$ <strong>dominates</strong> another estimator $\d_2$ iff<br>
\begin{align}<br>
\fa \t \in \T,\ R(\t, \d_1) \leq R(\t, \d_2)\,, \<br>
\ex \t \in \T,\ R(\t, \d_1) &lt; R(\t, \d_2)\,.<br>
\end{align}<br>
$\d_1$ must be at least as good (same risk or less) as $\d_2$ in every situation, and must be strictly better (less risk) in at least one situation, for the descriptor <em>dominance</em> to apply.</p>

<p>An estimator $\d$ is <strong>admissible</strong> if it is not dominated by any estimator.<br>
Admissibility does not mean an estimator is any good, however, but any inadmissible estimator can be automatically ruled out.</p>

<p>Let $\mc{D}$ be the set of all randomized estimators.<br>
A <strong>class</strong> (subset) of estimators $\mc{C} \subset \mc{D}$ is <strong>complete</strong> iff $\fa \d’ \in \mc{C}^c,\ \ex \d \in \mc{C}$ that dominates $\d’$.<br>
Here $(\cdot)^c$ is the compliment operator, i.e. $\mc{C}^c = \{\d’ \in \mc{D} : \d’ \notin \mc{C}\}$.</p>

<p>Let $\p$ be a probability measure on $\T$ and $\d$ be an estimator (from here on it does not matter if $\d$ is randomized or not because the risk does not depend on the arguments of $\d$).</p>

<p>The <strong>Bayes risk</strong> of $\d$ w.r.t. $\p$ is</p>

<script type="math/tex; mode=display">R(\p, \d) = \E_{\p(\t)}[R(\t, \d)] = \int_\T R(\t, \d) \p(\t) \D{\t}\,.</script>

<p>This is the expected risk w.r.t. $\p(\t)$, which is called our <strong>prior</strong>.</p>

<p>Bayes risk allows us to compare estimators by comparing numbers rather than functions, but now we have a new problem, which is that we have to choose a prior.</p>

<p>$\d$ is a <strong>Bayes estimator</strong> w.r.t. $\p$ iff</p>

<script type="math/tex; mode=display">R(\p, \d) \leq R(\p, \d'),\ \fa \d' \in \mc{D}\,.</script>

<p>Note that a Bayes estimator $\t$ can be dominated if $\pi$ assigns measure 0 to some subsets of $\T$. It is easy to show that if $\t$ is dominated by $\t’$, then $\t’$ is also Bayes and $R(\p, \d) = R(\p, \d’)$.</p>

<p><strong>Theorem 1</strong> (Bayes $\implies$ admissible): If prior $\pi(\theta)$ has exactly one Bayes estimator, then that estimator is admissible.</p>

<blockquote>
  <p>Thus the only thing that can dominate a Bayes estimator is another Bayes estimator. If there is only one Bayes estimator for a given prior, then it must be admissible.</p>
</blockquote>

<p><strong>Question:</strong> Under what conditions is there more than one Bayes estimator for a given prior?</p>

<p><strong>Theorem 3</strong> (Bayes $\implies$ admissible):<br>
<img src="https://i.imgur.com/9H363wf.png" alt=""></p>

<h2 id="complete-class-theorem-i"><a class="header-anchor" href="#complete-class-theorem-i">Complete class theorem I</a></h2>
<p>(admissible $\implies$ Bayes)</p>

<blockquote>
  <p>If $\d$ is admissible and $\T$ is finite, then $\d$ is Bayes (w.r.t some prior distribution).</p>
</blockquote>

<h2 id="complete-class-theorem-ii"><a class="header-anchor" href="#complete-class-theorem-ii">Complete class theorem II</a></h2>
<p>Class of Bayes estimators is complete</p>
<blockquote>
  <p>If $\T$ is finite and $\mc{S}$ is closed then the class of Bayes rules is complete and the admissible rules form a minimal complete class.</p>
</blockquote>

<h2 id="euclidean-parameter-spaces"><a class="header-anchor" href="#euclidean-parameter-spaces">Euclidean parameter spaces</a></h2>

<p>TODO: generalized Bayes estimator<br>
TODO: limiting Bayes estimator</p>

<p>Bayes $\implies$ Admissible<br>
<img src="https://i.imgur.com/BZJqBWn.png" alt=""></p>

<p>Admissible $\implies$ Bayes<br>
<img src="https://i.imgur.com/CGYqfbF.png" alt=""></p>

<h2 id="complete-class-theorem-iii"><a class="header-anchor" href="#complete-class-theorem-iii">Complete class theorem III</a></h2>
<p>Class of Bayes estimators is complete<br>
<img src="https://i.imgur.com/CFCCIMO.png" alt=""></p>

<h1 id="interpretation-and-implications"><a class="header-anchor" href="#interpretation-and-implications">Interpretation and implications</a></h1>

<p><strong>Question:</strong> What is the connection between <a href="https://en.wikipedia.org/wiki/Bayes_estimator#Definition" target="_blank" rel="noopener noreferrer">Bayesian estimators</a> and Bayesian posteriors?</p>

<p>Answer: Bayes estimator predicts the mean posterior for L2 loss, median for L1 loss.  [credit: John Chung]</p>

<p><strong>Theorem</strong>:<br>
If $\p(\t)$ is a given prior, then a corresponding Bayes estimator is $\d$ is</p>

<script type="math/tex; mode=display">\d(x) = \argmin_{\hat{\t}} \E_{\t \sim p_\p(\t \mid x)}\left[L(\t, \hat{\t})\right] = \argmin_{\hat{\t}} \int_{\T} L(\t, \hat{\t}) p_\pi(\t \mid x) \D{\t}\,,</script>

<p>where the posterior is $p_\pi(\t \mid x) = P_\t(x)\pi(\t)/p_\p(x)$ and marginal data distribution is $p_\p(x) = \int P_\t(x)\pi(\t) \D{\t}$.<br>
In words, the Bayes estimator minimizes the posterior expected loss for every $x$.</p>

<p><em>Proof:</em><br>
<br>(This proof is my own)</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\min_{\hat{\d}}R(\p, \hat{\d}) &= \min_{\hat{\d}} \int_\mc{X}\int_\T L(\t, \hat{\d}(x)) P_\t(x)\p(\t) \D{\t}\D{x} \\
  &= \min_{\hat{\d}} \int_\mc{X}\left(\int_\T L(\t, \hat{\d}(x)) p_\pi(\t \mid x) \D{\t}\right) p_\p(x) \D{x} \\
  &= \int_\mc{X}\left(\min_{\hat{\d}_x} \int_\T L(\t, \hat{\d}_x) p_\pi(\t \mid x) \D{\t}\right) p_\p(x) \D{x} \\
  &= \E_{x \sim p_\p(x)}\left[\min_{\hat{\d}_x} \int_\T L(\t, \hat{\d}_x) p_\pi(\t \mid x) \D{\t}\right] \\
  &= \E_{x \sim p_\p(x)}\left[\min_{\hat{\d}_x} \E_{\t \sim p_\p(\t \mid x)}\left[L(\t, \hat{\d}_x)\right] \right]\,.
\end{align} %]]></script>

<p>So the min Bayes risk is expected (w.r.t. data) minimum “posterior expected loss”.</p>

<p>Thus if we define $\d(x) := \d^*_x,\ \forall x \in \mc{X}$, where</p>

<script type="math/tex; mode=display">\d^*_x = \argmin_{\hat{\d}_x} \E_{\t \sim p_\p(\t \mid x)}\left[L(\t, \hat{\d}_x)\right]\,,</script>

<p>then $\d = \argmin_\hat{\d} R(\p, \hat{\d})\,.$<br>
<em>QED</em></p>

<p>The general form</p>

<script type="math/tex; mode=display">b^* = \argmin_b \E_A \left[L(A, b)\right]</script>

<p>is called the <em>systematic part</em> of random variable $A$. When $L$ is squared difference (i.e. $\ell^2$), then $b^*$ is the mean of $A$. When $L$ is absolute difference  (i.e. $\ell^1$), then $b^*$ is the median of $A$. When $L$ is the indicator loss (i.e. $\ell^0$), then $b^*$ is the mode of $A$. There are also losses corresponding to other distribution statistics like quantile loss. See the definition of <em>systematic part</em> in my post on the <a href="http://zhat.io/articles/bias-variance#bias-variance-decomposition-for-any-loss" target="_blank" rel="noopener noreferrer">generalized bias-variance decomposition</a>.</p>

<p>$\d$ will be the mean, median, or mode of the posterior for $\ell^2$, $\ell^1$, $\ell^0$ losses respectively. To avoid confusion, here it is stated explicitly:</p>

<p>If $L(\t, \hat{\t}) = (\t - \hat{\t})^2$, then</p>

<script type="math/tex; mode=display">\d(x) = \mathrm{Mean}_{\t \sim p_\p(\t \mid x)}\left[\t\right] = \E_{\t \sim p_\p(\t \mid x)}\left[\t\right]\,.</script>

<p>If $L(\t, \hat{\t}) = \lvert\t - \hat{\t}\rvert$, then</p>

<script type="math/tex; mode=display">\d(x) = \mathrm{Median}_{\t \sim p_\p(\t \mid x)}\left[\t\right]\,.</script>

<p>If $L(\t, \hat{\t}) = (\t - \hat{\t})^0$, then</p>

<script type="math/tex; mode=display">\d(x) = \mathrm{Mode}_{\t \sim p_\p(\t \mid x)}\left[\t\right]\,.</script>

<p>If <script type="math/tex">% <![CDATA[
L(\t, \hat{\t}) = \begin{cases}\tau\cdot(\t - \hat{\t}) & \t - \hat{\t} \geq 0 \\ (\tau-1)\cdot(\t - \hat{\t}) & \mathrm{otherwise}\end{cases}, %]]></script>     then</p>

<script type="math/tex; mode=display">\d(x) = \mathrm{Quantile}\{\tau\}_{\t \sim p_\p(\t \mid x)}\left[\t\right]\,,</script>

<p>and $\tau=\frac{1}{2}$ gives the median.</p>

<h2 id="discussion"><a class="header-anchor" href="#discussion">Discussion</a></h2>

<p>Do the complete class theorems prove the necessity of Bayesian epistemology (assuming you wish to be rational)?</p>
<ol>
  <li>Complete class theorems assume the data has a well defined probability distribution. If we use CCTs to justify Bayesian epistemology (i.e. usage of probability for outcomes which do not repeat, have a frequency or occurrence, or any well defined objective notion of probability) then this argument is circular. It depends on frequentist probability being a thing, and Bayesian probability is enticing over frequentist probability because frequentist probability only makes sense in limited circumstances where events have well defined frequencies of occurrence.</li>
  <li>Enforcing admissibility may be inconsequential. This framework is silent on how to define the hypothesis space and choose a prior, which matters quite a lot for 1 shot prediction, but doesn’t matter at infinite data limit. In practice we don’t care about the infinite data limit. In practice, picking the wrong hypothesis space or a bad prior may impact your utility much more than being admissible.</li>
  <li>The result above shows that only the <em>systematic part</em> (e.g. mean) of the posterior matters for minimizing Bayes risk.</li>
</ol>





    </article>
    <!-- http://sgeos.github.io/jekyll/disqus/2016/02/15/adding-disqus-to-a-jekyll-blog.html -->



<hr class="slender">

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://zhat.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" target="_blank" rel="noopener noreferrer">comments powered by Disqus.</a>
</noscript>
                            

    <span class="print-footer">Notes: Complete Class Theorems - June 11, 2020 - pragmanym</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links"> 
    <li><a href="/about">FAQ</a></li>  
    
      <li>
        <a href="mailto:pragmanym@gmail.com"><span class="icon-mail"></span></a>
      </li>
    
      <li>
        <a href="//github.com/danabo"><span class="icon-github"></span></a>
      </li>
    
      <li>
        <a href="/feed"><span class="icon-feed"></span></a>
      </li>
      
  </ul>
  <div class="credits">
  <span style="line-height: 3rem;">© 2020   PRAGMANYM</span><br>
  <span>This site uses <a href="//jekyllrb.com">Jekyll</a>. Look-and-feel inspired by the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>.</span> 
  </div>  
</footer>
  </body>
</html>
