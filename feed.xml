<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Z-Hat</title>
    <description>A (we)blog devoted to finding better representations</description>
    <link>pragmanym.github.io/zhat/</link>
    <atom:link href="pragmanym.github.io/zhat/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 09 Jun 2020 22:06:17 -0700</pubDate>
    <lastBuildDate>Tue, 09 Jun 2020 22:06:17 -0700</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Primer to Shannon's Information Theory</title>
        <description>&lt;p&gt;Shannon’s theory of information is usually just called &lt;em&gt;information theory&lt;/em&gt;, but is it deserving of that title? Does Shannon’s theory completely capture every possible meaning of the word &lt;em&gt;information&lt;/em&gt;? In the grand quests to creating AI and understanding the rules of the universe (i.e. grand unified theory) information may be key. Intelligent agents search for information and manipulate it. Particle interactions in physics may be viewed as information transfer. The physics of information may be key to interpreting quantum mechanics and resolving the measurement problem.&lt;/p&gt;

&lt;p&gt;If you endeavour to answer these hard questions, it is prudent to understand existing so-called theories of information so you can evaluate whether they are powerful enough and to take inspiration from them.&lt;/p&gt;

&lt;p&gt;Shannon’s information theory is a hard nut to crack. Hopefully this primer gets you far enough along to be able to read a textbook like &lt;em&gt;Elements of Information Theory&lt;/em&gt;. At the end I start to explore the question of whether Shannon’s theory is a complete theory of information, and where it might be lacking.&lt;/p&gt;

&lt;p&gt;This post is long. That is because Shannon’s information theory is a framework of thought. That framework has a vocabulary which is needed to appreciate the whole. I attempt to gradually build up this vocabulary, stopping along the way to build intuition. With this vocabulary in hand, you will be ready to explore the big questions at the end of this post.&lt;/p&gt;

&lt;!--more--&gt;

&lt;ul class=&quot;toc&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#self-information&quot; id=&quot;markdown-toc-self-information&quot;&gt;Self-Information&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#bits-not-bits&quot; id=&quot;markdown-toc-bits-not-bits&quot;&gt;&lt;em&gt;Bits&lt;/em&gt;, not bits&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#stepping-back&quot; id=&quot;markdown-toc-stepping-back&quot;&gt;Stepping back&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#entropy&quot; id=&quot;markdown-toc-entropy&quot;&gt;Entropy&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#regarding-notation&quot; id=&quot;markdown-toc-regarding-notation&quot;&gt;Regarding notation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#conditional-entropy&quot; id=&quot;markdown-toc-conditional-entropy&quot;&gt;Conditional Entropy&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#mutual-information&quot; id=&quot;markdown-toc-mutual-information&quot;&gt;Mutual Information&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#pointwise-mutual-information&quot; id=&quot;markdown-toc-pointwise-mutual-information&quot;&gt;Pointwise Mutual Information&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#properties-of-pmi&quot; id=&quot;markdown-toc-properties-of-pmi&quot;&gt;Properties of PMI&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#special-values&quot; id=&quot;markdown-toc-special-values&quot;&gt;Special Values&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#expected-mutual-information&quot; id=&quot;markdown-toc-expected-mutual-information&quot;&gt;Expected Mutual Information&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#channel-capacity&quot; id=&quot;markdown-toc-channel-capacity&quot;&gt;Channel capacity&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#shannon-information-for-continuous-distributions&quot; id=&quot;markdown-toc-shannon-information-for-continuous-distributions&quot;&gt;Shannon Information For Continuous Distributions&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#proof-that-mi-is-fininte-for-continuous-distributions&quot; id=&quot;markdown-toc-proof-that-mi-is-fininte-for-continuous-distributions&quot;&gt;Proof that MI is fininte for continuous distributions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#problems-with-shannon-information&quot; id=&quot;markdown-toc-problems-with-shannon-information&quot;&gt;Problems With Shannon Information&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1-tv-static-problem&quot; id=&quot;markdown-toc-1-tv-static-problem&quot;&gt;1. TV Static Problem&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-shannon-information-is-blind-to-scrambling&quot; id=&quot;markdown-toc-2-shannon-information-is-blind-to-scrambling&quot;&gt;2. Shannon Information is Blind to Scrambling&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3-deterministic-information&quot; id=&quot;markdown-toc-3-deterministic-information&quot;&gt;3. Deterministic information&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4-if-the-universe-is-continuous-everything-contains-infinite-information&quot; id=&quot;markdown-toc-4-if-the-universe-is-continuous-everything-contains-infinite-information&quot;&gt;4. If the universe is continuous everything contains infinite information&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5-shannon-information-ignores-the-meaning-of-messages&quot; id=&quot;markdown-toc-5-shannon-information-ignores-the-meaning-of-messages&quot;&gt;5. Shannon information ignores the meaning of messages&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#6-probability-distributions-are-not-objective&quot; id=&quot;markdown-toc-6-probability-distributions-are-not-objective&quot;&gt;6. Probability distributions are not objective&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#appendix&quot; id=&quot;markdown-toc-appendix&quot;&gt;Appendix&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#properties-of-conditional-entropy&quot; id=&quot;markdown-toc-properties-of-conditional-entropy&quot;&gt;Properties of Conditional Entropy&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bayes-rule&quot; id=&quot;markdown-toc-bayes-rule&quot;&gt;Bayes’ Rule&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cross-entropy-and-kl-divergence&quot; id=&quot;markdown-toc-cross-entropy-and-kl-divergence&quot;&gt;Cross Entropy and KL-Divergence&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;self-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#self-information&quot;&gt;Self-Information&lt;/a&gt;&lt;/h1&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\newcommand{\and}{\wedge}
\newcommand{\or}{\vee}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bm}{\boldsymbol}
\newcommand{\rX}{\bm{X}}
\newcommand{\rY}{\bm{Y}}
\newcommand{\rZ}{\bm{Z}}
\newcommand{\rC}{\bm{C}}
\newcommand{\diff}[1]{\mathop{\mathrm{d}#1}}&lt;/script&gt;

&lt;p&gt;I’m going to use non-standard notation which I believe avoids some confusion and ambiguities.&lt;/p&gt;

&lt;p&gt;Shannon defines information indirectly by defining quantity of information contained in a message/event. This is analogous how physics defines mass and energy in terms of their quantities.&lt;/p&gt;

&lt;p&gt;We can derive Shannon’s definition from two principles:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Quantity of information is a function only of probability of occurrence.&lt;/li&gt;
  &lt;li&gt;Quantity of information acts like quantity of bits when applied to computer memory.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(1) implies that messages/events must always come from a distribution, which is what provides the probabilities. Say you receive a message $x$ sampled from probability distribution (function) $p_X : X \to [0, 1]$ over a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;discrete&lt;/span&gt;&lt;label for=&quot;00fa9a5874a8c4791694a74a449fd8e01597cedd&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;00fa9a5874a8c4791694a74a449fd8e01597cedd&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Assume all distributions are discrete until the &lt;a href=&quot;#shannon-information-for-continuous-distributions&quot;&gt;continuous section&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; event space $X$. The &lt;strong&gt;self-information&lt;/strong&gt; of $x$, i.e. the amount of information gained by receiving $x$, is the quantity &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;$h(x)$&lt;/span&gt;&lt;label for=&quot;ad226219b7413f6c19c43a404b5a9d36708aa40e&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ad226219b7413f6c19c43a404b5a9d36708aa40e&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The standard notation is $I(x)$, but this is easy to confuse with mutual information &lt;a href=&quot;#expected-mutual-information&quot;&gt;below&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. Formally, (1) requires that $h(x) = f(p_X(x))$, where $f$ is some function of the probability $p_X(x)$ only, and not $x$ itself. So far, we don’t know what $f$ should be. Let’s dig deeper.&lt;/p&gt;

&lt;p&gt;For (2), consider computer memory. With $N$ bits, there is one thing out of $2^N$ possibilities the memory can store. Increasing the number of bits exponentially increases the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;number of counterfactuals&lt;/span&gt;&lt;label for=&quot;b6005829cc8dd4708170068c8b34f75f8b47c99b&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;b6005829cc8dd4708170068c8b34f75f8b47c99b&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;I mean the number of possible things that are not the case.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. It’s useful here to think of a probability distribution as a weighted possibility space. To achieve (2), i.e. equivalence to computer bits, we assume a uniform weighting when none is given (we just have a possibility space). For $2^N$ possible outcomes, the &lt;em&gt;weight&lt;/em&gt; of a single outcome is $\frac{1}{2^N} = 2^{-N}$, given a uniform weighting.&lt;/p&gt;

&lt;p&gt;Combining (1) and (2), $f$ should return $N$ when $p=2^{-N}$. It is clear that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
f(p) &amp;= -\log_2 p \\
\Longrightarrow h(x) &amp;= -\log_2 p_X(x)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;From here on, I will use $h(x)$ as a function of message $x$, without specifying the type of $x$. It can be anything, a number, a binary sequence, a string, etc. $f(p)$ is a function of probability, rather than messages. Keep in mind that $h(x) = f(p_X(x))$, and so $h$ implicitly assumes we have a probability distribution over $x$ defined somewhere. This distinction between $h$ and $f$ will help avoid confusion.&lt;/p&gt;

&lt;h2 id=&quot;bits-not-bits&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#bits-not-bits&quot;&gt;&lt;em&gt;Bits&lt;/em&gt;, not bits&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Things get interesting if $p_X(x)$ is not a (negative) integer power of two. $h(x)$ will be non-integer, and very likely irrational. What does it mean to have a fraction of a bit? Here is where we part ways from computer bits. It’s better to think of $h$ as a quantity of information, rather than a count of physical objects. We will continue to call the unit of $h(x)$ a &lt;em&gt;bit&lt;/em&gt;, but this is a mere convention.&lt;/p&gt;

&lt;p&gt;So then how is $h$ to be understood? What is the intuition behind this quantity? In short, Shannon bits are an &lt;a href=&quot;https://en.wikipedia.org/wiki/Analytic_continuation&quot;&gt;analytic continuation&lt;/a&gt; of computer bits. Just like how the &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_function&quot;&gt;gamma function&lt;/a&gt; extends factorial to continuous values, Shannon bits extend the computer bit to &lt;strong&gt;non-uniform distributions&lt;/strong&gt; over a &lt;strong&gt;non-power-of-2&lt;/strong&gt; number of counterfactuals. Let me explain these two phrases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;non-power-of-2&lt;/strong&gt;: We have memory that can store one out of $M$ possibilities, where $M \neq 2^N$. For example, I draw a card from a deck of 52. That card holds $-\log_2 \frac{1}{52} = \log_2 52 \approx 5.70044\ldots$ bits of information. A fractional bit can represent a non-power-of-2 possibility space, and quantifies the log-base conversion factor into base $M$. In this case $-log_{52} x = -\frac{\log_2 x}{\log_2 52}$. Note that it is actually common to use units of information other than base-2. For example a &lt;a href=&quot;https://en.wikipedia.org/wiki/Nat_(unit)&quot;&gt;&lt;em&gt;nat&lt;/em&gt;&lt;/a&gt; is log-base-e, a &lt;a href=&quot;https://en.wikipedia.org/wiki/Ternary_numeral_system&quot;&gt;&lt;em&gt;trit&lt;/em&gt;&lt;/a&gt; is base-3, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Hartley_(unit)&quot;&gt;&lt;em&gt;dit&lt;/em&gt; or &lt;em&gt;ban&lt;/em&gt;&lt;/a&gt; is base-10.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;non-uniform distributions&lt;/strong&gt;: Using the deck of cards example, let’s say we draw from a sub-deck containing all cards with the hearts suit. We’ve reduced the possibility space to a subset of a super-space, in this case size 13, and have reduced the information contained in a given card, $-\log_2 \frac{1}{13} \approx 3.70044\ldots$ bits. You can think of this as assigning a weight to each card: 0 for cards we exclude, and $\frac{1}{13}$ for cards we include. If we make the non-zero weights non-uniform, we now have an interpretational issue: what is the physical meaning of these weights? Thinking of this weight as a probability of occurrence is one way to recover physical meaning, but this is &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;not a requirement&lt;/span&gt;&lt;label for=&quot;02ee609808a0f8d62fa190ce247f9b35f70dd990&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;02ee609808a0f8d62fa190ce247f9b35f70dd990&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;And probability may not even be an objective property of physical systems in general.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. However, I will &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;call these weights probabilities&lt;/span&gt;&lt;label for=&quot;03b6daf7c965c2773f771111cb006d8764629617&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;03b6daf7c965c2773f771111cb006d8764629617&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The reason we wish to hold sum of weights fixed to 1 is so that we can consider the information contained in compound events which are sets of elementary events. In other words, think of the card drawn from the sub-deck of 13 as a card from &lt;em&gt;any suit&lt;/em&gt;, i.e. the set of 4 cards with the same number. The card represents an equivalence class over card number.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, and the weighted-possibility-spaces distributions, as that is the convention. But keep in mind that these weights do not necessarily represent frequencies of occurrence nor uncertainties. The meaning of probability itself is a subject of debate.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The reason we wish to hold sum of weights fixed to 1 is so that we can consider the information contained in compound events which are sets of elementary events. In other words, think of the card drawn from the sub-deck of 13 as a card from &lt;em&gt;any suit&lt;/em&gt;, i.e. the set of 4 cards with the same number. The card represents an equivalence class over card number.&lt;/p&gt;

&lt;p&gt;Let’s examine some of the properties of $h$ to build further intuition.&lt;/p&gt;

&lt;p&gt;First notice that $f(1) = 0$. An event with a probability of 1 contains no information. If $x$ is certain to occur, $x$ is uninformative. Likewise, $f(p) \to \infty$ as $p \to 0$. If $x$ is impossible, it contains infinite information! In general, $h(x)$ goes up as $p_X(x)$ goes down. The less likely an event, the more information it contains. Hopefully this sounds to you like a reasonable property of information.&lt;/p&gt;

&lt;p&gt;Next, we can be more specific about how $h$ goes up as $p_X$ goes down. Recall that $f(p) = -\log_2 p$ and $h(x) = f(p_X(x))$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(p/2) = f(p) + 1\,.&lt;/script&gt;

&lt;p&gt;If we halve the probability of an event, we add one bit of information to it. That is a nice way to think about our new unit of information. The &lt;em&gt;bit&lt;/em&gt; is a halving of probability. Other units can be defined in this way, e.g. the &lt;em&gt;nat&lt;/em&gt; is dividing of probability by Euler’s constant e, the &lt;em&gt;trit&lt;/em&gt; is a thirding of probability, etc.&lt;/p&gt;

&lt;p&gt;Finally, notice that $f(pq) = f(p) + f(q)$. Or to write it another way: $h(x \and y) = h(x) + h(y)$ iff $x$ and $y$ are independent events, because&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
h(x \and y) &amp;= -\log_2 p_{X,Y}(x \and y) \\
  &amp;= -\log_2 p_X(x)\cdot p_Y(y) \\
  &amp;= -\log_2 p_X(x) - \log_2 p_Y(y)\,,
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $x \and y$ indicates the composite event &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;“$x$ and $y$”&lt;/span&gt;&lt;label for=&quot;03c34b59dc7fea1cd51e8dbb51bdcfc9754145fd&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;03c34b59dc7fea1cd51e8dbb51bdcfc9754145fd&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;We could either think of $x$ and $y$ as composite events themselves from the same distribution, i.e. $x$ and $y$ are sets of &lt;a href=&quot;https://en.wikipedia.org/wiki/Elementary_event&quot;&gt;elementary events&lt;/a&gt;, or as elementary events from two different random variables which have a joint distribution, i.e, $(x, y) \sim (\rX, \rY)$. I will consider the latter case from here on out, because it is conceptually simpler.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. Hopefully this is also intuitive. If two events are dependent, i.e. they causally affect each other, it makes sense that they might contain redundant information, meaning that you can predict part of one from the other, and so their combined information is less than the sum of their individual information. You may be surprised to learn that the opposite can also be true. The combined information of two events can be greater than the sum of their individual information! This is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Interaction_information#Example_of_negative_interaction_information&quot;&gt;&lt;em&gt;synergy&lt;/em&gt;&lt;/a&gt;. More on that in the &lt;a href=&quot;#pointwise-mutual-information&quot;&gt;pointwise mutual information&lt;/a&gt; section.&lt;/p&gt;

&lt;p&gt;In short, we can derive $f(p) = -\log_2 p$ from (1) additivity of information, $f(pq) = f(p) + f(q)$, and (2) a choice of unit, $f(½) = 1$. &lt;a href=&quot;https://en.wikipedia.org/wiki/Entropy_(information_theory)#Rationale&quot;&gt;Proof&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;recap&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#recap&quot;&gt;Recap&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;To make the full analogy: a weighting over possibilities is like a continuous relaxation of a set. Out of a larger space of possibilities, we score each as being more or less possible, with the discrete case being all weights are 0 or a uniform non-zero weight. With a weighted possibility space we have a lot more freedom to work with extra information beyond just merely which possibilities are in the set. Probability distributions are more expressive than mere sets.&lt;/p&gt;

&lt;h2 id=&quot;stepping-back&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#stepping-back&quot;&gt;Stepping back&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The unit &lt;em&gt;bit&lt;/em&gt; that we’ve defined is connected to computer bits only because they both convert multiplication to addition.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Computer bits: $2^N\cdot2^M$ states $\Longrightarrow$ $N+M$ bits.&lt;/li&gt;
  &lt;li&gt;Shannon bits: $p\cdot q$ probability $\Longrightarrow$ $-\log_2 p - \log_2 q$ bits.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The way I’ve motivated $h$ is a departure from Shannon’s original motivation for defining self-information, which was to describe the theoretically optimal lossless compression for messages being sent over a communication channel. Under this viewpoint, $h(x)$ quantifies the theoretically minimum possible length (in physical bits) to encode message $x$ in computer memory without loss of information. Under this view, $h(x)$ should be thought of as the asymptotic average bit-length for the optimal encoding of $x$ in an infinite sequence of messages drawn from $p_X$. Hence why it makes sense for $h(x)$ to be a continuous value. For more details, see &lt;a href=&quot;https://en.wikipedia.org/wiki/Arithmetic_coding#Connections_with_other_compression_methods&quot;&gt;arithmetic coding&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We are now flipping Shannon’s original motivation on its head, and using the theoretically optimal encoding length in bits as the definition of information content. In the following discussion, we don’t care how messages/events are actually represented physically. Our definition of information only cares about probability of occurrence, and is in fact &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;blind to the contents of messages&lt;/span&gt;&lt;label for=&quot;0f502be1c82d5cf454a4e7396c9fabbdcabaac09&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;0f502be1c82d5cf454a4e7396c9fabbdcabaac09&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Something that could be seen as a fatal flaw, which I’ll discuss &lt;a href=&quot;#5-shannon-information-ignores-the-meaning-of-messages&quot;&gt;below&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. The connection of probability to optimal physical encoding is one of the beautiful results that propelled Shannon’s framework into its lofty position as &lt;em&gt;information theory&lt;/em&gt;. However, for our purposes, we simply care about defining quantity of information, and do not care at all about how best to compress data for practical purposes.&lt;/p&gt;

&lt;h1 id=&quot;entropy&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#entropy&quot;&gt;Entropy&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;In the last section I said that under the view of optimal lossless compression, $h(x)$ is the bit length of the optimal encoding for $x$ averaged over an infinite sample from random variable $\rX$, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Arithmetic_coding#Connections_with_other_compression_methods&quot;&gt;arithmetic coding&lt;/a&gt; can approach this limit. We could also consider the average bit length per message from $\rX$ (averaged across all messages). That is the &lt;strong&gt;entropy&lt;/strong&gt; of random variable $\rX$, which is the expected self-information,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
H[\rX] &amp;= \E_{x\sim \rX}[h(x)] \\
       &amp;= \E_{x\sim \rX}[-\log_2\,p_X(x)]\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the quantifying information view, think of entropy $H[\rX]$ as the number of bits you expect to gain by observing an event sampled from $p_X(x)$. In that sense it is a measure of uncertainty, i.e. how much information I do not have, i.e. quantifying what is unknown.&lt;/p&gt;

&lt;p&gt;Let’s build our intuition of entropy. A good way to view entropy is as a measure of how spread out a distribution is. Entropy is actually a type of &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_dispersion&quot;&gt;statistical dispersion&lt;/a&gt; of $p_X$, meaning you could use it as an &lt;a href=&quot;http://zhat.io/articles/19/bias-variance#what-is-variance-anyway&quot;&gt;alternative to statistical variance&lt;/a&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/posts/primer-shannon-information/bimodal.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;For example, a bi-modal distribution can have arbitrarily high variance by moving the modes far apart, but the overall spread-out-ness (entropy) will not necessarily change.&lt;/p&gt;

&lt;p&gt;The more spread out a distribution is, the higher its entropy. For bounded &lt;a href=&quot;https://en.wikipedia.org/wiki/Support_(mathematics)#Support_of_a_distribution&quot;&gt;support&lt;/a&gt;, the uniform distribution has highest entropy (&lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution#Other_examples.&quot;&gt;other max-entropy distributions&lt;/a&gt;). The &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;minimum possible entropy is 0&lt;/span&gt;&lt;label for=&quot;61d3d043a10d74a21176f1255203a29c26b0f6f4&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;61d3d043a10d74a21176f1255203a29c26b0f6f4&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Note that in the expectation, 0-probability outcomes have infinite self-information, so we have to use the convention that $p_X(x)\cdot h(x) = 0\cdot\infty = 0$.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, which indicates a deterministic distribution, i.e. $p_X(x) \in {0, 1}$ for all $x \in X$.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/2/22/Binary_entropy_plot.svg&quot; alt=&quot;Credit: &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/Binary_entropy_function&amp;quot;&amp;gt;https://en.wikipedia.org/wiki/Binary_entropy_function&amp;lt;/a&amp;gt;&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;Credit: &lt;a href=&quot;https://en.wikipedia.org/wiki/Binary_entropy_function&quot;&gt;https://en.wikipedia.org/wiki/Binary_entropy_function&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Though Shannon calls his new idea entropy, the connection to physical entropy is nontrivial. If there is a connection, that is more of a coincidence. Apparently Shannon’s decision to call it entropy was made by a suggestion by von Neumann at a party:  http://www.eoht.info/page/Neumann-Shannon+anecdote
[credit: Mark Moon]&lt;/p&gt;

&lt;p&gt;There are connections between information entropy and thermodynamics entropy (see https://plato.stanford.edu/entries/information-entropy/), but I do not yet understand them well enough to give an overview here - perhaps in a future post. Some physicists consider information to have a physical nature, and even a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;conservation law&lt;/span&gt;&lt;label for=&quot;4cd9a9099f83eeb0f5a534d111b0875861d2c3ec&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;4cd9a9099f83eeb0f5a534d111b0875861d2c3ec&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;In the sense that requiring physics be time-symmetric is equivalent to requiring information to be conserved.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;! Further reading: &lt;a href=&quot;https://theoreticalminimum.com/courses/statistical-mechanics/2013/spring/lecture-1&quot;&gt;The Theoretical Minimum - Entropy and conservation of information&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/No-hiding_theorem&quot;&gt;no-hiding theorem&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: Why expected self-information?
We could have used median or something else. Expectation is a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;default go-to operation over distributions&lt;/span&gt;&lt;label for=&quot;e3be61e72d21ae86483befb994084998b611b8df&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;e3be61e72d21ae86483befb994084998b611b8df&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;See my previous post: &lt;a href=&quot;http://zhat.io/articles/bias-variance#bias-variance-decomposition-for-any-loss&quot;&gt;http://zhat.io/articles/bias-variance#bias-variance-decomposition-for-any-loss&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; because of its nice properties, but ultimately it is an arbitrary choice. However, as we will see, one huge benefit in our case is that expectation is linear.&lt;/p&gt;

&lt;h3 id=&quot;regarding-notation&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#regarding-notation&quot;&gt;Regarding notation&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;From here on out, I will drop the subscript $X$ from $p_X(x)$ when $p(x)$ unambiguously refers to the probability of $x$. This is a common thing to do, but it can also lead to ambiguity if I want to write $p(0)$, the probability that $x$ is 0. A possible resolution is to use random variable notation, $p(\rX = 0)$, which I use in some places. However, there is the same issue for self-information. For example, quantities $h(x), h(y), h(x\and y), h(y \mid x)$. I will add subscripts to $h$ when it would be ambiguous otherwise, for example $h_X(0), h_Y(0), h_{X,Y}(x\and y), h_{Y\mid X}(0 \mid 0)$ .&lt;/p&gt;

&lt;h2 id=&quot;conditional-entropy&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#conditional-entropy&quot;&gt;Conditional Entropy&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Conditional self-information, defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
h(y \mid x) &amp;= -\log_2\,p(y \mid x)\\
  &amp;= -\log_2(p(y \and x) / p(x)) \\
  &amp;= h(x \and y) - h(x)\,,
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;is the information you stand to gain by observing $y$ given that you already observed $x$.  I let $x \and y$ denote the observation of $x$ and $y$ together (I could write $(x, y)$, but then $p((y, x))$ would look awkward).&lt;/p&gt;

&lt;p&gt;If $x$ and $y$ are independent events, $h(y \mid x) = h(y)$. Otherwise, $h(y \mid x)$ can be greater or less than $h(y)$. It may seem counterintuitive that $h(y \mid x) &amp;gt; h(y)$ can happen, because this implies you gain more from $y$ by just simply knowing something else, $x$. However, this reflects the fact that you are unlikely to see $x, y$ together. Likewise, if $h(y \mid x) &amp;lt; h(y)$ you are likely to see $x, y$ together. More on this in the next section.&lt;/p&gt;

&lt;p&gt;Confusingly, conditional entropy can refer to two different things.&lt;/p&gt;

&lt;p&gt;First is expected conditional self-information,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
H[\rY \mid \rX = x] &amp;= \E_{y\sim \rY \mid \rX=x}[h(y \mid x)] \\
  &amp;= \E_{y\sim \rY \mid \rX=x}[\log_2\left(\frac{p(x)}{p(x, y)}\right)] \\
  &amp;= \sum\limits_{y \in Y} p(y \mid x) \log_2\left(\frac{p(x)}{p(x, y)}\right)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The other is what is most often referred to as &lt;strong&gt;conditional entropy&lt;/strong&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
H[\rY \mid \rX] &amp;= \E_{x,y \sim \rX,\rY}[h(y \mid x)] \\
  &amp;= \E_{x,y \sim \rX,\rY}[\log_2\left(\frac{p(x)}{p(x, y)}\right)] \\
  &amp;= \E_{x\sim \rX} H[\rY \mid \rX = x]\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The intuition behind $H[\rY \mid \rX = x]$ will be the same as of entropy, $H[\rY]$, which we covered in the last section. Let’s gain some intuition for $H[\rY \mid \rX]$. If $H[\rY]$ measures uncertainty of $\rY$, then $H[\rY \mid \rX = x]$ measures conditional uncertainty given $x$, and $H[\rY \mid \rX]$ measures average conditional uncertainty w.r.t. $\rX$.&lt;/p&gt;

&lt;p&gt;The maximum value of $H[\rY \mid \rX]$ is $H[\rY]$, which is achieved when $\rX$ and $\rY$ are independent random variables. This should make sense, as recieving a message from $\rX$ does not tell you anything about $\rY$, so your state of uncertainty does not decrease.&lt;/p&gt;

&lt;p&gt;The minimum value of $H[\rY \mid \rX]$ is 0, which is achieved when $p_{\rY \mid \rX}(\rY \mid \rX = x)$ is deterministic for all $x$. In other words, you can define a function $g : X \rightarrow Y$ to map from $X$ to $Y$. This wouldn’t otherwise be the case when $\rY \mid \rX$ is stochastic.&lt;/p&gt;

&lt;p&gt;$H[\rY \mid \rX]$ is useful because it takes all $x \in X$ into consideration. You might have, for example, $H[\rY \mid \rX = x_1] = 0$ for $x_1$, but $H[\rY \mid \rX] &amp;gt; 0$, which means $y$ cannot always be deterministically decided from $x$. In the section on mutual information we will see how to think of $H[\rY \mid \rX]$ as a property of a stochastic function from $X$ to $Y$.&lt;/p&gt;

&lt;p&gt;Because of linearity of expectation, all identities that hold for self-information hold for their entropy counterparts. For example,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
h(y \mid x) &amp;= h(x \and y) - h(x) \\
\Longrightarrow H[\rY \mid \rX] &amp;= H[(\rX, \rY)] - H[\rX]\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;This is a nice result. This equation says that the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;average uncertainty about $\rY$ given $\rX$&lt;/span&gt;&lt;label for=&quot;40b324ca64f32166d198598020cd16fd3a369058&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;40b324ca64f32166d198598020cd16fd3a369058&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Amount of information left to observe in $\rY$ on average.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; equals the total expected information in their joint distribution, $(\rX, \rY)$, minus the average information in $\rX$. In other words, conditional entropy is the total information in $x \and y$ minus information in what you have, $x$, all averaged over all the possible $(x, y)$ you can have.&lt;/p&gt;

&lt;h1 id=&quot;mutual-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#mutual-information&quot;&gt;Mutual Information&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;In my view, mutual information is what holds promise as a definition of information. This it the most important topic to understand for tackling the “problems with Shannon information” section below.&lt;/p&gt;

&lt;h2 id=&quot;pointwise-mutual-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#pointwise-mutual-information&quot;&gt;Pointwise Mutual Information&lt;/a&gt;&lt;/h2&gt;

&lt;!-- Intuitively, if two events are causally connected, i.e. dependent, they contain redundant information combined. meaning that their combined information would be less than the sum of their information. It may  also be the case that their combined information could be greater than the sum of their information! This is called *synergy*. We will see examples of this later. --&gt;

&lt;p&gt;When two events $x$ and $y$ are dependent, how to we compute their total information? Previously we said that $h(x \and y) = h(x) + h(y)$ iff $p_X(x \and y) = p_X(x)p_X(y)$. However, the general case is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x \and y) = h(x) + h(y) - i(x, y)\,,&lt;/script&gt;

&lt;p&gt;where I am defining $i(x, y)$ such that this equation holds. Rearranging we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
i(x, y) &amp;= h(x) + h(y) - h(x \and y) \\
        &amp;= -\log_2(p_X(x)) - \log_2(p_X(y)) + \log_2(p_X(x \and y)) \\
        &amp;= \log_2\left(\frac{p_X(x, y)}{p_X(x)p_X(y)}\right)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$i(x, y)$ is called &lt;em&gt;pointwise mutual information&lt;/em&gt; (PMI). Informally, PMI measures the amount of bits shared by two events. To say that another way, it measures how much information I have about one event given I only observe the other. Notice that PMI is symmetric, $i(x, y) = i(y, x)$, so any two events contain the same information about each other.&lt;/p&gt;

&lt;p&gt;$i(x, y)$ is a difference in information. Positive $i(x, y)$ indicates &lt;em&gt;redundancy&lt;/em&gt;, i.e. total information is less than the sum of the parts: $h(x, y) &amp;lt; h(x) + h(y)$. However, it may also be the case that $i(x, y)$ is negative so that $h(x, y) &amp;gt; h(x) + h(y)$. &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;This is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Synergy#Information_theory&quot;&gt;&lt;em&gt;synergy&lt;/em&gt;&lt;/a&gt;.&lt;/span&gt;&lt;label for=&quot;4ce5fcaeebe9d13d3fd8d49c2de4ba62a623e205&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;4ce5fcaeebe9d13d3fd8d49c2de4ba62a623e205&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The word &lt;em&gt;synergy&lt;/em&gt; is conventionally used in the context of expected mutual information, and I am running the risk of conflating two distinct phenomenon under the same word. There is no synergy among two random variables under expected mutual information, and this type of synergy only appears among 3 or more random variables.  See &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_mutual_information#Synergy_and_redundancy&quot;&gt;https://en.wikipedia.org/wiki/Multivariate_mutual_information#Synergy_and_redundancy&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This is highly speculative, but synergy (either the pointwise-MI or expected-MI kind) may be a fundamental insight that could explain emergence and limitations of reductionism in understanding reality. See &lt;a href=&quot;https://www.scottaaronson.com/blog/?p=3294&quot;&gt;Higher-level causation exists (but I wish it didn’t)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(If you object that it doesn’t make sense to lose information by observing $x$ and $y$ together over observing them separately, it is important to note that $h(x) + h(y)$ is not a physically meaningful quantity, unless they are independent. Technically, you would have $h(x) + h(y \mid x)$.)&lt;/p&gt;

&lt;!--
Let's look at (an admittedly contrived) example of synergy. Suppose our [sample space](https://en.wikipedia.org/wiki/Sample_space) is $\{a, b, c\}$ (composed of [coutcomes](https://en.wikipedia.org/wiki/Sample_space#Conditions_of_a_sample_space) or [elementary events](https://en.wikipedia.org/wiki/Elementary_event)), and we have two events $x = \{a, b\}$ and $y = \{b, c\}$. $x, y$ co-occur if we draw outcome $b$. If $p(a) = 7/16, p(b) = 1/8, p(c) = 7/16$, then $p(x) = 9/16$, $p(y) = 9/16$, $p(x \and y) = 1/8$. $h(x) = h(y) \approx 0.83$ and $h(x \and y) = 3$, so $i(x, y) = 2\cdot0.83 - 3 \approx -1.34$ bits.


That may seem like a contrived example, because I was working with composite events instead of elementary events. The same phenomenon can happen for joint distributions of sample spaces.  &lt;font color=&quot;red&quot;&gt;TODO: explain the difference between the example above and a joint distribution.&lt;/font&gt;
--&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;
Let $X = {0, 1}$ and $Y = {0, 1}$, then the joint sample space is the cartesian product $X \times Y$. $p_X(x), p_Y(y)$ denote marginal probabilities, and $p_{X,Y}(x, y)$ is their joint probability. The joint probability table:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$x$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$y$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$p_{X,Y}(x, y)$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7/16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7/16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/16&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We have&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$h_X(0) = -\log_2 p_X(0) = -\log_2 1/2 = 1$&lt;/li&gt;
  &lt;li&gt;$h_Y(0) = -\log_2 p_Y(0) = -\log_2 1/2 = 1$&lt;/li&gt;
  &lt;li&gt;$h_{X,Y}(0 \and 0) = -\log_2 p_{X,Y}(0, 0) = -\log_2 1/16 = 4$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$i(0,0) = h_X(0) + h_Y(0) - h_{X,Y}(0 \and 0) = -2$, and so $(0,0)$ is synergistic. On the other hand, $i(0,1) \approx 0.80735$, indicating $(0,1)$ is redundant.&lt;/p&gt;

&lt;h2 id=&quot;properties-of-pmi&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#properties-of-pmi&quot;&gt;Properties of PMI&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Let’s explore some of the properties of PMI. From here on out, I will consider sampling elementary events from a joint distribution, $(x, y) \sim \bm{X}\times\bm{Y}$, where $\bm{X}, \bm{Y}$ are unspecified discrete (possibly infinite) random variables. For notational simplicity I’ll drop the subscripts from distributions, so $p(x), p(y), p(x, y)$ denote the marginals of $\bm{X}, \bm{Y}$ and the joint $\bm{X}\times\bm{Y}$.&lt;/p&gt;

&lt;p&gt;To recap, PMI measures the difference in bits between the product of marginals $p(x)p(y)$ and the joint $p(x, y)$, as evidenced by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
i(x, y) &amp;= \log_2\left(\frac{p(x, y)}{p(x)p(y)}\right) \\
        &amp;= h(x) + h(y) - h(x \and y)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Negative PMI implies synergy, while positive PMI implies redundancy.&lt;/p&gt;

&lt;p&gt;Another way to think about PMI is as a measure of how much $p(y \mid x)$ differs from $p(y)$ (and vice versa). Suppose an oracle sampled $(x, y) \sim \rX\times\rY$, but the outcome $(x, y)$ remains hidden from you. $p(y)$ is the information you stand to gain by having $y$ revealed to you. However, $p(y \mid x)$ is what you stand to gain from seeing $y$ if $x$ is already revealed. You do not know how much information $x$ contains about $y$ without seeing $y$. Only the oracle knows this. However, if you know $p(y \mid x)$, then you can compute your expected information gain (conditional uncertainty), $H[\rY \mid \rX=x]$.&lt;/p&gt;

&lt;p&gt;PMI measures the change in information you will gain about $y$ (from the oracle’s perspective) before and after $x$ is revealed (and vice versa). In this view, it makes sense to rewrite PMI as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
i(x, y) &amp;= \log_2\left(\frac{p(y \mid x)}{p(y)}\right) \\
        &amp;= -\log_2\,p(y) + \log_2\,p(y \mid x) \\
        &amp;= h(y) - h(y \mid x)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;special-values&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#special-values&quot;&gt;Special Values&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;By definition, $i(x, y) = 0$ iff $\bm{X}, \bm{Y}$ are independent. Verifying, we see that,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
i(x, y) &amp;= \log_2\left(\frac{p(x)p(y)}{p(x)p(y)}\right) \\
        &amp;= 0\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The maximum possible PMI happens when $x$ and $y$ are perfectly associated, i,e. $p(y \mid x) = 1$ or $p(x \mid y) = 1$. So $h(y \mid x) = 0$ or vice versa, meaning you know everything about $y$ if you have $x$. Then $i(x, y) = h(y) - h(y \mid x) = h(y)$. In general, the maximum possible PMI is $\min{h(x), h(y)}$.&lt;/p&gt;

&lt;p&gt;PMI has no minimum, and goes to $-\infty$ if $x$ and $y$ can never occur together but can occur separately, i.e. $p(x, y) = 0$ while $p(x), p(y) &amp;gt; 0$. We can see that $p(y \mid x) = p(x, y)/p(x) = 0$ so long as $p(x) &amp;gt; 0$. So $h(y \mid x) \to \infty$, and we have $i(x, y) = h(y) - h(y \mid x) \to -\infty$ if $h(y) &amp;gt; 0$.&lt;/p&gt;

&lt;p&gt;While redundancy is bounded, synergy is infinite. This should make sense, as $h(x), h(y)$ are bounded so there is a maximum amount of information to redundantly share. On the other hand, synergy measures how rare the co-occurrence of $(x,y)$ together are, relative to their marginal probabilities, where lower $p(x, y)$ means their co-occurrence is more special. So if $(x,y)$ can never occur, then their co-occurrence is infinitely special.&lt;/p&gt;

&lt;h2 id=&quot;expected-mutual-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#expected-mutual-information&quot;&gt;Expected Mutual Information&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Expected mutual information, also just called mutual information (MI), is given as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
I[\rX, \rY] &amp;= \E_{x\sim X, y\sim Y}[i(x, y)] \\
        &amp;= \E_{x\sim X, y\sim Y}\left[\log_2\left(\frac{p(x, y)}{p(x)p(y)}\right)\right]\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$I$ is to correlation as $H$ is to variance. While correlation measures to what extent $\rX$ and $\rY$ have a &lt;a href=&quot;https://en.wikipedia.org/wiki/Correlation_and_dependence&quot;&gt;linear relationship&lt;/a&gt;, $I$ measures the strength of their statistical dependency. While variance measures average distance from some critical point, $H$ is distance agnostic, i.e. it measures unordered dispersion. Similarly, while statistical correlation measures deviation of the mapping between $\rX$ and $\rY$ from perfectly linear, $I$ is shape agnostic, i.e. it measures unordered causal dependence.&lt;/p&gt;

&lt;p&gt;First off, it is important to point out that $I$ is always non-negative, unlike its pointwise counterpart (proof &lt;a href=&quot;https://math.stackexchange.com/a/159544&quot;&gt;here&lt;/a&gt;) You can see this intuitively by trying to construct an anti-dependent relationship between $\rX$ and $\rY$. On average, $p(x, y)$ would have to be less than the product of their marginals. You can construct individual cases where this is true for a particular $(x, y)$, but to do that, you will have to fill most of the probability table (for 2D joint) with p-mass to compensate. This is reflected in Jensen’s inequality. A direct consequence is $H[\rY] \geq H[\rY \mid \rX]$.&lt;/p&gt;

&lt;p&gt;$I$ being non-negative means you can safely think about it as a measure of information content. In this sense, information is stored in the relationship between $\rX$ and $\rY$.&lt;/p&gt;

&lt;p&gt;Note that by remembering that expectation is linear, some useful identities pop out of the definition above,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
I[\rX, \rY] &amp;= H[\rX] + H[\rY] - H[(\rX,\rY)] \\
  &amp;= H[\rX] - H[\rX \mid \rY] \\
  &amp;= H[\rY] - H[\rY \mid \rX]\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;An intuitive way to think about $I$ is as a continuous measure of &lt;em&gt;bijectivity&lt;/em&gt; of the stochastic function, $g(x) \sim p(\rY \mid \rX = x)$, where $g : X \rightarrow Y$. This is easier to see if we write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I[\rX, \rY] = H[\rY] - H[\rY \mid \rX]\,.&lt;/script&gt;

&lt;p&gt;$H[\rY]$ measures &lt;em&gt;surjectivity&lt;/em&gt;, i.e. how much $g$ spreads out over $Y$ (marginalized over $\rX$). &lt;em&gt;surjective&lt;/em&gt; (a.k.a. onto) in the set-theory sense means that $g$ maps to every element in $Y$. In the statistical sense, $g$ may map to every element in $Y$ with some probability, but to some elements much more frequently than others. We would say $p(y)$ is &lt;em&gt;peaky&lt;/em&gt;, the opposite of spread out. Recall that $H$ measures statistical dispersion. Larger $H[\rY]$ means more even spread of probability mass across all the elements in $Y$. In that sense, it measures how surjective $g$ is.&lt;/p&gt;

&lt;p&gt;$H[\rY \mid \rX]$ measures &lt;em&gt;anti-injectivity&lt;/em&gt;. &lt;em&gt;injective&lt;/em&gt; (a.k.a. one-to-one) in the set-theory sense means that $g$ maps every element in $X$ to a unique element in $Y$. There is no sharing, and you know which $x \in X$ was the input for any $y \in Y$ in the image of $g(X)$. In the statistical sense, $g$ may map a given $x$ to many elements in $Y$, each with some probability, i.e. fan-out. Anti-injective is like a reversal of injective, which is about fan-in. The more $g$ fans-out, the more anti-injective it is. Recall that $H[\rY \mid \rX]$ measures averge uncertainty about $\rY$ given an observation from $\rX$. This is, in a sense, the average statistical fan-out of $g$. Lower $H[\rY \mid \rX]$ means $g$’s output is more concentrated (peaky) on average for a given $x$, and higher means its output is more uniformly spread on average.&lt;/p&gt;

&lt;p&gt;For a function to be a bijection, is needs to be both injective and surjective. $H[\rY]$ may seem like a good continuous proxy for surjectivity, but $H[\rY \mid \rX]$ seems to measure something different from injectivity. Notice that $H[\rY \mid \rX]$ is affected by the injectivity of $g^{-1}$. If $g^{-1}$ maps many $y$s to the same $x$, then we are uncertain about what $g(x)$ should be.&lt;/p&gt;

&lt;p&gt;In general, I claim that $I[\rX, \rY]$ measures how bijective $g$ is. $I[\rX, \rY]$ is maximized when $H[\rY]$ is maximized and $H[\rY \mid \rX]$ is minimized (i.e. 0). That is, when $g$ is maximally surjective and minimally anti-injective, implying it is maximally injective. Higher $I[\rX, \rY]$ actually does indicate that $g$ is more invertible because $I$ is symmetric. It measures how much information can flow through $g$ in either direction.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg&quot; alt=&quot;Useful diagram for keeping track of the relationships between these concepts.&amp;lt;br/&amp;gt;Credit: &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/Mutual_information&amp;quot;&amp;gt;https://en.wikipedia.org/wiki/Mutual_information&amp;lt;/a&amp;gt;&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;Useful diagram for keeping track of the relationships between these concepts.&lt;br /&gt;Credit: &lt;a href=&quot;https://en.wikipedia.org/wiki/Mutual_information&quot;&gt;https://en.wikipedia.org/wiki/Mutual_information&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Useful diagram for keeping track of the relationships between these concepts&lt;/p&gt;

&lt;h3 id=&quot;channel-capacity&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#channel-capacity&quot;&gt;Channel capacity&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;$I$ is determined by $p(x)$ just as much as $p(y \mid x)$, but $g$ has ostensibly nothing to do with $p(x)$. If we want $I$ to measure properties of $g$ in isolation, it should not care about the distribution over its inputs. One solution to this issue is to use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Channel_capacity#Formal_definition&quot;&gt;&lt;strong&gt;capacity&lt;/strong&gt;&lt;/a&gt; of $g$, defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
C[g] &amp;= \sup_{p_X(x)} I[\rX, \rY] \\
  &amp;= \sup_{p_X(x)} \E_{y\sim p_{g(x)}, x \sim p_X}[i(x, y)] \\
  &amp;= \sup_{p_X(x)} \E_{y\sim p_{Y \mid X=x}, x \sim p_X}[-h(y \mid x) + h(x)]\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;In other words, if you don’t have a preference for $p(x)$, choose $p(x)$ which maximizes $I[\rX, \rY]$.&lt;/p&gt;

&lt;h1 id=&quot;shannon-information-for-continuous-distributions&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#shannon-information-for-continuous-distributions&quot;&gt;Shannon Information For Continuous Distributions&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;Up to now we’ve only considered discrete distributions. Describing the information content in continuous distributions and their events is tricky business, and a bit more nuanced than usually portrayed. Let’s explore this.&lt;/p&gt;

&lt;p&gt;For this discussion, let’s consider a random variable $\rX$ with &lt;a href=&quot;https://en.wikipedia.org/wiki/Support_(mathematics)#Support_of_a_distribution&quot;&gt;support&lt;/a&gt; over $\R$. Let $f(x)$ be the probability density function (pdf) of $\rX$.&lt;/p&gt;

&lt;p&gt;Elementary events $x \in \rX$ &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;do not have probabilities perse&lt;/span&gt;&lt;label for=&quot;81ebfb9082ef06c6091bdf79ef229fe6555037fd&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;81ebfb9082ef06c6091bdf79ef229fe6555037fd&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;you could say their probability mass is 0 in the limit&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. Self-information is a function of probability mass, so we should instead compute self-info of events that are intervals (or measurable sets) over $\R$. For example,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
h(a &lt; x &lt; b) &amp;= -\log_2\,p(a &lt; \rX &lt; b)\\
  &amp;= -\log_2\left(\int_a^b f(x) \diff x\right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Conjecture: The entropy of any distributions with uncountable support is infinite. This should make sense, as we now have uncountably many possible outcomes. One observation rules out infinitely many alternatives, so it should contain infinite information. We can see this clearly because the entropy of a uniform distribution over $N$ possibilities is $\log_2 N$ which grows to infinity as $N$ does. On the other hand, a one-hot distribution over $N$ possibilities has 0 entropy, because you will &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;always observe&lt;/span&gt;&lt;label for=&quot;2b312968d2ec3765f14ce161dd47e1212e3f03cc&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;2b312968d2ec3765f14ce161dd47e1212e3f03cc&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Unless you observe an impossible outcome, in which case you gain infinite information!&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; the probability-1 outcome and gain 0 information. So we expect the Dirac-delta distribution to have 0 entropy.&lt;/p&gt;

&lt;p&gt;But wait, the Gaussian distribution is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution&quot;&gt;maximum-entropy&lt;/a&gt; distribution, implying that the entropy of continuous distributions can be numerically compared! People talk about entropy of continuous distributions all the time! What people normally call entropy for continuous distributions is actually &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_entropy&quot;&gt;differential entropy&lt;/a&gt;, which is not the same thing as the $H$ we’ve been working with.&lt;/p&gt;

&lt;p&gt;I’ll show that $H[\rX]$ is infinite when the distribution has continuous support.  https://www.crmarsh.com/static/pdf/Charles_Marsh_Continuous_Entropy.pdf.  To do that, let’s take a &lt;a href=&quot;https://en.wikipedia.org/wiki/Riemann_sum&quot;&gt;Riemann sum&lt;/a&gt; of $f(x)$. Let ${x_i}_{i=-\infty}^\infty$ be a set of points equally spaced by intervals of $\Delta$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
% \def\u{\Delta x}
\def\u{\Delta}
\begin{align}
H[\rX] &amp;= -\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u \log_2\left(f(x_i) \u\right) \\
  &amp;= -\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u \log_2\left(f(x_i)\right) - \lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u \log_2\left(\u\right)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The left term is just the Riemann integral of $f(x)\log_2(f(x))$, which I will define as &lt;strong&gt;differential entropy&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta[f] := -\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u \log_2\left(f(x_i)\right) = -\int_{-\infty}^\infty f(x) \log_2\left(f(x)\right) \diff{x}\,.&lt;/script&gt;

&lt;p&gt;(typically $h$ is used for differential entropy, but I’ve already used it for self-information, so I’m using $\eta$ instead)&lt;/p&gt;

&lt;p&gt;The right term can be simplified using the &lt;a href=&quot;https://tutorial.math.lamar.edu/Classes/CalcI/LimitsProperties.aspx&quot;&gt;limit product rule&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u \log_2\left(\u\right) = -\left(\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u\right)\cdot\left(\lim\limits_{\u \to 0}\log_2\left(\u\right)\right)\,.&lt;/script&gt;

&lt;p&gt;Note that&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u = \int_{-\infty}^\infty f(x) \diff{x} = 1\,,&lt;/script&gt;
because $f(x)$ is a p.d.f.&lt;/p&gt;

&lt;p&gt;Putting it all together we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H[\rX] = \eta[f] - \lim\limits_{\u \to 0}\log_2\left(\u\right)\,.&lt;/script&gt;

&lt;p&gt;$\log_2(\u) \to -\infty$ as $\u \to 0$, so $H[\rX]$ explodes to infinity when $\eta[f]$ is finite, which it is for most well-behaved functions.&lt;/p&gt;

&lt;p&gt;A simple proof that $H$ is finite for continuous distributions with support over an uncountable set: the Riemann sum above will only have at most finitely many non-zero terms as $\Delta \to \infty$.&lt;/p&gt;

&lt;p&gt;Differential entropy is very different from entropy. It can be unboundedly negative. For example, the differential entropy of a Gaussian distribution with variance $\sigma^2$ is $\frac{1}{2}\ln(2\pi e \sigma^2)$. Taking the limit as $\sigma \to 0$, we see the differential entropy of the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;Dirac-delta distribution is $-\infty$&lt;/span&gt;&lt;label for=&quot;db0f66f8ebc12d35b9801d084e4ee4afbd893dc5&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;db0f66f8ebc12d35b9801d084e4ee4afbd893dc5&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Plugging $\eta[f] = -\infty$ into our relation $H[\rX] = \eta[f] - \lim\limits_{\u \to 0}\log_2\left(\u\right)$, we see why entropy of $\delta(x)$ would be 0.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. A notable problem with differential entropy is that its not invariant to change of coordinates, and there is a proposed fix for that: &lt;a href=&quot;https://en.wikipedia.org/wiki/Limiting_density_of_discrete_points&quot;&gt;https://en.wikipedia.org/wiki/Limiting_density_of_discrete_points&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;proof-that-mi-is-fininte-for-continuous-distributions&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#proof-that-mi-is-fininte-for-continuous-distributions&quot;&gt;Proof that MI is fininte for continuous distributions&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;A very nice result is that expected mutual information is finite where entropy would be infinite, so long as there is some amount of noise between the two random variables. This implies that even if physical processes are continuous and contain infinite information, we can only get finite information out of them, because measurement requires establishing a statistical correlation between the measurement device and that system which is necessarily noisy. MI is agnostic to discrete or continuous universes! As long as there is some amount of noise in between a system and your measurement, your measurement will contain finite information about the system.&lt;/p&gt;

&lt;p&gt;The proof follows the same Riemann sum approach from the previous section. I will show that mutual information and differential mutual information are equivalent. Since differential mutual information finite for well behaved functions, so is mutual information!&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
I[\rX, \rY] &amp;= -\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty \sum\limits_{j=-\infty}^\infty f_{XY}(x_i, y_j) \u^2 \log_2\left(\frac{f_{XY}(x_i, y_j)\u^2}{f_X(x_i)\u f_Y(y_i)\u} \right) \\
  &amp;= -\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty \sum\limits_{j=-\infty}^\infty f_{XY}(x_i, y_j) \u^2 \log_2\left(\frac{f_{XY}(x_i, y_j)}{f_X(x_i)f_Y(y_i)} \right) \\
  &amp;= \int_{-\infty}^\infty \int_{-\infty}^\infty f_{XY}(x_i, y_j) \log_2\left(\frac{f_{XY}(x_i, y_j)}{f_X(x_i)f_Y(y_i)} \right) \diff{y}\diff{x}\,
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;because the $\Delta$s cancel inside the log.&lt;/p&gt;

&lt;p&gt;If $p(\rY \mid \rX = x)$ is a Dirac-delta for all $x$, and $p(\rY)$ has continuous support, then $I[\rX, \rY]= H[\rY] - H[\rY \mid \rX] = \infty$ because $H[\rY]=\infty$ and $H[\rY \mid \rX]=0$. Thus some noise between $\rX$ and $\rY$ is required to make the MI finite. It follows that $I[\rX, \rX] = H[\rX] = \infty$ when $\rX$ has continuous support.&lt;/p&gt;

&lt;h1 id=&quot;problems-with-shannon-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#problems-with-shannon-information&quot;&gt;Problems With Shannon Information&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Do the concepts just outlined capture our colloquial understanding of information? Are there situations where they behave differently from how we expect information to behave? I’ll go through some fairly immediate objections to this Shannon’s definition of information, and some remedies.&lt;/p&gt;

&lt;h2 id=&quot;1-tv-static-problem&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#1-tv-static-problem&quot;&gt;1. TV Static Problem&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Imagine a TV displaying static noise. If we assume a fairly uniform distribution over all “static noise” images, we know that the entropy of the TV visuals will be high, because probability mass is spread fairly evenly across all possible images. Each image on average has a very low probability of occurring. According to Shannon, each image then contains a large amount of information.&lt;/p&gt;

&lt;p&gt;That may sound absurd. &lt;a href=&quot;https://en.wikipedia.org/wiki/Noise_(signal_processing)&quot;&gt;Noise&lt;/a&gt;, by some definitions, carries no useful information. Noise is uninformative.  To a human looking at TV static, the information gained is that the TV is not displaying anything. This is a very high level piece of information, but much less than the supposedly high information content of the static itself.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/posts/primer-shannon-information/tv-static.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The resolution here is to define what it means for a human to obtain information. I propose looking at the mutual information between the TV and the viewer’s brain. Let $\rX$ be a random variable over TV images, and $\rZ$ be a random variable over the viewer’s brain states. The support of $\rX$ is the space of all possible TV screens, so static and SpongeBob are just different distributions over the same space. Now, the state of the viewer’s brain is causally connected to what is on the TV screen, but the nature of their visual encoder (visual cortex) determines $p(\rZ \mid \rX)$, and thus $p(\rZ, \rX)$. I would guess that any person who says TV static is uninformative does not retain much detail about the patterns in the static. Basically, that person would just remember that they saw static. What we have here is a region of large fan-in. Many static images are collapsed to a single output for their visual encoder, namely the label “TV noise”. So the information contained in TV static is low to a human, because $I[\rX, \rZ]$ is low when $\rX$ is the distribution of TV static.&lt;/p&gt;

&lt;p&gt;Note that the signal, “TV noise”, is still rather informative, if you consider the space of all possible labels you could assign to the TV screen, e.g. “SpongeBob” or “sitcom”. Further, that you are looking at a TV and not anything else is information.&lt;/p&gt;

&lt;h2 id=&quot;2-shannon-information-is-blind-to-scrambling&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#2-shannon-information-is-blind-to-scrambling&quot;&gt;2. Shannon Information is Blind to Scrambling&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Encryption scrambles information to make it inaccessible to prying eyes. Encryption is usually lossless, meaning the original message is fully recoverable. If $\rX$ is a distribution over messages, then the encryption function Enc should preserve that distribution. To Shannon information, $\rX$ and $\text{Enc}(\rX)$ contain the same information. Shannon information is therefore blind to operations like scrambling which do something interesting to the information present, i.e. like making it accessible or inaccessible.&lt;/p&gt;

&lt;p&gt;The resolution is again mutual information. While permuting message space (or any bijective transformation) does not change information content under Shannon, it changes the useful information content. A human looking at (or otherwise perceiving) a message is creating a casual link between the message and a representation in the brain. This link has mutual information. Likewise, any measurement apparatus establishes a link between physical state and a representation of that state (the measurement result), again establishing mutual information.&lt;/p&gt;

&lt;p&gt;Information in a message becomes inaccessible or useless when the representation of the message cannot distinguish between two messages. Encryption maps the part of message space that human brains can discriminate, i.e. meaningful English sentences (or other such meaningful content) to a part of message space that humans cannot discriminate, i.e. apparently arbitrary character strings. These arbitrary strings appear to be meaningless because they are all mapped to the same or similar representation in our heads, namely the “junk text” label. In short, mutual information between plain text and brain states is much higher than mutual information between encrypted text and brain states.&lt;/p&gt;

&lt;h2 id=&quot;3-deterministic-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#3-deterministic-information&quot;&gt;3. Deterministic information&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;How is data on disk contain information if it is fixed and known? How does the output of a deterministic computer program contain information? How do math proofs contain information? All these things do not have an inherent probability distribution. If there is uncertainty, we might call it &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;logical uncertainty&lt;/span&gt;&lt;label for=&quot;0e18784002a64f3be42ab79f74a4282e0d11c58f&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;0e18784002a64f3be42ab79f74a4282e0d11c58f&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;See &lt;a href=&quot;https://intelligence.org/2016/04/21/two-new-papers-uniform/&quot;&gt;New papers dividing logical uncertainty into two subproblems&lt;/a&gt;&lt;br /&gt;and &lt;a href=&quot;https://golem.ph.utexas.edu/category/2016/09/logical_uncertainty_and_logica.html&quot;&gt;Logical Uncertainty and Logical Induction&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. It is an open question whether logical uncertainty and empirical uncertainty should be conflated, and both brought under the umbrella of probability theory.&lt;/p&gt;

&lt;p&gt;This is similar to asking, how does Shannon information account for what I already know? When I observe a message I didn’t already know it is informative, but what about the information contained in messages I currently have?  It is also an open question whether probability should be considered objective or subjective, and whether quantities of information are objective or subjective. Perhaps you regard a message you have to be informative, because you are implicitly modeling its information w.r.t. some other receiver who has not yet received it.&lt;/p&gt;

&lt;h2 id=&quot;4-if-the-universe-is-continuous-everything-contains-infinite-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#4-if-the-universe-is-continuous-everything-contains-infinite-information&quot;&gt;4. If the universe is continuous everything contains infinite information&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;This one is resolved by the discussion above about mutual information of continuous distributions being finite, so long as there is noise between the two random variables. Thus, in a universe where all measurements are noisy, mutual information is always finite regardless of the underlying meta-physics (whether objects contain finite or infinite information in an absolute sense).&lt;/p&gt;

&lt;h2 id=&quot;5-shannon-information-ignores-the-meaning-of-messages&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#5-shannon-information-ignores-the-meaning-of-messages&quot;&gt;5. Shannon information ignores the meaning of messages&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;There is a competing information theory, &lt;a href=&quot;https://en.wikipedia.org/wiki/Algorithmic_information_theory&quot;&gt;algorithmic information theory&lt;/a&gt; which uses the length of the shortest program that can output a message $x$ as the information measure of $x$, called &lt;a href=&quot;https://en.wikipedia.org/wiki/Kolmogorov_complexity&quot;&gt;Kolmogorov complexity&lt;/a&gt;. If $x$ is less compressible, it contains more information. This is analogous to low $p_X(x)$ leading to its optimal &lt;a href=&quot;https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding&quot;&gt;Shannon-Fano&lt;/a&gt; being longer, and thus containing more information.&lt;/p&gt;

&lt;p&gt;Algorithmic information theory addresses the criticism that $h(x)$ depends only on the probability of $x$, rather than the meaning of $x$. If $x$ is a word, sentence, or even a book, the information content of $x$ supposedly does not depend on what the text is! Algorithmic information theory defines information as a property of the content of $x$ as a string, and drops the dependency on probability.&lt;/p&gt;

&lt;p&gt;I think this criticism does not consider what &lt;em&gt;meaning&lt;/em&gt; is. A steel-man’ed Shannon information at least seems self-consistent to me. Again, the right approach is to use mutual information. I propose that the meaning of a piece of text is ultimately due to the brain state it invokes in you when you read it. Your &lt;a href=&quot;https://www.deeplearningbook.org/contents/representation.html&quot;&gt;representation&lt;/a&gt; of the text shares information with the text. So while yes the probability of $x$ in the void may be meaningless, the joint probability of $(x, z)$ where $z$ is your brain state is what gives $x$ meaning. Also note that condensing a book into one random variable $\rX$ is very misleading, because this distribution factors! A book is a single outcome from a distribution over all strings of characters, and we write this distribution as $p(\rC_i \mid \rC_{i-1}, \ldots, \rC_2, \rC_1)$ where $\rC_i$ is the random variable for the $i$-th character in the book. In this way, each character position contains semantic information in its probability distribution conditioned on the previous character choices. The premise of &lt;a href=&quot;https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf&quot;&gt;language modeling&lt;/a&gt; in machine learning is that the statistical relationships between words (their frequencies of co-occurrence) in a corpus of text &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;determine their meaning&lt;/span&gt;&lt;label for=&quot;420e4613ec913eb3ebc7f105b4ba7df0378bbd7b&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;420e4613ec913eb3ebc7f105b4ba7df0378bbd7b&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The theory goes that a computer which can estimate frequencies of words very precisely would implicitly have to create internal representations of those words which encode their meaning, and so beefed up language modeling is all that is needed for intelligence.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-probability-distributions-are-not-objective&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#6-probability-distributions-are-not-objective&quot;&gt;6. Probability distributions are not objective&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;I touched on this already. Probability has two interpretations: frequentist (objective) and Bayesian (subjective). It is unclear if frequentist probability is an objective property of matter. For repeatable controlled experiments, a frequentist description is reasonable, like in games of chance, and in statistical mechanics and quantum mechanics. When probability is extended to systems that don’t repeat in any meaningful sense, like the stock market or historical events, the objectiveness is dubious. There is a camp that argues probability should reflect the state of belief of an observer, and is more a measurement of the brain doing the observing than the thing being observed.&lt;/p&gt;

&lt;p&gt;So then this leads to an interesting question: is Shannon information a property of a system being observed, or a property of the observer in relation to it (or both together)? Is information objective in the sense that multiple independent parties can do measurements to verify a quantity of information, or is it subjective in the sense that it depends on the beliefs of the person doing the calculating? I am not aware of any answer or consensus on this question for information in general,&lt;/p&gt;

&lt;h1 id=&quot;appendix&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#appendix&quot;&gt;Appendix&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;properties-of-conditional-entropy&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#properties-of-conditional-entropy&quot;&gt;Properties of Conditional Entropy&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Source: https://en.wikipedia.org/wiki/Conditional_entropy#Properties&lt;/p&gt;

&lt;p&gt;$H[\rY \mid \rX] = H[(\rX, \rY)] - H[\rX]$&lt;/p&gt;

&lt;p&gt;Bayes’ rule of conditional entropy:
$H[\rY \mid \rX] = H[\rX \mid \rY] - H[\rX] + H[\rY]$&lt;/p&gt;

&lt;p&gt;Minimum value:
$H[\rY \mid \rX] = 0$ when $p(y \mid x)$ is always deterministic, i.e. one-hot, i.e. $p(y \mid x) \in {0, 1}$ for all $(x, y) \in X \times Y$.&lt;/p&gt;

&lt;p&gt;Maximum value:
$H[\rY \mid \rX] = H[\rY]$ when $\rX, \rY$ are independent.&lt;/p&gt;

&lt;h2 id=&quot;bayes-rule&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#bayes-rule&quot;&gt;Bayes’ Rule&lt;/a&gt;&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y \mid x) = p(x \mid y)p(y)/p(x)&lt;/script&gt;

&lt;p&gt;can be rewritten in terms of self-information:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(y \mid x) = h(x \mid y) + h(y) - h(x)\,.&lt;/script&gt;

&lt;p&gt;The information contained in $y$ given $x$ is proportional to the information contained in $x$ given $y$ plus the information contained in $y$. This is just Bayes’ rule in log-space, but makes it a bit easier to reason about what Bayes’ rule is doing. Whether $y$ is likely in its own right and whether $x$ is likely given $y$ both contribute to the total information.&lt;/p&gt;

&lt;h2 id=&quot;cross-entropy-and-kl-divergence&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#cross-entropy-and-kl-divergence&quot;&gt;Cross Entropy and KL-Divergence&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Unlikely everything we’ve seen so far, these are necessarily functions of probability functions, rather than random variables. Further, these are both comparisons of probability functions over the same support.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(P,Q) = -\sum_x P(x)\log Q(x)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{K}\left(P\,||\,Q\right) = \sum_{x} P(x)\log
{\frac{P(x)}{Q(x)}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{K}\left(P\,||\,Q\right) = H(P,Q) - H(P)&lt;/script&gt;

&lt;p&gt;Sources:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://stats.stackexchange.com/questions/111445/analysis-of-kullback-leibler-divergence&lt;/li&gt;
  &lt;li&gt;https://stats.stackexchange.com/questions/357963/what-is-the-difference-cross-entropy-and-kl-divergence&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Mutual information can be &lt;a href=&quot;https://en.wikipedia.org/wiki/Mutual_information#Relation_to_Kullback%E2%80%93Leibler_divergence&quot;&gt;written in terms of KL-divergence&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I[\rX, \rY] = \mathbb{K}\left(p_{X,Y}\,||\,p_X \cdot p_Y\right)&lt;/script&gt;

&lt;p&gt;where $(p_X \cdot p_Y)(x, y) \mapsto p_X(x) \cdot p_Y(x)$.&lt;/p&gt;

</description>
        <pubDate>Tue, 09 Jun 2020 00:00:00 -0700</pubDate>
        <link>pragmanym.github.io/zhat/articles/primer-shannon-information</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/primer-shannon-information</guid>
        
        
        <category>primer</category>
        
      </item>
    
      <item>
        <title>Wallace: Emergence of particles from QFT - Study Notes</title>
        <description>
</description>
        <pubDate>Fri, 10 Jan 2020 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/wallace-particles-qft</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/wallace-particles-qft</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Visualizing Quantum Field States - Study Notes</title>
        <description>
</description>
        <pubDate>Fri, 10 Jan 2020 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/visualizing-quantum-fields</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/visualizing-quantum-fields</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Solomonoff Induction - Study Notes</title>
        <description>
</description>
        <pubDate>Tue, 31 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/solomonoff-induction</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/solomonoff-induction</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Weak Measurement (Quantum Mechanics) - Study Notes</title>
        <description>
</description>
        <pubDate>Tue, 24 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/weak-measurement</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/weak-measurement</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Topology: Sphere &amp; Torus - Study Notes</title>
        <description>
</description>
        <pubDate>Mon, 23 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/topology-sphere-torus</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/topology-sphere-torus</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Cox's Theorem - Study Notes</title>
        <description>
</description>
        <pubDate>Mon, 23 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/coxs-theorem</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/coxs-theorem</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Quantum State</title>
        <description>&lt;p&gt;The two views of quantum state:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Quantum states are $L^2$-normalized complex-valued functions over classical configuration space.&lt;/li&gt;
  &lt;li&gt;Quantum states are unit vectors residing in a complex Hilbert space, $\mathcal{H}$.&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\newcommand{\bm}{\boldsymbol}
\newcommand{\diff}[1]{\mathop{\mathrm{d}#1}} 
\newcommand{\bra}[1]{\langle#1\rvert}
\newcommand{\ket}[1]{\lvert#1\rangle}
\newcommand{\braket}[2]{\langle#1\vert#2\rangle}&lt;/script&gt;

&lt;!--more--&gt;

&lt;ul class=&quot;toc&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-quantum-states-are-functions&quot; id=&quot;markdown-toc-1-quantum-states-are-functions&quot;&gt;1) Quantum States Are Functions&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#punchline-quantum-states-are-probability-distributions&quot; id=&quot;markdown-toc-punchline-quantum-states-are-probability-distributions&quot;&gt;Punchline: quantum states are probability distributions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-quantum-states-are-vectors&quot; id=&quot;markdown-toc-2-quantum-states-are-vectors&quot;&gt;2) Quantum States Are Vectors&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#measurement-bases&quot; id=&quot;markdown-toc-measurement-bases&quot;&gt;Measurement Bases&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#a-proper-basis&quot; id=&quot;markdown-toc-a-proper-basis&quot;&gt;A Proper Basis&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#punchline-uncertainty-principle-from-change-of-basis&quot; id=&quot;markdown-toc-punchline-uncertainty-principle-from-change-of-basis&quot;&gt;Punchline: Uncertainty Principle From Change Of Basis&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a very short primer on quantum mechanics, and the bare minimum that needs to be explained in order to understand what QM is about. My goal is to provide some conceptual punchlines with minimal prereqs. The formalism of QM is filled with technicalities and complications, which I try to point out where possible. Keep in mind I am glossing over a ton of details.&lt;/p&gt;

&lt;p&gt;Note, throughout I am setting $\hbar = 1$.&lt;/p&gt;

&lt;h1 id=&quot;1-quantum-states-are-functions&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#1-quantum-states-are-functions&quot;&gt;1) Quantum States Are Functions&lt;/a&gt;&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Quantum states are $L^2$-normalized complex-valued functions over classical configuration space.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For our purposes, classical configuration space is position space. A system with $N$ particles in 3D space has $3N$ dimensional configuration space. For simplicity, I’ll mostly talk about one particle in 1D space, i.e. 1D configuration space.&lt;/p&gt;

&lt;p&gt;So to restate, a quantum state is a function,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi : \mathbb{R}^n \rightarrow \mathbb{C}\,.&lt;/script&gt;

&lt;p&gt;for $n$-dimensional configuration space such that $\lvert\psi\rvert = 1$, i.e. $\psi$ is $L^2$-normalized.&lt;/p&gt;

&lt;p&gt;The $L^2$-norm of a complex-valued function is given as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert\psi\rvert = \sqrt{\int_{\mathbb{R}^n} \psi(x)\bar{\psi}(x)\diff{x
^n}}\,,&lt;/script&gt;

&lt;p&gt;where $\bar{f}(x) = \text{Re}[f(x)] - i\,\text{Im}[f(x)]$ is the complex conjugate.&lt;/p&gt;

&lt;h2 id=&quot;punchline-quantum-states-are-probability-distributions&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#punchline-quantum-states-are-probability-distributions&quot;&gt;Punchline: quantum states are probability distributions&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;You probably already guessed that $\psi$ encodes the probability of finding particles in space. The probability of observing system $\psi$ to be in &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;some region of configuration space&lt;/span&gt;&lt;label for=&quot;ec51e042e3a91e03088eeebfa8e927d02ca43c07&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ec51e042e3a91e03088eeebfa8e927d02ca43c07&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Remember that $n$ is the total number of spatial coordinates that describes the system. We are calculating the probability for all the coordinates together to be in some range.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; $D \subseteq \mathbb{R}^n$ is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_D  \psi(x) \bar{\psi}(x)\diff{x^n}\,.&lt;/script&gt;

&lt;p&gt;$\psi(x) \bar{\psi}(x) = \lvert\psi(x)\rvert^2$ is the absolute value squared of the complex value $\psi(x)$ (not to be confused with the function-norm $\lvert\psi\rvert$). In other words, $\lvert\psi(x)\rvert^2$ is a probability density. $\psi(x)$ is called a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;&lt;em&gt;probability amplitude&lt;/em&gt;&lt;/span&gt;&lt;label for=&quot;8fd9acc0f5baccc0d4f8236ac8210c8d7555b801&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;8fd9acc0f5baccc0d4f8236ac8210c8d7555b801&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;“Probability amplitude” refers specifically to a complex number whose square is a probability. I don’t think the word “amplitude” makes this apparent and it is rather ambiguous, but it’s handy to have a term to refer to these not-quite-probabilities and that is the convention.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-quantum-states-are-vectors&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#2-quantum-states-are-vectors&quot;&gt;2) Quantum States Are Vectors&lt;/a&gt;&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Quantum states are unit vectors residing in a complex Hilbert space, $\mathcal{H}$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;&lt;em&gt;Hilbert space&lt;/em&gt;&lt;/span&gt;&lt;label for=&quot;da5780db3d541999f81e9889c2af05f261b22854&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;da5780db3d541999f81e9889c2af05f261b22854&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The definition of Hilbert space is surprisingly ambiguous or too technical in most sources. For a while I was confused about what was special about Hilbert spaces in contrast with typical vector spaces. The axioms of linear algebra &lt;a href=&quot;https://math.stackexchange.com/a/28876&quot;&gt;allow for complex-valued scalars&lt;/a&gt;. The other notable feature of Hilbert space is that it allows for infinite dimensions, but that doesn’t make it unique. There are many ways to construct infinite dimensional vector spaces, e.g. &lt;a href=&quot;https://math.stackexchange.com/a/466741&quot;&gt;this answer&lt;/a&gt;. The distinguishing feature of Hilbert space is that it is &lt;a href=&quot;https://en.wikipedia.org/wiki/Complete_metric_space&quot;&gt;&lt;em&gt;complete&lt;/em&gt;&lt;/a&gt;, meaning that all &lt;a href=&quot;https://en.wikipedia.org/wiki/Cauchy_sequence&quot;&gt;Cauchy sequences&lt;/a&gt; of vectors in Hilbert space converge to vectors in Hilbert space. This ensures we can do calculus. It is true that all finite dimensional inner product spaces are Hilbert spaces, and they are simultaneously many other types of spaces, e.g. Banach spaces. Finite dimensional vector spaces do show up in quantum mechanics, and they are still called Hilbert spaces for consistency. Further reading: &lt;a href=&quot;https://www.quantiki.org/wiki/hilbert-spaces&quot;&gt;A&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Hilbert_space#History&quot;&gt;B&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is a generalization of Euclidean vector space to real or complex-valued vector spaces, with finite or infinite dimensions. The generalization of the Euclidean inner-product for functions is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle f, g \rangle = \int_{\mathbb{R}^n} f(x)\bar{g}(x)\diff{x^n}\,.&lt;/script&gt;

&lt;p&gt;Let $\mathcal{H}$ be a Hilbert space of $L^2$-normalized complex-valued functions. Then any $\psi \in \mathcal{H}$ is a unit-vector, since $\lvert\psi\rvert^2 = \langle \psi, \psi \rangle = 1$.&lt;/p&gt;

&lt;h2 id=&quot;measurement-bases&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#measurement-bases&quot;&gt;Measurement Bases&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Given that the elements of $\mathcal{H}$ are functions over $x$, one straightforward way to choose a basis for $\mathcal{H}$ is to think of $\psi \in \mathcal{H}$ as a vector with uncountably many entries, one for each input $x$. This is achieved with a basis of &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function&quot;&gt;Dirac delta functions&lt;/a&gt;&lt;/span&gt;&lt;label for=&quot;68625e91f114b5de914c4a1ae96b612e8a90302e&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;68625e91f114b5de914c4a1ae96b612e8a90302e&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;0 everywhere except for the origin. Defined so that its total area is 1. Technically they are not functions, but &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function#As_a_distribution&quot;&gt;distributions&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. Writing $\psi$ in this basis,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\psi &amp;= \int_{\mathbb{R}^n} \psi(\chi)\cdot\left[ x \mapsto \delta(x-\chi)\right]\diff{\chi^n} \\
     &amp;= x \mapsto \int_{\mathbb{R}^n} \psi(\chi)\delta(x-\chi)\diff{\chi^n} \\
     &amp;= x \mapsto \psi(x)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;This integral is an uncountable linear combination, where &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;$x \mapsto \delta(x-\chi)$&lt;/span&gt;&lt;label for=&quot;6e2c209cbdb6c596f74638233310e8ffd60cc39e&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;6e2c209cbdb6c596f74638233310e8ffd60cc39e&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Note I am using non-standard notation. “$x \mapsto f(x)$” instantiates a function object in-line, just as “$\{f(y) \mid \text{condition}[y]\}$” instantiates a set in-line.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the $\chi$-th basis element and $\psi(\chi)$ is the corresponding coefficient.&lt;/p&gt;

&lt;p&gt;This basis of Dirac deltas corresponds to position measurement. Particles and systems of particles have other measurable properties, like momentum and total energy, which have their own bases. More on that below.&lt;/p&gt;

&lt;h2 id=&quot;a-proper-basis&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#a-proper-basis&quot;&gt;A Proper Basis&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;There is a problem here. $b_\chi = x \mapsto \delta(x-\chi)$ is not $L^2$-normalizable, because $\lvert b_\chi\rvert^2 = \int_{\mathbb{R}^n} \delta(x-\chi)^2\diff{x^n}$ is undefined. Such a basis that is not actually contained in our vector space is sometimes called an &lt;em&gt;improper basis&lt;/em&gt;. This can be remedied by augmenting $\mathcal{H}$ to include objects like Dirac deltas. The resulting construction is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Rigged_Hilbert_space&quot;&gt;rigged Hilbert space&lt;/a&gt;. However, this is not an ideal solution because &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;now we have quantum states&lt;/span&gt;&lt;label for=&quot;0f554cb904b48367e4f303e35eba33200371deb9&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;0f554cb904b48367e4f303e35eba33200371deb9&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;We will soon see that we want our Hilbert space to be closed under Fourier transform, so if Dirac deltas are included, then so should sin waves of the form $x \mapsto e^{ikx}$, which are also not $L^2$-normalizable.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; which cannot be interpreted as probability distributions.&lt;/p&gt;

&lt;p&gt;There are plenty of valid orthonormal bases for the Hilbert space of &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;unit vectors&lt;/span&gt;&lt;label for=&quot;a407fae48b39bd45ead1aeb29b1a295c009b4471&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;a407fae48b39bd45ead1aeb29b1a295c009b4471&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;And also $L^2$-normalized functions.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. You may be surprised to learn that such bases are necessarily countable. One of the most well known is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_harmonic_oscillator#Hamiltonian_and_energy_eigenstates&quot;&gt;measurement basis for the total energy of the harmonic oscillator&lt;/a&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{B} = \{x \mapsto(-1)^n(2^nn!\sqrt{\pi})^{-\frac{1}{2}}e^{\frac{x^2}{2}}\frac{\diff{^n}}{\diff{x^n}}\left[e^{-x^2}\right]\,\mid\, n \in \mathbb{N}\}\,.&lt;/script&gt;

&lt;p&gt;Symbolically these functions look quite complicated, but you can get a sense for the pattern of what these functions look like as $n$ increases:
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Herm5.svg/900px-Herm5.svg.png&quot; alt=&quot;&quot; /&gt;
*Plot of first 6 basis functions: 0 (black), 1 (red), 2 (blue), 3 (yellow), 4 (green), and 5 (magenta). Note that these are real-valued functions. Source: &lt;a href=&quot;https://en.wikipedia.org/wiki/Hermite_polynomials#Hermite_functions&quot;&gt;https://en.wikipedia.org/wiki/Hermite_polynomials#Hermite_functions&lt;/a&gt;.
 *&lt;/p&gt;

&lt;p&gt;It’s amazing that can write any function in $\mathcal{H}$ as a linear combination of the functions in $\mathcal{B}$, i.e. any continuous complex-valued $L^2$-normalized function can be written as a weighted sum of these functions. We can also use $\mathcal{B}$ to explicitly construct our $L^2$-normalized Hilbert space,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{H} = \{c_0H_0(x) + c_1H_1(x) + c_2H_2(x) + \ldots \,|\, c_0,c_1,c_2,\ldots\in\mathbb{C}\}\,,&lt;/script&gt;

&lt;p&gt;where $H_n(x) \in \mathcal{B}$ is the $n$-th &lt;a href=&quot;https://en.wikipedia.org/wiki/Hermite_polynomials#Definition&quot;&gt;Hermite function&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;punchline-uncertainty-principle-from-change-of-basis&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#punchline-uncertainty-principle-from-change-of-basis&quot;&gt;Punchline: Uncertainty Principle From Change Of Basis&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Under the vector view, the coefficients of a quantum state in some measurement basis are &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;probability amplitudes&lt;/span&gt;&lt;label for=&quot;da0f2680d7f9c26d8d21b75aee55f3fa4e0dbc0d&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;da0f2680d7f9c26d8d21b75aee55f3fa4e0dbc0d&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Their absolute squares are probabilities or probability densities.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. The basis vectors are called &lt;em&gt;definite&lt;/em&gt; states, and their linear combinations are indefinite. So if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi = c_1 B_1 + c_2 B_2 + \ldots\,&lt;/script&gt;

&lt;p&gt;for measurement basis $\{B_1, B_2, \ldots\}$, when the corresponding property of $\psi$ is measured, the system will instantaneously jump to one of $B_i$ with probability $\lvert c_i\rvert^2$. This instantaneous jump is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Wave_function_collapse&quot;&gt;&lt;em&gt;wave-function collapse&lt;/em&gt;&lt;/a&gt;. The experimenter will also know which of the basis states the system is in because there is a corresponding measurement readout for each (unless the measurement is degenerate). In the position basis, we recover the probability interpretation of states as functions.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Superposition&lt;/em&gt; is the name given to the physical phenomenon of a system being in an indefinite state, meaning that when you observe some property the outcome is probabilistic. Mathematically superposition is modeled by linear combinations of definite states. A key facet of quantum mechanics is that multiple measurable properties of a system cannot be &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;simultaneously definite&lt;/span&gt;&lt;label for=&quot;7066f1de8a35a976b9f072db9e312f0a6ba14845&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;7066f1de8a35a976b9f072db9e312f0a6ba14845&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Aside from so-called &lt;a href=&quot;https://en.wikipedia.org/wiki/Complete_set_of_commuting_observables&quot;&gt;commuting observables&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. We get this phenomenon for free out of the framework of linear algebra, by representing properties to be measured as orthonormal bases in Hilbert space.&lt;/p&gt;

&lt;p&gt;The famous example is position and momentum. Both these properties of a particle cannot be known at the same time. We saw that the position (improper) basis for one spatial coordinate is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{B}_\text{x} = \{x \mapsto \delta(x - \chi) \mid \chi \in \mathbb{R}\}\,.&lt;/script&gt;

&lt;p&gt;The momentum basis is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Position_and_momentum_space#Relation_between_space_and_reciprocal_space&quot;&gt;Fourier conjugate&lt;/a&gt; of the position basis, given as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{B}_\text{p} = \{x \mapsto \frac{1}{\sqrt{2\pi}}e^{ipx} \mid p \in \mathbb{R}\}\,.&lt;/script&gt;

&lt;p&gt;The real part of these functions are sin waves. An element $[x \mapsto \delta(x - \chi)] \in \mathcal{B}_\text{x}$ of the position basis can be written as a linear combination of momentum basis elements,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
[x \mapsto \delta(x - \chi)] 
  &amp;= \int_\mathbb{R} \frac{1}{\sqrt{2\pi}}e^{-ip\chi} \cdot \vec{b}_p \diff{p} \\
  &amp;= \int_\mathbb{R} \frac{1}{\sqrt{2\pi}}e^{-ip\chi} \cdot \left[x \mapsto \frac{1}{\sqrt{2\pi}}e^{ipx}\right] \diff{p} \\
  &amp;= x \mapsto \int_\mathbb{R} \frac{1}{\sqrt{2\pi}}e^{-ip\chi} \frac{1}{\sqrt{2\pi}}e^{ipx} \diff{p}
\end{align}\,. %]]&gt;&lt;/script&gt;

&lt;p&gt;In the momentum basis, a position basis element becomes a sine wave, i.e. the coefficient of the $p$-th momentum basis element is $\frac{1}{\sqrt{2\pi}}e^{-ip\chi}$.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Heisenberg uncertainty principle&lt;/em&gt;, informally, states that a quantum state which is more localized in space (peakier distribution in the position basis) is necessarily less localized in momentum space (spread out distribution in the momentum basis), and vice versa. This uncertainty principle is not an extra assertion on top of quantum mechanics, but is a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;consequence of the Fourier transform&lt;/span&gt;&lt;label for=&quot;a49269f8c12334e2ed9b8c18b484bc3255faf867&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;a49269f8c12334e2ed9b8c18b484bc3255faf867&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The same uncertainty principle is a problem when converting audio recordings to sheet music, as the frequency resolution of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectrogram&quot;&gt;spectrogram&lt;/a&gt; is inversely proportional to the time resolution. In other words, you cannot know the exact frequency of a fast changing sound due to the same uncertainty principle. Here time plays the role of space. &lt;a href=&quot;https://lts2.epfl.ch/blog/gbr/2015/05/08/uncertainty-principle-in-quantum-physics-and-signal-processing/&quot;&gt;Further reading.&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. More generally, there is an uncertainty principle between any two measurable properties with &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;different measurement bases&lt;/span&gt;&lt;label for=&quot;9acabeda9899d14b3b917f753be9cfa784d0a28c&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;9acabeda9899d14b3b917f753be9cfa784d0a28c&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Technically this is not true, as some properties can be &lt;a href=&quot;https://en.wikipedia.org/wiki/Complete_set_of_commuting_observables&quot;&gt;observed simultaneously&lt;/a&gt;. How this happens is out of the scope of this post, but essentially, when measurement readouts correspond to multiple basis elements the system will collapse to a superposition of those elements, i.e. it will be projected onto their span. A second measurement of a “compatible” property will further collapse the system to a single basis element.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;This definition of quantum state is very helpful for visualizing what your system looks like in physical space. The vector definition is unfortunately more abstract and can often obscure the connection to physical space. The vector definition has its own benefits. In my opinion, knowing how to go between both definitions is optimal for understanding.&lt;/p&gt;

&lt;p&gt;I didn’t touch on how quantization comes into play, i.e. discrete measurement outcomes. In short, discrete measurement bases result in discrete measurement readouts. For example, total energy will be discrete for particles in potential wells, and continuous for free particles. What determines whether the measurement basis will be continuous or discrete, and how to derive these bases all together, is a complicated matter that gets into &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_theory&quot;&gt;spectral theory of linear operators&lt;/a&gt;. Maybe a topic of a future post.&lt;/p&gt;

&lt;p&gt;It’s important to point out is that &lt;a href=&quot;https://en.wikipedia.org/wiki/Measurement_in_quantum_mechanics&quot;&gt;measurement in quantum mechanics&lt;/a&gt; is not explained, but &lt;a href=&quot;https://en.wikipedia.org/wiki/Wave_function_collapse#History_and_context&quot;&gt;taken as given&lt;/a&gt;. The non-determinism of measurement outcomes is part of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac%E2%80%93von_Neumann_axioms&quot;&gt;axioms of quantum mechanics&lt;/a&gt;, at least under the Copenhagen interpretation. There are alternative interpretations which deny wave-function collapse, notably &lt;a href=&quot;https://en.wikipedia.org/wiki/Many-worlds_interpretation&quot;&gt;pilot-wave&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Many-worlds_interpretation&quot;&gt;many-worlds&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Sun, 22 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/quantum-minimal-conceptual-unit</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/quantum-minimal-conceptual-unit</guid>
        
        
        <category>Brain Dump</category>
        
      </item>
    
      <item>
        <title>Bias-Variance Decomposition For Machine Learning</title>
        <description>&lt;script type=&quot;math/tex; mode=display&quot;&gt;\newcommand{\Real}{ {\mathbb{R}} }
\newcommand{\E}{ {\mathbb{E}} }
\newcommand{\V}{ {\mathbb{V}} }
\newcommand{\D}{\mathcal{D}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand\Yh{ {\hat{Y}} }
\newcommand{\ep}{ {\boldsymbol{\varepsilon}} }
\newcommand{\s}{\mathbb{S}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}&lt;/script&gt;

&lt;p&gt;All about the bias-variance decomposition as it pertains to machine learning. All you need to know:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp; \E_D[(f(x; D) - y(x))^2] \qquad\quad\ \textrm{Avg. error}\\
&amp; = (\E_D[f(x; D)] - y(x))^2 \qquad  \textrm{Bias}_y(f)^2\\
&amp;\phantom{=}\, + \V_D[f(x; D)]     \qquad\qquad\quad\ \, \textrm{Variance}(f)\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;!--more--&gt;

&lt;ul class=&quot;toc&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#preamble&quot; id=&quot;markdown-toc-preamble&quot;&gt;Preamble&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#context&quot; id=&quot;markdown-toc-context&quot;&gt;Context&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bias-variance-decomposition-for-ℓ2-loss&quot; id=&quot;markdown-toc-bias-variance-decomposition-for-ℓ2-loss&quot;&gt;Bias-Variance Decomposition For ℓ2 Loss&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#symbol-breakdown&quot; id=&quot;markdown-toc-symbol-breakdown&quot;&gt;Symbol breakdown&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#d-for-dataset&quot; id=&quot;markdown-toc-d-for-dataset&quot;&gt;$D$ for dataset&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#test-on-x&quot; id=&quot;markdown-toc-test-on-x&quot;&gt;Test on $x$&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#label-with-y&quot; id=&quot;markdown-toc-label-with-y&quot;&gt;Label with $y$&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#f-is-the-model--training-algo&quot; id=&quot;markdown-toc-f-is-the-model--training-algo&quot;&gt;$f$ is the model + training algo&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#an-illustration&quot; id=&quot;markdown-toc-an-illustration&quot;&gt;An Illustration&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#flexibility-is-broken&quot; id=&quot;markdown-toc-flexibility-is-broken&quot;&gt;Flexibility is broken&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bias-variance-decomposition-for-any-loss&quot; id=&quot;markdown-toc-bias-variance-decomposition-for-any-loss&quot;&gt;Bias-Variance Decomposition For Any Loss&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#what-is-variance-anyway&quot; id=&quot;markdown-toc-what-is-variance-anyway&quot;&gt;What is variance anyway?&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#acknowledgments&quot; id=&quot;markdown-toc-acknowledgments&quot;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;preamble&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#preamble&quot;&gt;Preamble&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;Why am I writing this? In short, because I think there should be an equation somewhere out there that defines the bias-variance decomposition unambiguously and in complete generality.&lt;/p&gt;

&lt;p&gt;You’ve likely heard of the bias-variance trade-off. There is not always a trade-off, hence I call it a &lt;em&gt;decomposition&lt;/em&gt;. You may think that the trade-off is about trading model flexibility for bias. Probe your understanding and ask yourself, does flexibility always lead to variance? How does flexibility lead to variance? What is varying? The model? The dataset? The variance comes from the model’s interaction with the training data, rather than these things in isolation.&lt;/p&gt;

&lt;p&gt;When you picture this &lt;em&gt;trade-off&lt;/em&gt;, what comes to mind? A &lt;a href=&quot;http://scott.fortmann-roe.com/docs/BiasVariance.html&quot;&gt;dart-board&lt;/a&gt;? A &lt;a href=&quot;https://books.google.com/books?id=NZP6AQAAQBAJ&amp;amp;lpg=PA22&amp;amp;vq=wiggly&amp;amp;pg=PA22#q=wiggly&quot;&gt;wiggly&lt;/a&gt; or rigid function? Those are metaphors - approximations of the concept. Ask yourself, why do the dart throws have bias and variance? How does “wiggliness” contribute to variance? Do some types of wiggliness have less variance than others? These analogies don’t include the source of randomness.&lt;/p&gt;

&lt;p&gt;My life is easiest when I am presented with a formal definition. I can then pull nuances out of it, and refer to it to resolve my confusions. My understanding of an idea is as good as its most precise definition. An equation is the bedrock of intuitive understanding.&lt;/p&gt;

&lt;p&gt;The power of math is that it is precise, but that precision is lost when an equation has multiple meanings. This ambiguity is only possible when authors take notational shortcuts for visual simplicity and assume the reader can infer the missing information. I prefer a precise equation accompanied by exposition.&lt;/p&gt;

&lt;p&gt;So why is this post so long? I could just give the definition (which I did below) and be done with it, but, 1) it’s valuable to unpack the meaning of the equation and explore tricky cases, and, 2) this topic is not so simple, as I discovered while researching this. A truly general definition of the BV-decomposition requires getting abstract… though obscure it’s interesting. However, the special case for ℓ2 loss is all you need to understand and apply this topic.&lt;/p&gt;

&lt;h1 id=&quot;context&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#context&quot;&gt;Context&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;If you found this post with zero context, the bias-variance (BV) decomposition is a mathematical identity stating that expected &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;prediction error&lt;/span&gt;&lt;label for=&quot;5c28819791baf6fa2e6ae991dce4e20db3b229c0&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;5c28819791baf6fa2e6ae991dce4e20db3b229c0&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;On an evaluation/test dataset.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; of a model equals bias-squared plus variance. The precise meaning of these words is best understood by looking at the equation below.&lt;/p&gt;

&lt;p&gt;BV-decomposition is a commonly used idea in machine learning and statistics. Understanding it is essential for communicating with researchers and practitioners. The decomposition (often a trade-off) is used for reasoning about model design. Tons of textbooks and papers give examples of this, e.g. &lt;a href=&quot;https://books.google.com/books?id=tVIjmNS3Ob8C&amp;amp;q=bias-variance#v=onepage&amp;amp;q=7.3.1%20Example%20bias-variance%20tradeoff&amp;amp;f=false&quot;&gt;Elements of Statistical Learning&lt;/a&gt;, or &lt;a href=&quot;https://arxiv.org/pdf/1301.2315.pdf&quot;&gt;reinforcement learning literature&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;bias-variance-decomposition-for-ℓ2-loss&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#bias-variance-decomposition-for-ℓ2-loss&quot;&gt;Bias-Variance Decomposition For ℓ2 Loss&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;This discussion will assume &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;supervised data&lt;/span&gt;&lt;label for=&quot;efbaae27692066e22f22f83d1ec9ef236f2a1a1c&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;efbaae27692066e22f22f83d1ec9ef236f2a1a1c&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;I have not seen BV-decomposition for unsupervised models, but my guess is that you can treat it as a supervised problem where $y = p(x)$.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, e.g. pairs $(x, y)$ where $x$ is an input and $y$ is the observed output. &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;ℓ2&lt;/span&gt;&lt;label for=&quot;5b53813f2890dea036468713f6034958e33025d5&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;5b53813f2890dea036468713f6034958e33025d5&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Usually noted as $L^2$. Presumably originating from &lt;a href=&quot;https://en.wikipedia.org/wiki/Lp_space#Definition&quot;&gt;$L^p$-norm&lt;/a&gt;. I’m using curly ℓ to avoid confusion when $L$ denotes any loss function. See the &lt;a href=&quot;#bias-variance-decomposition-for-any-loss&quot;&gt;Any-Loss section&lt;/a&gt; below.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;loss&lt;/span&gt;&lt;label for=&quot;06b82caf79d08d6c5b8eb70db4ddca09885fc10a&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;06b82caf79d08d6c5b8eb70db4ddca09885fc10a&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Loss is machine learning jargon for error function.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, a.k.a. squared error, is the squared difference between a prediction and the observed output.&lt;/p&gt;

&lt;p&gt;The bias-variance decomposition comes from statistics, which considers the &lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator#Mean_squared_error&quot;&gt;error of a parameter estimator&lt;/a&gt;. In ML, we are concerned with the error of a model’s prediction (output) and a given target output, which we call the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;label&lt;/span&gt;&lt;label for=&quot;5720ea25baeabd25f3c70af8dfd57e178177986f&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;5720ea25baeabd25f3c70af8dfd57e178177986f&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;&lt;em&gt;label&lt;/em&gt; originates from classification where the model is predicting a nominal value, e.g. category/class. Here I use &lt;em&gt;label&lt;/em&gt; to mean to anything that is predicted, and I assume the label is real valued. I just need a word that refers to $y$ in $(x, y)$, including the given/observed and model’s prediction.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; (a scalar value). The bias-variance decomposition is a mathematical identity stating that expected test loss (prediction error) equals bias-squared plus variance across training datasets. For ℓ2 loss &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;we have,&lt;/span&gt;&lt;label for=&quot;9b941cfe0481999bd29f622bafc5ffa18254b108&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;9b941cfe0481999bd29f622bafc5ffa18254b108&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Adapted from the equation in &lt;a href=&quot;http://www.dam.brown.edu/people/geman/Homepage/Essays%20and%20ideas%20about%20neurobiology/bias-variance.pdf&quot;&gt;“Neural networks and the bias/variance dilemma”&lt;/a&gt;, Geman, S., Bienenstock, E., and Doursat, R. (1992)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;  &lt;!-- citation --&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp; \E_D[(f(x; D) - y(x))^2] \qquad\quad\ \textrm{Avg. error}\\
&amp; = (\E_D[f(x; D)] - y(x))^2 \qquad  \textrm{Bias}_y(f)^2\\
&amp;\phantom{=}\, + \V_D[f(x; D)]     \qquad\qquad\quad\ \, \textrm{Variance}(f)\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;I am leaving out the derivation of the BV-decomposition, as it can be easily found in a number of sources, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation&quot;&gt;Wikipedia&lt;/a&gt;. My purpose here is to clear up common points of confusion.&lt;/p&gt;

&lt;p&gt;For completion, here is the definition of $\E$ and $\V$. Let $g:\Real \rightarrow \Real$ be an arbitrary (deterministic) function over the reals, and $Z$ be an arbitrary random variable over domain $D_Z \subseteq \Real$. Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E_Z[g(Z)] = \int_{D_Z} p_Z(z) g(z) dz&lt;/script&gt;

&lt;p&gt;is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Expected_value&quot;&gt;expected value&lt;/a&gt; of $g(Z)$, and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\V_Z[g(Z)] = \E_X[(g(Z) - \E_X[g(Z)])^2]&lt;/script&gt;

&lt;p&gt;is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance#Definition&quot;&gt;variance&lt;/a&gt; of $g(Z)$.&lt;/p&gt;

&lt;h2 id=&quot;symbol-breakdown&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#symbol-breakdown&quot;&gt;Symbol breakdown&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;d-for-dataset&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#d-for-dataset&quot;&gt;$D$ for dataset&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;$D$ is a random variable over all possible &lt;strong&gt;training&lt;/strong&gt; datasets. Technically, this equation does not make any assumptions about the nature of the training data, but generally $\{(x_1,y_1),(x_2,y_2), \ldots,(x_n,y_n)\}$ is a sample from $D$, where $(x_i,y_i)$ is an input, $x_i$, with its observed label, $y_i$. Additionally, each $(x_i,y_i)$ is typically assumed to be sampled i.i.d., though that is also not required here either. The dataset size $n$ may be stochastic as well.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;first&lt;/strong&gt; big point of confusion surrounding BV-decomposition is that the training data comes from a distribution. In practice, you have a fixed dataset, so how does it make sense to take the expectation over all possible training datasets? This equation wants you to imagine that you can draw as many different dataset as you want from the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;data generating process&lt;/span&gt;&lt;label for=&quot;0601b0bf148eaab97dd51194673e2d06a20ea6cb&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;0601b0bf148eaab97dd51194673e2d06a20ea6cb&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;whatever physical process produced your current training dataset&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. That could mean, for example, having photographers go out and take &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;new photos and label the objects&lt;/span&gt;&lt;label for=&quot;e41db9272d77540c2561a40e3e500274bb9d97b4&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;e41db9272d77540c2561a40e3e500274bb9d97b4&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Photographer + camera + environment + labeler is the physical system that generates the data. You can regard the process as inducing a probability distribution $p(x, y)$. Technically the physical setup would need to be exactly the same each time a photo is taken to sample i.i.d. from $p(x, y)$, but uncorrelated differences in photo taking are fine, e.g. many photographers who have their own styles. I do acknowledge that nothing is truly i.i.d. in reality.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, or the population census retaken. Redrawing a new training set likely won’t happen in reality, but this is the correct conceptual understanding.&lt;/p&gt;

&lt;h3 id=&quot;test-on-x&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#test-on-x&quot;&gt;Test on $x$&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;$x$ is a fixed &lt;strong&gt;test&lt;/strong&gt; input. Our loss $(f(x; D) - y(x))^2$ is measuring test error on $x$. However, we take the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;expectation of this test error&lt;/span&gt;&lt;label for=&quot;33aca9ddf266bcb2f8c9b30aed5842d7b704e846&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;33aca9ddf266bcb2f8c9b30aed5842d7b704e846&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;$\E_D[(f(x; D) - y(x))^2]$&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; over &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;all possible training datasets $D$&lt;/span&gt;&lt;label for=&quot;ba7efec6a7dec33e15a9327fe626526569d63f61&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ba7efec6a7dec33e15a9327fe626526569d63f61&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;$x$ may appear in some of them&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. What $x$ to test on is application dependent, and need not be specified here. It’s important to note that $x$ is held fixed, and not a random variable. This test error equals the model’s bias plus variance on a static input, across all training sets.&lt;/p&gt;

&lt;p&gt;Rather than a single test input, you could consider a set of test inputs, $\{x_1,x_2,\ldots,x_m\}$. In which case you are interested in the average loss across the fixed inputs: loss = $\frac{1}{M}\sum_{i=1}^{m} \E_D[(f(x_i; D) - y(x_i))^2]$.&lt;/p&gt;

&lt;p&gt;Alternatively you could consider a random variable over all possible test inputs, $X$, and take the expectation test error: loss = $\E_X[\E_D[(f(X; D) - y(X))^2]]$.&lt;/p&gt;

&lt;p&gt;The underlying identity is the same in all cases.&lt;/p&gt;

&lt;h3 id=&quot;label-with-y&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#label-with-y&quot;&gt;Label with $y$&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I am assuming a deterministic labeling function $y(x)$ for input $x$. It provides the observed label, a.k.a. &lt;a href=&quot;https://en.wikipedia.org/wiki/Ground_truth&quot;&gt;ground truth&lt;/a&gt;. In practice, the labeling process is also stochastic, and there may not even be a clear &lt;em&gt;truth&lt;/em&gt; of the matter. The more general case of the BV-decomposition uses $Y(x)$, where $Y$ is a random variable with conditional distribution, $p_{Y \mid X}(Y \mid X=x)$. $x$ is still a fixed constant here, but could be made a random variable as well if need be.&lt;/p&gt;

&lt;p&gt;When $Y(x)$ is stochastic, we get an extra term in the decomposition:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \E_{Y(x)}[\E_D[(f(x; D) - Y(x))^2]] \\
&amp; = (\E_D[f(x; D)] - \E_{Y(x)}[Y(x)])^2 \qquad\,   \textrm{Bias}_y(f)^2\\
&amp;\phantom{=}\, + \V_D[f(x; D)]   \qquad\qquad\qquad\qquad\ \ \,  \textrm{Variance}(f)\\
&amp;\phantom{=}\, + \V_{Y(x)}[Y(x)]  \qquad\qquad\qquad\qquad\ \ \,   \textrm{Variance}(y)\textrm{, i.e. “noise&quot;}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Presumably the human model builder cannot control $\V_{Y(x)}[Y(x)]$, so this is the lower bound on their model’s loss. If bias and variance were reduced to 0, this term would remain.&lt;/p&gt;

&lt;h3 id=&quot;f-is-the-model--training-algo&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#f-is-the-model--training-algo&quot;&gt;$f$ is the model + training algo&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;$f$ is both the model and training algorithm. $f(x; d)$ is a function of input $x$, and a training dataset $d$. $f$ outputs its prediction for $x$, given that it was trained on the dataset. The colon is commonly used in ML to separate model inputs with parameter or training inputs. You can think of $f(d)=f’$ as a outputting another function $f’$ which makes the prediction, $f’(x)$.&lt;/p&gt;

&lt;p&gt;This formulation does not make reference to model parameters, and leaves open the possibility of non-parametric models. The BV-decomposition is a universal property, and the process that goes from dataset to predictive model is irrelevant. I am assuming $f$ is deterministic w.r.t. both $x$ and $d$. However, if any of its inputs are random variables, the output is a random variable. So $f(x; D)$ is a random variable, as well as $f(X; d)$.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;second&lt;/strong&gt; big point of confusion in the BV-decomposition is that the model’s output distribution is the result of applying the model to a training distribution. It does not make sense to talk about the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;variance of a model&lt;/span&gt;&lt;label for=&quot;5030c2df78371a93b2afff82f26639204861c9af&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;5030c2df78371a93b2afff82f26639204861c9af&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Intrinsic model variance due to stochastic elements like random weight init or stochastic inference is not the primary concern of the BV-decomposition.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; without reference to its training data, nor are we concerned with the variance of the training data by itself.&lt;/p&gt;

&lt;p&gt;In practice, &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;training algorithms&lt;/span&gt;&lt;label for=&quot;2dd74aa0c34118bd29f22e19e6232b791f73f9ef&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;2dd74aa0c34118bd29f22e19e6232b791f73f9ef&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;e.g. stochastic gradient descent.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; have stochastic elements, and in some cases &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;predictions can only be sampled&lt;/span&gt;&lt;label for=&quot;b99718f48c87cf69329695e9ea527f479eb48625&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;b99718f48c87cf69329695e9ea527f479eb48625&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;e.g. generative models like GANs and Boltzmann Machines&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; from the model stochastically. Again, our BV equation can be generalized to include additional sources of randomness by taking the expectation over the additional random variables.&lt;/p&gt;

&lt;p&gt;To include noise in the error, you can add $\ep$, a noise random variable, to the arguments of $f$. Loss = $\E_\ep[\E_D[(f(x, \ep; D) - y(x))^2]]$. For training noise, add $\ep$ to the training inputs, i.e. $f(x; D, \ep)$. This difference is purely semantic. Noise is noise.&lt;/p&gt;

&lt;h2 id=&quot;an-illustration&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#an-illustration&quot;&gt;An Illustration&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Let’s do a regression problem. We train our model on two different training sets. $x’$ in red is the test data point.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/posts/bias-variance/variance-2.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;figure&gt;&lt;img src=&quot;/assets/posts/bias-variance/variance-1.png&quot; alt=&quot;Images made with https://sketch.io/sketchpad/&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;Images made with https://sketch.io/sketchpad/&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The model’s prediction of test input $x’$ varies greatly between trainings. It is common to say this variance indicates the model is overfitting, which means informally that it’s fitting correlations present in the training set which are not present across datasets. However, overfitting is not an identical concept to variance of a test prediction.&lt;/p&gt;

&lt;p&gt;Practitioners would typically say this model has high variance, but that is a jargony shorthand. There is nothing about a model in isolation that has bias or variance. What we really mean is that this particular model applied to this particular training distribution has high variance. This realization affords us two ways to reduce variance:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Make the size of the training set larger (straight forward to prove that increasing dataset size decreases its variance).&lt;/li&gt;
  &lt;li&gt;Make the model less flexible (reduce its capacity)&lt;/li&gt;
&lt;/ol&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/posts/bias-variance/more-data-same-capacity.png&quot; alt=&quot;More data, same model flexibility.&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;More data, same model flexibility.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/posts/bias-variance/same-data-low-capacity.png&quot; alt=&quot;Same data, lower model flexibility.&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;Same data, lower model flexibility.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Note that reducing our model’s flexibility requires making an assumption about our data to reduce the hypothesis space. In this case, perhaps we assumed that our data is drawn from a lower order polynomial. If our constraint/assumption is not well-founded, then the model’s prediction on $x’$ will be systematically wrong, even if the prediction itself does not change much between training sets.&lt;/p&gt;

&lt;p&gt;Here is an example of a systematically wrong assumption. We have two training sets as before. The observed label for $x’$ is shown in gray.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&quot;/assets/posts/bias-variance/bias-1.png&quot; alt=&quot;&quot; width=&quot;50%&quot; /&gt;&lt;img src=&quot;/assets/posts/bias-variance/bias-2.png&quot; alt=&quot;&quot; width=&quot;50%&quot; /&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;This model visually makes a good fit to the training set, but clearly the assumption of linearity is erroneous. Our model’s prediction at $x’$ is wrong by roughly the same amount both times. It has high bias and low variance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;There is only a trade-off from variance to bias when we the experimenters run out of exploitable knowledge about the data.&lt;/strong&gt; An experimenter with perfect knowledge of the data distribution should be able to build a model that achieves 0 bias and variance (leaving only irreducible noise in the loss). In practice you will eventually reach some limit on your knowledge of the data. Researchers will sometimes try to add bias to their models in ways that reduce its variance. In some cases variance of prediction hurts a lot more than biased prediction, and in other cases you can actually get a slight reduction in overall error in the process of making this trade-off from variance to bias.&lt;/p&gt;

&lt;h2 id=&quot;flexibility-is-broken&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#flexibility-is-broken&quot;&gt;Flexibility is broken&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Model flexibility is more or less the same concept as &lt;a href=&quot;https://stats.stackexchange.com/questions/312424/what-is-the-capacity-of-a-machine-learning-model/312578#312578&quot;&gt;model capacity&lt;/a&gt;, which is the size of the hypothesis space. A flexible model can fit a more diverse set of functions. A somewhat common belief is that flexibility causes variance, or that flexibility even is variance. Flexibility in my mind is not a well defined concept, but variance is.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What is flexibility? How do you quantify it? How do you compare flexibilities?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A model can be flexible (or inflexible) in many different ways. No matter how flexible a model is, assumptions are made about its hypothesis space. These assumptions are called the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;inductive bias&lt;/span&gt;&lt;label for=&quot;83123f9b28aa2b33c82129e2ddba4096b16ea61f&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;83123f9b28aa2b33c82129e2ddba4096b16ea61f&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Defined as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Inductive_bias&quot;&gt;“set of assumptions”&lt;/a&gt;. This is qualitative. Not to be confused with statistical bias which is a computable quantity.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; of the model.&lt;/p&gt;

&lt;!--
&lt;figure&gt;&lt;img src='/assets/posts/bias-variance/inductive-bias-2.png' alt='Two models with high flexibility but very different inductive biases, applied to the same dataset.' width='50%'&gt;&lt;img src='/assets/posts/bias-variance/inductive-bias-1.png' alt='Two models with high flexibility but very different inductive biases, applied to the same dataset.' width='50%'&gt;&lt;figcaption&gt;Two models with high flexibility but very different inductive biases, applied to the same dataset.&lt;/figcaption&gt;&lt;/figure&gt;
--&gt;

&lt;p&gt;It is important to be clear that model flexibility by itself does not cause bias or variance. Bias and variance are the direct result of how that flexibility (or lack thereof) interacts with the data.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/posts/bias-variance/flexibility-2.png&quot; alt=&quot;(left) data sampled from a linear curve with large Gaussian noise, and (right) data sampled from a order-7 polynomial with small Gaussian noise. This order-7 polynomial model could be said to have high flexibility, and will have high variance on the left, but low variance on the right.&quot; width=&quot;50%&quot; /&gt;&lt;img src=&quot;/assets/posts/bias-variance/flexibility-1.png&quot; alt=&quot;(left) data sampled from a linear curve with large Gaussian noise, and (right) data sampled from a order-7 polynomial with small Gaussian noise. This order-7 polynomial model could be said to have high flexibility, and will have high variance on the left, but low variance on the right.&quot; width=&quot;50%&quot; /&gt;&lt;figcaption&gt;(left) data sampled from a linear curve with large Gaussian noise, and (right) data sampled from a order-7 polynomial with small Gaussian noise. This order-7 polynomial model could be said to have high flexibility, and will have high variance on the left, but low variance on the right.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;If you know a lot about the structure of your data and you give your model the ability to fit that structure, the variance of your model on that data may be low (picture on the right). You incur variance if that flexibility is not &lt;em&gt;useful&lt;/em&gt; (picture on the left).&lt;/p&gt;

&lt;h1 id=&quot;bias-variance-decomposition-for-any-loss&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#bias-variance-decomposition-for-any-loss&quot;&gt;Bias-Variance Decomposition For Any Loss&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Above I defined the bias-variance decomposition for ℓ2 loss. This is the standard presentation, but some will (rightly) question whether this decomposition is universal, meaning that it is true for all loss functions. The preference for ℓ2 loss in statistics is unfortunately &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error#Criticism&quot;&gt;somewhat arbitrary&lt;/a&gt;, and its popularity is mainly due to its nice math properties. I found a BV-decomposition generalized for all losses in this neat paper by &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;James et al.&lt;/span&gt;&lt;label for=&quot;1a1fe32522916b95fcdc00dec5d78c047b148223&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;1a1fe32522916b95fcdc00dec5d78c047b148223&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/1956/1b17519c58b9cb4c514dd102d08f307a5987.pdf&quot;&gt;“Generalizations of the Bias/Variance Decomposition for Prediction Error”&lt;/a&gt;, James, G., Hastie, T. (1997)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, which I will summarize here.  &lt;!-- citation --&gt;&lt;/p&gt;

&lt;p&gt;A loss $L : X \times X \rightarrow [0, \infty)$ is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Metric_(mathematics)#Definition&quot;&gt;distance&lt;/a&gt; function that satisfies,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$L(x, y) \geq 0$&lt;span style=&quot;font-style: italic; position: absolute; left: 35%&quot;&gt;non-negativity&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;$L(x, y) = 0 \Longleftrightarrow x = y$&lt;span style=&quot;font-style: italic; position: absolute; left: 35%&quot;&gt;identity&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;$L(x, y) = L(y, x)$&lt;span style=&quot;font-style: italic; position: absolute; left: 35%&quot;&gt;symmetry&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;$L$ is convex&lt;/span&gt;&lt;label for=&quot;546a46685429727b8f30be911a48289f4ea0b907&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;546a46685429727b8f30be911a48289f4ea0b907&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Stronger than requiring triangle inequality.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that $L$ operates on a single pair of values, and does not average over a dataset.&lt;/p&gt;

&lt;p&gt;James et al. defines the systematic part of random variable &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;$Y$&lt;/span&gt;&lt;label for=&quot;c03753b4b933880e5805dbef63e9d2d882c0c904&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;c03753b4b933880e5805dbef63e9d2d882c0c904&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;In this section, $Y$ is an underlying data distribution, and $\Yh$ is a model. Both are random variables. I am not showing the dependence of the model on training data and input $X$ for simplicity. In other words, I am not specifying why model $\Yh$ has variance, or even that this model has inputs and is supposed to generalize to a test set. However, everything below still holds with those details added back in.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; w.r.t. loss $L$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\s[Y] = \argmin_\theta \E_Y L(Y, \theta)&lt;/script&gt;

&lt;p&gt;This generalizes mean. Many common statistics (e.g. mean, median, mode) are &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;the systematic parts for common losses&lt;/span&gt;&lt;label for=&quot;90064962dbc49723d0f544518b2c435284bb450b&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;90064962dbc49723d0f544518b2c435284bb450b&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Source: &lt;a href=&quot;http://www.johnmyleswhite.com/notebook/2013/03/22/modes-medians-and-means-an-unifying-perspective/&quot;&gt;http://www.johnmyleswhite.com/notebook/2013/03/22/modes-medians-and-means-an-unifying-perspective/&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;$L(y, \hat{y}) =$&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Systematic part&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$(y - \hat{y})^0$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Mode&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$\lvert y - \hat{y} \rvert$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Median&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$(y - \hat{y})^2$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Arithmetic Mean&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$(\xfrac{1}{y} - \xfrac{1}{\hat{y}})^2$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Harmonic Mean&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$(\mathrm{ln}(y) - \mathrm{ln}(\hat{y}))^2$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Geometric Mean&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{cases} \tau \cdot (y - \hat{y}) &amp; y - \hat{y} \geq 0 \\ (\tau - 1) \cdot (y - \hat{y}) &amp; \mathrm{otherwise} \end{cases} %]]&gt;&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$\tau$-th Quantile&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;caption&gt;$\tau$ is a constant between 0 and 1. $\tau=\frac{1}{2}$ gives the median.&lt;/caption&gt;&lt;/table&gt;

&lt;!-- &lt;span class='marginnote-outer'&gt;&lt;span class='marginnote-ref'&gt;$\tau$-th Quantile&lt;/span&gt;&lt;label for='75adb4f877d352d398c8823aa64625a8f6d7862f' class='margin-toggle'&gt; &amp;#8853;&lt;/label&gt;&lt;input type='checkbox' id='75adb4f877d352d398c8823aa64625a8f6d7862f' class='margin-toggle'/&gt;&lt;span class='marginnote'&gt;&lt;span class='marginnote-inner'&gt;$\tau$ is a constant between 0 and 1. $\tau=\frac{1}{2}$ gives median.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; --&gt;

&lt;p&gt;James et al. shards the concepts of bias and variance into additional distinct concepts. For ℓ2 variance, we have the notion of &lt;em&gt;typical&lt;/em&gt; distance from some reference point of interest. James et al. points out that there are other ways to define variance which become equivalent under ℓ2 loss, but are not the same in general for other losses.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\V_\Yh[\Yh]
&amp; = \E_\Yh(\Yh - \E_\Yh\Yh)^2 \\
&amp; = \E_{Y,\Yh}(Y - \Yh)^2 - \E_Y(Y - \E_\Yh\Yh)^2 \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Likewise, bias squared can be written in two different ways.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\Bias[Y, \Yh]^2
&amp; = (\E_YY-\E_\Yh\Yh)^2 \\
&amp; = \E_Y(Y-\E_\Yh\Yh)^2 - \E_Y(Y-\E_YY)^2 \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;When we generalize loss and mean, these alternative ways of writing bias and variance become distinct statistical operations.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;variance effect&lt;/strong&gt;, $\textrm{VE}[Y, \Yh]$, is the expected change in prediction error when using $\Yh$ instead of $\s[\Yh]$ to predict $Y$. In other words, it measures the effect of predicting with a distribution and all its variability, vs the constant systematic part. This contrasts with intrinsic variance, $\V_\Yh[\Yh]$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\V_\Yh[\Yh] &amp;= \E_\Yh L(\Yh, \s[\Yh]) \\
\textrm{VE}[Y, \Yh] &amp;= \E_{Y,\Yh}[L(Y, \Yh) - L(Y, \s[\Yh])] \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;&lt;strong&gt;systematic effect&lt;/strong&gt;&lt;/span&gt;&lt;label for=&quot;5c8e80a5b3081b4f49de63bc6ff8c48d7b853f1f&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;5c8e80a5b3081b4f49de63bc6ff8c48d7b853f1f&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Not to be confused with systematic part. Not the terminology I would have gone with.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, $\textrm{SE}[Y, \Yh]$, is the expected change in prediction error when using $\s[\Yh]$ instead of $\s[Y]$ to predict $Y$. In other words, it measures the effect of predicting with the systematic part of the model, vs the systematic part of the data distribution. This contrasts with intrinsic bias squared, $\Bias[Y, \Yh]^2$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\Bias[Y, \Yh]^2 &amp;= L(\s[Y], \s[\Yh]) \\
\textrm{SE}[Y, \Yh] &amp;= \E_Y[L(Y, \s[\Yh]) - L(Y, \s[Y])] \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We may now state the generalized decomposition in terms of systematic and variance effect:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E_{Y,\Yh}L(Y, \Yh) = \textrm{SE}[Y, \Yh] + \textrm{VE}[Y, \Yh] + \V_Y[Y]&lt;/script&gt;

&lt;p&gt;Plugging in, we get,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\E_{Y,\Yh}L(Y, \Yh)
&amp;= \E_Y[L(Y, \s[\Yh]) - L(Y, \s[Y])] \\
&amp;\phantom{=}\, + \E_{Y,\Yh}[L(Y, \Yh) - L(Y, \s[\Yh])] \\
&amp;\phantom{=}\, + \E_YL(Y, \s[Y])
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Check out James et al. for some application examples of the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;generalized BV-decomposition&lt;/span&gt;&lt;label for=&quot;347f9f52dcd30e516865dc101e6c5330acc524e4&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;347f9f52dcd30e516865dc101e6c5330acc524e4&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;There exist more exotic BV-decompositions as well, e.g.
&lt;a href=&quot;https://math.stackexchange.com/questions/3017916/bias-variance-decomposition-for-kl-divergence&quot;&gt;KL-divergence&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-is-variance-anyway&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#what-is-variance-anyway&quot;&gt;What is variance anyway?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I was taught that variance is exactly equal to its ℓ2 &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;definition&lt;/a&gt;. I never questioned this until I had a heated argument with a friend who pointed out that variance was a collection of concepts before it was given a formula. The colloquial term &lt;em&gt;variance&lt;/em&gt; is defined as &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;“the fact or quality of being different, divergent, or inconsistent”&lt;/span&gt;&lt;label for=&quot;9bae4a6d7d13008870d9e32008ea0bc7daca6a45&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;9bae4a6d7d13008870d9e32008ea0bc7daca6a45&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;&lt;a href=&quot;https://www.google.com/search?q=define+variance&quot;&gt;https://www.google.com/search?q=define+variance&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. As it turns out, there are many ways to formalize aspects of this word, and one is not necessarily better than the other.&lt;/p&gt;

&lt;p&gt;Ultimately, the word &lt;em&gt;variance&lt;/em&gt;, when used in the context of statistics, will mean ℓ2 variance by convention. &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_dispersion&quot;&gt;Statistical dispersion&lt;/a&gt; is the technical name given to the umbrella of variance formulations. For example, &lt;a href=&quot;https://en.wikipedia.org/wiki/Median_absolute_deviation&quot;&gt;Median Absolute Deviation&lt;/a&gt; (MAD) is a not-so-obscure alternative to ℓ2 variance for all you &lt;a href=&quot;https://creativemaths.net/blog/median/&quot;&gt;median-lovers&lt;/a&gt; out there. &lt;a href=&quot;https://en.wikipedia.org/wiki/Entropy_(information_theory)&quot;&gt;Entropy&lt;/a&gt; is another one. Though not usually thought of as a measure of variance, entropy measures the spread of a distribution without distance to a reference point, which makes it particularly useful for &lt;a href=&quot;https://en.wikipedia.org/wiki/Categorical_variable&quot;&gt;categorical data&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I think formalizing concepts is a big deal, but I don’t take formalisms as absolute truth. They are more like pieces of code that can be connected together and modified. I am picky when it comes to the &lt;em&gt;API&lt;/em&gt; that a math definition uses, i.e. the objects that are abstracted away. I expect my formula to be general purpose, but there is not going to be a definition that captures all possible cases of an idea. My philosophy is to start with good math, and use it to understand the nuances of an idea. Then use intuition and creativity to think about the idea in new ways, and potentially invent a new formalism. The math you started with gives you a foundation for building and a sandbox for playing.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgments&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#acknowledgments&quot;&gt;Acknowledgments&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;I want to thank John Chung, Frazer Kirkman, Ivan Vendrov, and Roman Novak for their valuable discussions and feedback on this post. I give special thanks to Jeremy Nixon who inspired me to research this topic thoroughly and offered insights on the interaction between mathematics and informal ideas in research.&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Jul 2019 00:00:00 -0700</pubDate>
        <link>pragmanym.github.io/zhat/articles/bias-variance</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/bias-variance</guid>
        
        
        <category>primer</category>
        
      </item>
    
  </channel>
</rss>
