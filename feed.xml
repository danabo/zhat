<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Z-Hat</title>
    <description>A (we)blog devoted to finding better representations</description>
    <link>pragmanym.github.io/zhat/</link>
    <atom:link href="pragmanym.github.io/zhat/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 10 Jan 2020 19:26:44 -0800</pubDate>
    <lastBuildDate>Fri, 10 Jan 2020 19:26:44 -0800</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Wallace: Emergence of particles from QFT - Study Notes</title>
        <description>
</description>
        <pubDate>Fri, 10 Jan 2020 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/wallace-particles-qft</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/wallace-particles-qft</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Visualizing Quantum Field States - Study Notes</title>
        <description>
</description>
        <pubDate>Fri, 10 Jan 2020 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/visualizing-quantum-fields</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/visualizing-quantum-fields</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Solomonoff Induction - Study Notes</title>
        <description>
</description>
        <pubDate>Tue, 31 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/solomonoff-induction</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/solomonoff-induction</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Weak Measurement (Quantum Mechanics) - Study Notes</title>
        <description>
</description>
        <pubDate>Tue, 24 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/weak-measurement</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/weak-measurement</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Topology: Sphere &amp; Torus - Study Notes</title>
        <description>
</description>
        <pubDate>Mon, 23 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/topology-sphere-torus</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/topology-sphere-torus</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Cox's Theorem - Study Notes</title>
        <description>
</description>
        <pubDate>Mon, 23 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/coxs-theorem</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/coxs-theorem</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Quantum State</title>
        <description>&lt;p&gt;The two views of quantum state:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Quantum states are $L^2$-normalized complex-valued functions over classical configuration space.&lt;/li&gt;
  &lt;li&gt;Quantum states are unit vectors residing in a complex Hilbert space, $\mathcal{H}$.&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\newcommand{\bm}{\boldsymbol}
\newcommand{\diff}[1]{\mathop{\mathrm{d}#1}} 
\newcommand{\bra}[1]{\langle#1\rvert}
\newcommand{\ket}[1]{\lvert#1\rangle}
\newcommand{\braket}[2]{\langle#1\vert#2\rangle}&lt;/script&gt;

&lt;!--more--&gt;

&lt;ul class=&quot;toc&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-quantum-states-are-functions&quot; id=&quot;markdown-toc-1-quantum-states-are-functions&quot;&gt;1) Quantum States Are Functions&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#punchline-quantum-states-are-probability-distributions&quot; id=&quot;markdown-toc-punchline-quantum-states-are-probability-distributions&quot;&gt;Punchline: quantum states are probability distributions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-quantum-states-are-vectors&quot; id=&quot;markdown-toc-2-quantum-states-are-vectors&quot;&gt;2) Quantum States Are Vectors&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#measurement-bases&quot; id=&quot;markdown-toc-measurement-bases&quot;&gt;Measurement Bases&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#a-proper-basis&quot; id=&quot;markdown-toc-a-proper-basis&quot;&gt;A Proper Basis&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#punchline-uncertainty-principle-from-change-of-basis&quot; id=&quot;markdown-toc-punchline-uncertainty-principle-from-change-of-basis&quot;&gt;Punchline: Uncertainty Principle From Change Of Basis&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a very short primer on quantum mechanics, and the bare minimum that needs to be explained in order to understand what QM is about. My goal is to provide some conceptual punchlines with minimal prereqs. The formalism of QM is filled with technicalities and complications, which I try to point out where possible. Keep in mind I am glossing over a ton of details.&lt;/p&gt;

&lt;p&gt;Note, throughout I am setting $\hbar = 1$.&lt;/p&gt;

&lt;h1 id=&quot;1-quantum-states-are-functions&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#1-quantum-states-are-functions&quot;&gt;1) Quantum States Are Functions&lt;/a&gt;&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Quantum states are $L^2$-normalized complex-valued functions over classical configuration space.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For our purposes, classical configuration space is position space. A system with $N$ particles in 3D space has $3N$ dimensional configuration space. For simplicity, I’ll mostly talk about one particle in 1D space, i.e. 1D configuration space.&lt;/p&gt;

&lt;p&gt;So to restate, a quantum state is a function,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi : \mathbb{R}^n \rightarrow \mathbb{C}\,.&lt;/script&gt;

&lt;p&gt;for $n$-dimensional configuration space such that $\lvert\psi\rvert = 1$, i.e. $\psi$ is $L^2$-normalized.&lt;/p&gt;

&lt;p&gt;The $L^2$-norm of a complex-valued function is given as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert\psi\rvert = \sqrt{\int_{\mathbb{R}^n} \psi(x)\bar{\psi}(x)\diff{x
^n}}\,,&lt;/script&gt;

&lt;p&gt;where $\bar{f}(x) = \text{Re}[f(x)] - i\,\text{Im}[f(x)]$ is the complex conjugate.&lt;/p&gt;

&lt;h2 id=&quot;punchline-quantum-states-are-probability-distributions&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#punchline-quantum-states-are-probability-distributions&quot;&gt;Punchline: quantum states are probability distributions&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;You probably already guessed that $\psi$ encodes the probability of finding particles in space. The probability of observing system $\psi$ to be in &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;some region of configuration space&lt;/span&gt;&lt;label for=&quot;ec51e042e3a91e03088eeebfa8e927d02ca43c07&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ec51e042e3a91e03088eeebfa8e927d02ca43c07&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Remember that $n$ is the total number of spatial coordinates that describes the system. We are calculating the probability for all the coordinates together to be in some range.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; $D \subseteq \mathbb{R}^n$ is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_D  \psi(x) \bar{\psi}(x)\diff{x^n}\,.&lt;/script&gt;

&lt;p&gt;$\psi(x) \bar{\psi}(x) = \lvert\psi(x)\rvert^2$ is the absolute value squared of the complex value $\psi(x)$ (not to be confused with the function-norm $\lvert\psi\rvert$). In other words, $\lvert\psi(x)\rvert^2$ is a probability density. $\psi(x)$ is called a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;&lt;em&gt;probability amplitude&lt;/em&gt;&lt;/span&gt;&lt;label for=&quot;8fd9acc0f5baccc0d4f8236ac8210c8d7555b801&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;8fd9acc0f5baccc0d4f8236ac8210c8d7555b801&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;“Probability amplitude” refers specifically to a complex number whose square is a probability. I don’t think the word “amplitude” makes this apparent and it is rather ambiguous, but it’s handy to have a term to refer to these not-quite-probabilities and that is the convention.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-quantum-states-are-vectors&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#2-quantum-states-are-vectors&quot;&gt;2) Quantum States Are Vectors&lt;/a&gt;&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Quantum states are unit vectors residing in a complex Hilbert space, $\mathcal{H}$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;&lt;em&gt;Hilbert space&lt;/em&gt;&lt;/span&gt;&lt;label for=&quot;da5780db3d541999f81e9889c2af05f261b22854&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;da5780db3d541999f81e9889c2af05f261b22854&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The definition of Hilbert space is surprisingly ambiguous or too technical in most sources. For a while I was confused about what was special about Hilbert spaces in contrast with typical vector spaces. The axioms of linear algebra &lt;a href=&quot;https://math.stackexchange.com/a/28876&quot;&gt;allow for complex-valued scalars&lt;/a&gt;. The other notable feature of Hilbert space is that it allows for infinite dimensions, but that doesn’t make it unique. There are many ways to construct infinite dimensional vector spaces, e.g. &lt;a href=&quot;https://math.stackexchange.com/a/466741&quot;&gt;this answer&lt;/a&gt;. The distinguishing feature of Hilbert space is that it is &lt;a href=&quot;https://en.wikipedia.org/wiki/Complete_metric_space&quot;&gt;&lt;em&gt;complete&lt;/em&gt;&lt;/a&gt;, meaning that all &lt;a href=&quot;https://en.wikipedia.org/wiki/Cauchy_sequence&quot;&gt;Cauchy sequences&lt;/a&gt; of vectors in Hilbert space converge to vectors in Hilbert space. This ensures we can do calculus. It is true that all finite dimensional inner product spaces are Hilbert spaces, and they are simultaneously many other types of spaces, e.g. Banach spaces. Finite dimensional vector spaces do show up in quantum mechanics, and they are still called Hilbert spaces for consistency. Further reading: &lt;a href=&quot;https://www.quantiki.org/wiki/hilbert-spaces&quot;&gt;A&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Hilbert_space#History&quot;&gt;B&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is a generalization of Euclidean vector space to real or complex-valued vector spaces, with finite or infinite dimensions. The generalization of the Euclidean inner-product for functions is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle f, g \rangle = \int_{\mathbb{R}^n} f(x)\bar{g}(x)\diff{x^n}\,.&lt;/script&gt;

&lt;p&gt;Let $\mathcal{H}$ be a Hilbert space of $L^2$-normalized complex-valued functions. Then any $\psi \in \mathcal{H}$ is a unit-vector, since $\lvert\psi\rvert^2 = \langle \psi, \psi \rangle = 1$.&lt;/p&gt;

&lt;h2 id=&quot;measurement-bases&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#measurement-bases&quot;&gt;Measurement Bases&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Given that the elements of $\mathcal{H}$ are functions over $x$, one straightforward way to choose a basis for $\mathcal{H}$ is to think of $\psi \in \mathcal{H}$ as a vector with uncountably many entries, one for each input $x$. This is achieved with a basis of &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function&quot;&gt;Dirac delta functions&lt;/a&gt;&lt;/span&gt;&lt;label for=&quot;68625e91f114b5de914c4a1ae96b612e8a90302e&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;68625e91f114b5de914c4a1ae96b612e8a90302e&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;0 everywhere except for the origin. Defined so that its total area is 1. Technically they are not functions, but &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function#As_a_distribution&quot;&gt;distributions&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. Writing $\psi$ in this basis,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\psi &amp;= \int_{\mathbb{R}^n} \psi(\chi)\cdot\left[ x \mapsto \delta(x-\chi)\right]\diff{\chi^n} \\
     &amp;= x \mapsto \int_{\mathbb{R}^n} \psi(\chi)\delta(x-\chi)\diff{\chi^n} \\
     &amp;= x \mapsto \psi(x)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;This integral is an uncountable linear combination, where &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;$x \mapsto \delta(x-\chi)$&lt;/span&gt;&lt;label for=&quot;6e2c209cbdb6c596f74638233310e8ffd60cc39e&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;6e2c209cbdb6c596f74638233310e8ffd60cc39e&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Note I am using non-standard notation. “$x \mapsto f(x)$” instantiates a function object in-line, just as “$\{f(y) \mid \text{condition}[y]\}$” instantiates a set in-line.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the $\chi$-th basis element and $\psi(\chi)$ is the corresponding coefficient.&lt;/p&gt;

&lt;p&gt;This basis of Dirac deltas corresponds to position measurement. Particles and systems of particles have other measurable properties, like momentum and total energy, which have their own bases. More on that below.&lt;/p&gt;

&lt;h2 id=&quot;a-proper-basis&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#a-proper-basis&quot;&gt;A Proper Basis&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;There is a problem here. $b_\chi = x \mapsto \delta(x-\chi)$ is not $L^2$-normalizable, because $\lvert b_\chi\rvert^2 = \int_{\mathbb{R}^n} \delta(x-\chi)^2\diff{x^n}$ is undefined. Such a basis that is not actually contained in our vector space is sometimes called an &lt;em&gt;improper basis&lt;/em&gt;. This can be remedied by augmenting $\mathcal{H}$ to include objects like Dirac deltas. The resulting construction is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Rigged_Hilbert_space&quot;&gt;rigged Hilbert space&lt;/a&gt;. However, this is not an ideal solution because &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;now we have quantum states&lt;/span&gt;&lt;label for=&quot;0f554cb904b48367e4f303e35eba33200371deb9&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;0f554cb904b48367e4f303e35eba33200371deb9&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;We will soon see that we want our Hilbert space to be closed under Fourier transform, so if Dirac deltas are included, then so should sin waves of the form $x \mapsto e^{ikx}$, which are also not $L^2$-normalizable.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; which cannot be interpreted as probability distributions.&lt;/p&gt;

&lt;p&gt;There are plenty of valid orthonormal bases for the Hilbert space of &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;unit vectors&lt;/span&gt;&lt;label for=&quot;a407fae48b39bd45ead1aeb29b1a295c009b4471&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;a407fae48b39bd45ead1aeb29b1a295c009b4471&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;And also $L^2$-normalized functions.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. You may be surprised to learn that such bases are necessarily countable. One of the most well known is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_harmonic_oscillator#Hamiltonian_and_energy_eigenstates&quot;&gt;measurement basis for the total energy of the harmonic oscillator&lt;/a&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{B} = \{x \mapsto(-1)^n(2^nn!\sqrt{\pi})^{-\frac{1}{2}}e^{\frac{x^2}{2}}\frac{\diff{^n}}{\diff{x^n}}\left[e^{-x^2}\right]\,\mid\, n \in \mathbb{N}\}\,.&lt;/script&gt;

&lt;p&gt;Symbolically these functions look quite complicated, but you can get a sense for the pattern of what these functions look like as $n$ increases:
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Herm5.svg/900px-Herm5.svg.png&quot; alt=&quot;&quot; /&gt;
*Plot of first 6 basis functions: 0 (black), 1 (red), 2 (blue), 3 (yellow), 4 (green), and 5 (magenta). Note that these are real-valued functions. Source: &lt;a href=&quot;https://en.wikipedia.org/wiki/Hermite_polynomials#Hermite_functions&quot;&gt;https://en.wikipedia.org/wiki/Hermite_polynomials#Hermite_functions&lt;/a&gt;.
 *&lt;/p&gt;

&lt;p&gt;It’s amazing that can write any function in $\mathcal{H}$ as a linear combination of the functions in $\mathcal{B}$, i.e. any continuous complex-valued $L^2$-normalized function can be written as a weighted sum of these functions. We can also use $\mathcal{B}$ to explicitly construct our $L^2$-normalized Hilbert space,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{H} = \{c_0H_0(x) + c_1H_1(x) + c_2H_2(x) + \ldots \,|\, c_0,c_1,c_2,\ldots\in\mathbb{C}\}\,,&lt;/script&gt;

&lt;p&gt;where $H_n(x) \in \mathcal{B}$ is the $n$-th &lt;a href=&quot;https://en.wikipedia.org/wiki/Hermite_polynomials#Definition&quot;&gt;Hermite function&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;punchline-uncertainty-principle-from-change-of-basis&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#punchline-uncertainty-principle-from-change-of-basis&quot;&gt;Punchline: Uncertainty Principle From Change Of Basis&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Under the vector view, the coefficients of a quantum state in some measurement basis are &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;probability amplitudes&lt;/span&gt;&lt;label for=&quot;da0f2680d7f9c26d8d21b75aee55f3fa4e0dbc0d&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;da0f2680d7f9c26d8d21b75aee55f3fa4e0dbc0d&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Their absolute squares are probabilities or probability densities.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. The basis vectors are called &lt;em&gt;definite&lt;/em&gt; states, and their linear combinations are indefinite. So if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi = c_1 B_1 + c_2 B_2 + \ldots\,&lt;/script&gt;

&lt;p&gt;for measurement basis $\{B_1, B_2, \ldots\}$, when the corresponding property of $\psi$ is measured, the system will instantaneously jump to one of $B_i$ with probability $\lvert c_i\rvert^2$. This instantaneous jump is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Wave_function_collapse&quot;&gt;&lt;em&gt;wave-function collapse&lt;/em&gt;&lt;/a&gt;. The experimenter will also know which of the basis states the system is in because there is a corresponding measurement readout for each (unless the measurement is degenerate). In the position basis, we recover the probability interpretation of states as functions.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Superposition&lt;/em&gt; is the name given to the physical phenomenon of a system being in an indefinite state, meaning that when you observe some property the outcome is probabilistic. Mathematically superposition is modeled by linear combinations of definite states. A key facet of quantum mechanics is that multiple measurable properties of a system cannot be &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;simultaneously definite&lt;/span&gt;&lt;label for=&quot;7066f1de8a35a976b9f072db9e312f0a6ba14845&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;7066f1de8a35a976b9f072db9e312f0a6ba14845&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Aside from so-called &lt;a href=&quot;https://en.wikipedia.org/wiki/Complete_set_of_commuting_observables&quot;&gt;commuting observables&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. We get this phenomenon for free out of the framework of linear algebra, by representing properties to be measured as orthonormal bases in Hilbert space.&lt;/p&gt;

&lt;p&gt;The famous example is position and momentum. Both these properties of a particle cannot be known at the same time. We saw that the position (improper) basis for one spatial coordinate is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{B}_\text{x} = \{x \mapsto \delta(x - \chi) \mid \chi \in \mathbb{R}\}\,.&lt;/script&gt;

&lt;p&gt;The momentum basis is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Position_and_momentum_space#Relation_between_space_and_reciprocal_space&quot;&gt;Fourier conjugate&lt;/a&gt; of the position basis, given as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{B}_\text{p} = \{x \mapsto \frac{1}{\sqrt{2\pi}}e^{ipx} \mid p \in \mathbb{R}\}\,.&lt;/script&gt;

&lt;p&gt;The real part of these functions are sin waves. An element $[x \mapsto \delta(x - \chi)] \in \mathcal{B}_\text{x}$ of the position basis can be written as a linear combination of momentum basis elements,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
[x \mapsto \delta(x - \chi)] 
  &amp;= \int_\mathbb{R} \frac{1}{\sqrt{2\pi}}e^{-ip\chi} \cdot \vec{b}_p \diff{p} \\
  &amp;= \int_\mathbb{R} \frac{1}{\sqrt{2\pi}}e^{-ip\chi} \cdot \left[x \mapsto \frac{1}{\sqrt{2\pi}}e^{ipx}\right] \diff{p} \\
  &amp;= x \mapsto \int_\mathbb{R} \frac{1}{\sqrt{2\pi}}e^{-ip\chi} \frac{1}{\sqrt{2\pi}}e^{ipx} \diff{p}
\end{align}\,. %]]&gt;&lt;/script&gt;

&lt;p&gt;In the momentum basis, a position basis element becomes a sine wave, i.e. the coefficient of the $p$-th momentum basis element is $\frac{1}{\sqrt{2\pi}}e^{-ip\chi}$.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Heisenberg uncertainty principle&lt;/em&gt;, informally, states that a quantum state which is more localized in space (peakier distribution in the position basis) is necessarily less localized in momentum space (spread out distribution in the momentum basis), and vice versa. This uncertainty principle is not an extra assertion on top of quantum mechanics, but is a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;consequence of the Fourier transform&lt;/span&gt;&lt;label for=&quot;a49269f8c12334e2ed9b8c18b484bc3255faf867&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;a49269f8c12334e2ed9b8c18b484bc3255faf867&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The same uncertainty principle is a problem when converting audio recordings to sheet music, as the frequency resolution of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectrogram&quot;&gt;spectrogram&lt;/a&gt; is inversely proportional to the time resolution. In other words, you cannot know the exact frequency of a fast changing sound due to the same uncertainty principle. Here time plays the role of space. &lt;a href=&quot;https://lts2.epfl.ch/blog/gbr/2015/05/08/uncertainty-principle-in-quantum-physics-and-signal-processing/&quot;&gt;Further reading.&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. More generally, there is an uncertainty principle between any two measurable properties with &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;different measurement bases&lt;/span&gt;&lt;label for=&quot;9acabeda9899d14b3b917f753be9cfa784d0a28c&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;9acabeda9899d14b3b917f753be9cfa784d0a28c&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Technically this is not true, as some properties can be &lt;a href=&quot;https://en.wikipedia.org/wiki/Complete_set_of_commuting_observables&quot;&gt;observed simultaneously&lt;/a&gt;. How this happens is out of the scope of this post, but essentially, when measurement readouts correspond to multiple basis elements the system will collapse to a superposition of those elements, i.e. it will be projected onto their span. A second measurement of a “compatible” property will further collapse the system to a single basis element.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;This definition of quantum state is very helpful for visualizing what your system looks like in physical space. The vector definition is unfortunately more abstract and can often obscure the connection to physical space. The vector definition has its own benefits. In my opinion, knowing how to go between both definitions is optimal for understanding.&lt;/p&gt;

&lt;p&gt;I didn’t touch on how quantization comes into play, i.e. discrete measurement outcomes. In short, discrete measurement bases result in discrete measurement readouts. For example, total energy will be discrete for particles in potential wells, and continuous for free particles. What determines whether the measurement basis will be continuous or discrete, and how to derive these bases all together, is a complicated matter that gets into &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_theory&quot;&gt;spectral theory of linear operators&lt;/a&gt;. Maybe a topic of a future post.&lt;/p&gt;

&lt;p&gt;It’s important to point out is that &lt;a href=&quot;https://en.wikipedia.org/wiki/Measurement_in_quantum_mechanics&quot;&gt;measurement in quantum mechanics&lt;/a&gt; is not explained, but &lt;a href=&quot;https://en.wikipedia.org/wiki/Wave_function_collapse#History_and_context&quot;&gt;taken as given&lt;/a&gt;. The non-determinism of measurement outcomes is part of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac%E2%80%93von_Neumann_axioms&quot;&gt;axioms of quantum mechanics&lt;/a&gt;, at least under the Copenhagen interpretation. There are alternative interpretations which deny wave-function collapse, notably &lt;a href=&quot;https://en.wikipedia.org/wiki/Many-worlds_interpretation&quot;&gt;pilot-wave&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Many-worlds_interpretation&quot;&gt;many-worlds&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Sun, 22 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/quantum-minimal-conceptual-unit</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/quantum-minimal-conceptual-unit</guid>
        
        
        <category>Brain Dump</category>
        
      </item>
    
      <item>
        <title>Bias-Variance Decomposition For Machine Learning</title>
        <description>&lt;script type=&quot;math/tex; mode=display&quot;&gt;\newcommand{\Real}{ {\mathbb{R}} }
\newcommand{\E}{ {\mathbb{E}} }
\newcommand{\V}{ {\mathbb{V}} }
\newcommand{\D}{\mathcal{D}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand\Yh{ {\hat{Y}} }
\newcommand{\ep}{ {\boldsymbol{\varepsilon}} }
\newcommand{\s}{\mathbb{S}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}&lt;/script&gt;

&lt;p&gt;All about the bias-variance decomposition as it pertains to machine learning. All you need to know:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp; \E_D[(f(x; D) - y(x))^2] \qquad\quad\ \textrm{Avg. error}\\
&amp; = (\E_D[f(x; D)] - y(x))^2 \qquad  \textrm{Bias}_y(f)^2\\
&amp;\phantom{=}\, + \V_D[f(x; D)]     \qquad\qquad\quad\ \, \textrm{Variance}(f)\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;!--more--&gt;

&lt;ul class=&quot;toc&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#preamble&quot; id=&quot;markdown-toc-preamble&quot;&gt;Preamble&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#context&quot; id=&quot;markdown-toc-context&quot;&gt;Context&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bias-variance-decomposition-for-ℓ2-loss&quot; id=&quot;markdown-toc-bias-variance-decomposition-for-ℓ2-loss&quot;&gt;Bias-Variance Decomposition For ℓ2 Loss&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#symbol-breakdown&quot; id=&quot;markdown-toc-symbol-breakdown&quot;&gt;Symbol breakdown&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#d-for-dataset&quot; id=&quot;markdown-toc-d-for-dataset&quot;&gt;$D$ for dataset&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#test-on-x&quot; id=&quot;markdown-toc-test-on-x&quot;&gt;Test on $x$&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#label-with-y&quot; id=&quot;markdown-toc-label-with-y&quot;&gt;Label with $y$&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#f-is-the-model--training-algo&quot; id=&quot;markdown-toc-f-is-the-model--training-algo&quot;&gt;$f$ is the model + training algo&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#an-illustration&quot; id=&quot;markdown-toc-an-illustration&quot;&gt;An Illustration&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#flexibility-is-broken&quot; id=&quot;markdown-toc-flexibility-is-broken&quot;&gt;Flexibility is broken&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bias-variance-decomposition-for-any-loss&quot; id=&quot;markdown-toc-bias-variance-decomposition-for-any-loss&quot;&gt;Bias-Variance Decomposition For Any Loss&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#what-is-variance-anyway&quot; id=&quot;markdown-toc-what-is-variance-anyway&quot;&gt;What is variance anyway?&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#acknowledgments&quot; id=&quot;markdown-toc-acknowledgments&quot;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;preamble&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#preamble&quot;&gt;Preamble&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;Why am I writing this? In short, because I think there should be an equation somewhere out there that defines the bias-variance decomposition unambiguously and in complete generality.&lt;/p&gt;

&lt;p&gt;You’ve likely heard of the bias-variance trade-off. There is not always a trade-off, hence I call it a &lt;em&gt;decomposition&lt;/em&gt;. You may think that the trade-off is about trading model flexibility for bias. Probe your understanding and ask yourself, does flexibility always lead to variance? How does flexibility lead to variance? What is varying? The model? The dataset? The variance comes from the model’s interaction with the training data, rather than these things in isolation.&lt;/p&gt;

&lt;p&gt;When you picture this &lt;em&gt;trade-off&lt;/em&gt;, what comes to mind? A &lt;a href=&quot;http://scott.fortmann-roe.com/docs/BiasVariance.html&quot;&gt;dart-board&lt;/a&gt;? A &lt;a href=&quot;https://books.google.com/books?id=NZP6AQAAQBAJ&amp;amp;lpg=PA22&amp;amp;vq=wiggly&amp;amp;pg=PA22#q=wiggly&quot;&gt;wiggly&lt;/a&gt; or rigid function? Those are metaphors - approximations of the concept. Ask yourself, why do the dart throws have bias and variance? How does “wiggliness” contribute to variance? Do some types of wiggliness have less variance than others? These analogies don’t include the source of randomness.&lt;/p&gt;

&lt;p&gt;My life is easiest when I am presented with a formal definition. I can then pull nuances out of it, and refer to it to resolve my confusions. My understanding of an idea is as good as its most precise definition. An equation is the bedrock of intuitive understanding.&lt;/p&gt;

&lt;p&gt;The power of math is that it is precise, but that precision is lost when an equation has multiple meanings. This ambiguity is only possible when authors take notational shortcuts for visual simplicity and assume the reader can infer the missing information. I prefer a precise equation accompanied by exposition.&lt;/p&gt;

&lt;p&gt;So why is this post so long? I could just give the definition (which I did below) and be done with it, but, 1) it’s valuable to unpack the meaning of the equation and explore tricky cases, and, 2) this topic is not so simple, as I discovered while researching this. A truly general definition of the BV-decomposition requires getting abstract… though obscure it’s interesting. However, the special case for ℓ2 loss is all you need to understand and apply this topic.&lt;/p&gt;

&lt;h1 id=&quot;context&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#context&quot;&gt;Context&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;If you found this post with zero context, the bias-variance (BV) decomposition is a mathematical identity stating that expected &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;prediction error&lt;/span&gt;&lt;label for=&quot;5c28819791baf6fa2e6ae991dce4e20db3b229c0&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;5c28819791baf6fa2e6ae991dce4e20db3b229c0&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;On an evaluation/test dataset.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; of a model equals bias-squared plus variance. The precise meaning of these words is best understood by looking at the equation below.&lt;/p&gt;

&lt;p&gt;BV-decomposition is a commonly used idea in machine learning and statistics. Understanding it is essential for communicating with researchers and practitioners. The decomposition (often a trade-off) is used for reasoning about model design. Tons of textbooks and papers give examples of this, e.g. &lt;a href=&quot;https://books.google.com/books?id=tVIjmNS3Ob8C&amp;amp;q=bias-variance#v=onepage&amp;amp;q=7.3.1%20Example%20bias-variance%20tradeoff&amp;amp;f=false&quot;&gt;Elements of Statistical Learning&lt;/a&gt;, or &lt;a href=&quot;https://arxiv.org/pdf/1301.2315.pdf&quot;&gt;reinforcement learning literature&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;bias-variance-decomposition-for-ℓ2-loss&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#bias-variance-decomposition-for-ℓ2-loss&quot;&gt;Bias-Variance Decomposition For ℓ2 Loss&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;This discussion will assume &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;supervised data&lt;/span&gt;&lt;label for=&quot;efbaae27692066e22f22f83d1ec9ef236f2a1a1c&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;efbaae27692066e22f22f83d1ec9ef236f2a1a1c&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;I have not seen BV-decomposition for unsupervised models, but my guess is that you can treat it as a supervised problem where $y = p(x)$.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, e.g. pairs $(x, y)$ where $x$ is an input and $y$ is the observed output. &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;ℓ2&lt;/span&gt;&lt;label for=&quot;5b53813f2890dea036468713f6034958e33025d5&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;5b53813f2890dea036468713f6034958e33025d5&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Usually noted as $L^2$. Presumably originating from &lt;a href=&quot;https://en.wikipedia.org/wiki/Lp_space#Definition&quot;&gt;$L^p$-norm&lt;/a&gt;. I’m using curly ℓ to avoid confusion when $L$ denotes any loss function. See the &lt;a href=&quot;#bias-variance-decomposition-for-any-loss&quot;&gt;Any-Loss section&lt;/a&gt; below.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;loss&lt;/span&gt;&lt;label for=&quot;06b82caf79d08d6c5b8eb70db4ddca09885fc10a&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;06b82caf79d08d6c5b8eb70db4ddca09885fc10a&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Loss is machine learning jargon for error function.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, a.k.a. squared error, is the squared difference between a prediction and the observed output.&lt;/p&gt;

&lt;p&gt;The bias-variance decomposition comes from statistics, which considers the &lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator#Mean_squared_error&quot;&gt;error of a parameter estimator&lt;/a&gt;. In ML, we are concerned with the error of a model’s prediction (output) and a given target output, which we call the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;label&lt;/span&gt;&lt;label for=&quot;5720ea25baeabd25f3c70af8dfd57e178177986f&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;5720ea25baeabd25f3c70af8dfd57e178177986f&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;&lt;em&gt;label&lt;/em&gt; originates from classification where the model is predicting a nominal value, e.g. category/class. Here I use &lt;em&gt;label&lt;/em&gt; to mean to anything that is predicted, and I assume the label is real valued. I just need a word that refers to $y$ in $(x, y)$, including the given/observed and model’s prediction.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; (a scalar value). The bias-variance decomposition is a mathematical identity stating that expected test loss (prediction error) equals bias-squared plus variance across training datasets. For ℓ2 loss &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;we have,&lt;/span&gt;&lt;label for=&quot;9b941cfe0481999bd29f622bafc5ffa18254b108&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;9b941cfe0481999bd29f622bafc5ffa18254b108&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Adapted from the equation in &lt;a href=&quot;http://www.dam.brown.edu/people/geman/Homepage/Essays%20and%20ideas%20about%20neurobiology/bias-variance.pdf&quot;&gt;“Neural networks and the bias/variance dilemma”&lt;/a&gt;, Geman, S., Bienenstock, E., and Doursat, R. (1992)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;  &lt;!-- citation --&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp; \E_D[(f(x; D) - y(x))^2] \qquad\quad\ \textrm{Avg. error}\\
&amp; = (\E_D[f(x; D)] - y(x))^2 \qquad  \textrm{Bias}_y(f)^2\\
&amp;\phantom{=}\, + \V_D[f(x; D)]     \qquad\qquad\quad\ \, \textrm{Variance}(f)\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;I am leaving out the derivation of the BV-decomposition, as it can be easily found in a number of sources, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation&quot;&gt;Wikipedia&lt;/a&gt;. My purpose here is to clear up common points of confusion.&lt;/p&gt;

&lt;p&gt;For completion, here is the definition of $\E$ and $\V$. Let $g:\Real \rightarrow \Real$ be an arbitrary (deterministic) function over the reals, and $Z$ be an arbitrary random variable over domain $D_Z \subseteq \Real$. Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E_Z[g(Z)] = \int_{D_Z} p_Z(z) g(z) dz&lt;/script&gt;

&lt;p&gt;is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Expected_value&quot;&gt;expected value&lt;/a&gt; of $g(Z)$, and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\V_Z[g(Z)] = \E_X[(g(Z) - \E_X[g(Z)])^2]&lt;/script&gt;

&lt;p&gt;is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance#Definition&quot;&gt;variance&lt;/a&gt; of $g(Z)$.&lt;/p&gt;

&lt;h2 id=&quot;symbol-breakdown&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#symbol-breakdown&quot;&gt;Symbol breakdown&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;d-for-dataset&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#d-for-dataset&quot;&gt;$D$ for dataset&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;$D$ is a random variable over all possible &lt;strong&gt;training&lt;/strong&gt; datasets. Technically, this equation does not make any assumptions about the nature of the training data, but generally $\{(x_1,y_1),(x_2,y_2), \ldots,(x_n,y_n)\}$ is a sample from $D$, where $(x_i,y_i)$ is an input, $x_i$, with its observed label, $y_i$. Additionally, each $(x_i,y_i)$ is typically assumed to be sampled i.i.d., though that is also not required here either. The dataset size $n$ may be stochastic as well.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;first&lt;/strong&gt; big point of confusion surrounding BV-decomposition is that the training data comes from a distribution. In practice, you have a fixed dataset, so how does it make sense to take the expectation over all possible training datasets? This equation wants you to imagine that you can draw as many different dataset as you want from the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;data generating process&lt;/span&gt;&lt;label for=&quot;0601b0bf148eaab97dd51194673e2d06a20ea6cb&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;0601b0bf148eaab97dd51194673e2d06a20ea6cb&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;whatever physical process produced your current training dataset&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. That could mean, for example, having photographers go out and take &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;new photos and label the objects&lt;/span&gt;&lt;label for=&quot;e41db9272d77540c2561a40e3e500274bb9d97b4&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;e41db9272d77540c2561a40e3e500274bb9d97b4&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Photographer + camera + environment + labeler is the physical system that generates the data. You can regard the process as inducing a probability distribution $p(x, y)$. Technically the physical setup would need to be exactly the same each time a photo is taken to sample i.i.d. from $p(x, y)$, but uncorrelated differences in photo taking are fine, e.g. many photographers who have their own styles. I do acknowledge that nothing is truly i.i.d. in reality.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, or the population census retaken. Redrawing a new training set likely won’t happen in reality, but this is the correct conceptual understanding.&lt;/p&gt;

&lt;h3 id=&quot;test-on-x&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#test-on-x&quot;&gt;Test on $x$&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;$x$ is a fixed &lt;strong&gt;test&lt;/strong&gt; input. Our loss $(f(x; D) - y(x))^2$ is measuring test error on $x$. However, we take the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;expectation of this test error&lt;/span&gt;&lt;label for=&quot;33aca9ddf266bcb2f8c9b30aed5842d7b704e846&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;33aca9ddf266bcb2f8c9b30aed5842d7b704e846&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;$\E_D[(f(x; D) - y(x))^2]$&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; over &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;all possible training datasets $D$&lt;/span&gt;&lt;label for=&quot;ba7efec6a7dec33e15a9327fe626526569d63f61&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ba7efec6a7dec33e15a9327fe626526569d63f61&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;$x$ may appear in some of them&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. What $x$ to test on is application dependent, and need not be specified here. It’s important to note that $x$ is held fixed, and not a random variable. This test error equals the model’s bias plus variance on a static input, across all training sets.&lt;/p&gt;

&lt;p&gt;Rather than a single test input, you could consider a set of test inputs, $\{x_1,x_2,\ldots,x_m\}$. In which case you are interested in the average loss across the fixed inputs: loss = $\frac{1}{M}\sum_{i=1}^{m} \E_D[(f(x_i; D) - y(x_i))^2]$.&lt;/p&gt;

&lt;p&gt;Alternatively you could consider a random variable over all possible test inputs, $X$, and take the expectation test error: loss = $\E_X[\E_D[(f(X; D) - y(X))^2]]$.&lt;/p&gt;

&lt;p&gt;The underlying identity is the same in all cases.&lt;/p&gt;

&lt;h3 id=&quot;label-with-y&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#label-with-y&quot;&gt;Label with $y$&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I am assuming a deterministic labeling function $y(x)$ for input $x$. It provides the observed label, a.k.a. &lt;a href=&quot;https://en.wikipedia.org/wiki/Ground_truth&quot;&gt;ground truth&lt;/a&gt;. In practice, the labeling process is also stochastic, and there may not even be a clear &lt;em&gt;truth&lt;/em&gt; of the matter. The more general case of the BV-decomposition uses $Y(x)$, where $Y$ is a random variable with conditional distribution, $p_{Y \mid X}(Y \mid X=x)$. $x$ is still a fixed constant here, but could be made a random variable as well if need be.&lt;/p&gt;

&lt;p&gt;When $Y(x)$ is stochastic, we get an extra term in the decomposition:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \E_{Y(x)}[\E_D[(f(x; D) - Y(x))^2]] \\
&amp; = (\E_D[f(x; D)] - \E_{Y(x)}[Y(x)])^2 \qquad\,   \textrm{Bias}_y(f)^2\\
&amp;\phantom{=}\, + \V_D[f(x; D)]   \qquad\qquad\qquad\qquad\ \ \,  \textrm{Variance}(f)\\
&amp;\phantom{=}\, + \V_{Y(x)}[Y(x)]  \qquad\qquad\qquad\qquad\ \ \,   \textrm{Variance}(y)\textrm{, i.e. “noise&quot;}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Presumably the human model builder cannot control $\V_{Y(x)}[Y(x)]$, so this is the lower bound on their model’s loss. If bias and variance were reduced to 0, this term would remain.&lt;/p&gt;

&lt;h3 id=&quot;f-is-the-model--training-algo&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#f-is-the-model--training-algo&quot;&gt;$f$ is the model + training algo&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;$f$ is both the model and training algorithm. $f(x; d)$ is a function of input $x$, and a training dataset $d$. $f$ outputs its prediction for $x$, given that it was trained on the dataset. The colon is commonly used in ML to separate model inputs with parameter or training inputs. You can think of $f(d)=f’$ as a outputting another function $f’$ which makes the prediction, $f’(x)$.&lt;/p&gt;

&lt;p&gt;This formulation does not make reference to model parameters, and leaves open the possibility of non-parametric models. The BV-decomposition is a universal property, and the process that goes from dataset to predictive model is irrelevant. I am assuming $f$ is deterministic w.r.t. both $x$ and $d$. However, if any of its inputs are random variables, the output is a random variable. So $f(x; D)$ is a random variable, as well as $f(X; d)$.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;second&lt;/strong&gt; big point of confusion in the BV-decomposition is that the model’s output distribution is the result of applying the model to a training distribution. It does not make sense to talk about the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;variance of a model&lt;/span&gt;&lt;label for=&quot;5030c2df78371a93b2afff82f26639204861c9af&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;5030c2df78371a93b2afff82f26639204861c9af&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Intrinsic model variance due to stochastic elements like random weight init or stochastic inference is not the primary concern of the BV-decomposition.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; without reference to its training data, nor are we concerned with the variance of the training data by itself.&lt;/p&gt;

&lt;p&gt;In practice, &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;training algorithms&lt;/span&gt;&lt;label for=&quot;2dd74aa0c34118bd29f22e19e6232b791f73f9ef&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;2dd74aa0c34118bd29f22e19e6232b791f73f9ef&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;e.g. stochastic gradient descent.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; have stochastic elements, and in some cases &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;predictions can only be sampled&lt;/span&gt;&lt;label for=&quot;b99718f48c87cf69329695e9ea527f479eb48625&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;b99718f48c87cf69329695e9ea527f479eb48625&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;e.g. generative models like GANs and Boltzmann Machines&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; from the model stochastically. Again, our BV equation can be generalized to include additional sources of randomness by taking the expectation over the additional random variables.&lt;/p&gt;

&lt;p&gt;To include noise in the error, you can add $\ep$, a noise random variable, to the arguments of $f$. Loss = $\E_\ep[\E_D[(f(x, \ep; D) - y(x))^2]]$. For training noise, add $\ep$ to the training inputs, i.e. $f(x; D, \ep)$. This difference is purely semantic. Noise is noise.&lt;/p&gt;

&lt;h2 id=&quot;an-illustration&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#an-illustration&quot;&gt;An Illustration&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Let’s do a regression problem. We train our model on two different training sets. $x’$ in red is the test data point.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/posts/bias-variance/variance-2.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;figure&gt;&lt;img src=&quot;/assets/posts/bias-variance/variance-1.png&quot; alt=&quot;Images made with https://sketch.io/sketchpad/&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;Images made with https://sketch.io/sketchpad/&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The model’s prediction of test input $x’$ varies greatly between trainings. It is common to say this variance indicates the model is overfitting, which means informally that it’s fitting correlations present in the training set which are not present across datasets. However, overfitting is not an identical concept to variance of a test prediction.&lt;/p&gt;

&lt;p&gt;Practitioners would typically say this model has high variance, but that is a jargony shorthand. There is nothing about a model in isolation that has bias or variance. What we really mean is that this particular model applied to this particular training distribution has high variance. This realization affords us two ways to reduce variance:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Make the size of the training set larger (straight forward to prove that increasing dataset size decreases its variance).&lt;/li&gt;
  &lt;li&gt;Make the model less flexible (reduce its capacity)&lt;/li&gt;
&lt;/ol&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/posts/bias-variance/more-data-same-capacity.png&quot; alt=&quot;More data, same model flexibility.&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;More data, same model flexibility.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/posts/bias-variance/same-data-low-capacity.png&quot; alt=&quot;Same data, lower model flexibility.&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;Same data, lower model flexibility.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Note that reducing our model’s flexibility requires making an assumption about our data to reduce the hypothesis space. In this case, perhaps we assumed that our data is drawn from a lower order polynomial. If our constraint/assumption is not well-founded, then the model’s prediction on $x’$ will be systematically wrong, even if the prediction itself does not change much between training sets.&lt;/p&gt;

&lt;p&gt;Here is an example of a systematically wrong assumption. We have two training sets as before. The observed label for $x’$ is shown in gray.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&quot;/assets/posts/bias-variance/bias-1.png&quot; alt=&quot;&quot; width=&quot;50%&quot; /&gt;&lt;img src=&quot;/assets/posts/bias-variance/bias-2.png&quot; alt=&quot;&quot; width=&quot;50%&quot; /&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;This model visually makes a good fit to the training set, but clearly the assumption of linearity is erroneous. Our model’s prediction at $x’$ is wrong by roughly the same amount both times. It has high bias and low variance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;There is only a trade-off from variance to bias when we the experimenters run out of exploitable knowledge about the data.&lt;/strong&gt; An experimenter with perfect knowledge of the data distribution should be able to build a model that achieves 0 bias and variance (leaving only irreducible noise in the loss). In practice you will eventually reach some limit on your knowledge of the data. Researchers will sometimes try to add bias to their models in ways that reduce its variance. In some cases variance of prediction hurts a lot more than biased prediction, and in other cases you can actually get a slight reduction in overall error in the process of making this trade-off from variance to bias.&lt;/p&gt;

&lt;h2 id=&quot;flexibility-is-broken&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#flexibility-is-broken&quot;&gt;Flexibility is broken&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Model flexibility is more or less the same concept as &lt;a href=&quot;https://stats.stackexchange.com/questions/312424/what-is-the-capacity-of-a-machine-learning-model/312578#312578&quot;&gt;model capacity&lt;/a&gt;, which is the size of the hypothesis space. A flexible model can fit a more diverse set of functions. A somewhat common belief is that flexibility causes variance, or that flexibility even is variance. Flexibility in my mind is not a well defined concept, but variance is.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What is flexibility? How do you quantify it? How do you compare flexibilities?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A model can be flexible (or inflexible) in many different ways. No matter how flexible a model is, assumptions are made about its hypothesis space. These assumptions are called the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;inductive bias&lt;/span&gt;&lt;label for=&quot;83123f9b28aa2b33c82129e2ddba4096b16ea61f&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;83123f9b28aa2b33c82129e2ddba4096b16ea61f&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Defined as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Inductive_bias&quot;&gt;“set of assumptions”&lt;/a&gt;. This is qualitative. Not to be confused with statistical bias which is a computable quantity.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; of the model.&lt;/p&gt;

&lt;!--
&lt;figure&gt;&lt;img src='/assets/posts/bias-variance/inductive-bias-2.png' alt='Two models with high flexibility but very different inductive biases, applied to the same dataset.' width='50%'&gt;&lt;img src='/assets/posts/bias-variance/inductive-bias-1.png' alt='Two models with high flexibility but very different inductive biases, applied to the same dataset.' width='50%'&gt;&lt;figcaption&gt;Two models with high flexibility but very different inductive biases, applied to the same dataset.&lt;/figcaption&gt;&lt;/figure&gt;
--&gt;

&lt;p&gt;It is important to be clear that model flexibility by itself does not cause bias or variance. Bias and variance are the direct result of how that flexibility (or lack thereof) interacts with the data.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/posts/bias-variance/flexibility-2.png&quot; alt=&quot;(left) data sampled from a linear curve with large Gaussian noise, and (right) data sampled from a order-7 polynomial with small Gaussian noise. This order-7 polynomial model could be said to have high flexibility, and will have high variance on the left, but low variance on the right.&quot; width=&quot;50%&quot; /&gt;&lt;img src=&quot;/assets/posts/bias-variance/flexibility-1.png&quot; alt=&quot;(left) data sampled from a linear curve with large Gaussian noise, and (right) data sampled from a order-7 polynomial with small Gaussian noise. This order-7 polynomial model could be said to have high flexibility, and will have high variance on the left, but low variance on the right.&quot; width=&quot;50%&quot; /&gt;&lt;figcaption&gt;(left) data sampled from a linear curve with large Gaussian noise, and (right) data sampled from a order-7 polynomial with small Gaussian noise. This order-7 polynomial model could be said to have high flexibility, and will have high variance on the left, but low variance on the right.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;If you know a lot about the structure of your data and you give your model the ability to fit that structure, the variance of your model on that data may be low (picture on the right). You incur variance if that flexibility is not &lt;em&gt;useful&lt;/em&gt; (picture on the left).&lt;/p&gt;

&lt;h1 id=&quot;bias-variance-decomposition-for-any-loss&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#bias-variance-decomposition-for-any-loss&quot;&gt;Bias-Variance Decomposition For Any Loss&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Above I defined the bias-variance decomposition for ℓ2 loss. This is the standard presentation, but some will (rightly) question whether this decomposition is universal, meaning that it is true for all loss functions. The preference for ℓ2 loss in statistics is unfortunately &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error#Criticism&quot;&gt;somewhat arbitrary&lt;/a&gt;, and its popularity is mainly due to its nice math properties. I found a BV-decomposition generalized for all losses in this neat paper by &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;James et al.&lt;/span&gt;&lt;label for=&quot;1a1fe32522916b95fcdc00dec5d78c047b148223&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;1a1fe32522916b95fcdc00dec5d78c047b148223&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/1956/1b17519c58b9cb4c514dd102d08f307a5987.pdf&quot;&gt;“Generalizations of the Bias/Variance Decomposition for Prediction Error”&lt;/a&gt;, James, G., Hastie, T. (1997)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, which I will summarize here.  &lt;!-- citation --&gt;&lt;/p&gt;

&lt;p&gt;A loss $L : X \times X \rightarrow [0, \infty)$ is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Metric_(mathematics)#Definition&quot;&gt;distance&lt;/a&gt; function that satisfies,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$L(x, y) \geq 0$&lt;span style=&quot;font-style: italic; position: absolute; left: 35%&quot;&gt;non-negativity&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;$L(x, y) = 0 \Longleftrightarrow x = y$&lt;span style=&quot;font-style: italic; position: absolute; left: 35%&quot;&gt;identity&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;$L(x, y) = L(y, x)$&lt;span style=&quot;font-style: italic; position: absolute; left: 35%&quot;&gt;symmetry&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;$L$ is convex&lt;/span&gt;&lt;label for=&quot;546a46685429727b8f30be911a48289f4ea0b907&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;546a46685429727b8f30be911a48289f4ea0b907&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Stronger than requiring triangle inequality.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that $L$ operates on a single pair of values, and does not average over a dataset.&lt;/p&gt;

&lt;p&gt;James et al. defines the systematic part of random variable &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;$Y$&lt;/span&gt;&lt;label for=&quot;c03753b4b933880e5805dbef63e9d2d882c0c904&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;c03753b4b933880e5805dbef63e9d2d882c0c904&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;In this section, $Y$ is an underlying data distribution, and $\Yh$ is a model. Both are random variables. I am not showing the dependence of the model on training data and input $X$ for simplicity. In other words, I am not specifying why model $\Yh$ has variance, or even that this model has inputs and is supposed to generalize to a test set. However, everything below still holds with those details added back in.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; w.r.t. loss $L$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\s[Y] = \argmin_\theta \E_Y L(Y, \theta)&lt;/script&gt;

&lt;p&gt;This generalizes mean. Many common statistics (e.g. mean, median, mode) are &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;the systematic parts for common losses&lt;/span&gt;&lt;label for=&quot;90064962dbc49723d0f544518b2c435284bb450b&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;90064962dbc49723d0f544518b2c435284bb450b&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Source: &lt;a href=&quot;http://www.johnmyleswhite.com/notebook/2013/03/22/modes-medians-and-means-an-unifying-perspective/&quot;&gt;http://www.johnmyleswhite.com/notebook/2013/03/22/modes-medians-and-means-an-unifying-perspective/&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;$L(y, \hat{y}) =$&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Systematic part&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$(y - \hat{y})^0$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Mode&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$\lvert y - \hat{y} \rvert$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Median&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$(y - \hat{y})^2$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Arithmetic Mean&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$(\xfrac{1}{y} - \xfrac{1}{\hat{y}})^2$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Harmonic Mean&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$(\mathrm{ln}(y) - \mathrm{ln}(\hat{y}))^2$&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Geometric Mean&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{cases} \tau \cdot (y - \hat{y}) &amp; y - \hat{y} \geq 0 \\ (\tau - 1) \cdot (y - \hat{y}) &amp; \mathrm{otherwise} \end{cases} %]]&gt;&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;$\tau$-th Quantile&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;caption&gt;$\tau$ is a constant between 0 and 1. $\tau=\frac{1}{2}$ gives the median.&lt;/caption&gt;&lt;/table&gt;

&lt;!-- &lt;span class='marginnote-outer'&gt;&lt;span class='marginnote-ref'&gt;$\tau$-th Quantile&lt;/span&gt;&lt;label for='75adb4f877d352d398c8823aa64625a8f6d7862f' class='margin-toggle'&gt; &amp;#8853;&lt;/label&gt;&lt;input type='checkbox' id='75adb4f877d352d398c8823aa64625a8f6d7862f' class='margin-toggle'/&gt;&lt;span class='marginnote'&gt;&lt;span class='marginnote-inner'&gt;$\tau$ is a constant between 0 and 1. $\tau=\frac{1}{2}$ gives median.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; --&gt;

&lt;p&gt;James et al. shards the concepts of bias and variance into additional distinct concepts. For ℓ2 variance, we have the notion of &lt;em&gt;typical&lt;/em&gt; distance from some reference point of interest. James et al. points out that there are other ways to define variance which become equivalent under ℓ2 loss, but are not the same in general for other losses.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\V_\Yh[\Yh]
&amp; = \E_\Yh(\Yh - \E_\Yh\Yh)^2 \\
&amp; = \E_{Y,\Yh}(Y - \Yh)^2 - \E_Y(Y - \E_\Yh\Yh)^2 \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Likewise, bias squared can be written in two different ways.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\Bias[Y, \Yh]^2
&amp; = (\E_YY-\E_\Yh\Yh)^2 \\
&amp; = \E_Y(Y-\E_\Yh\Yh)^2 - \E_Y(Y-\E_YY)^2 \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;When we generalize loss and mean, these alternative ways of writing bias and variance become distinct statistical operations.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;variance effect&lt;/strong&gt;, $\textrm{VE}[Y, \Yh]$, is the expected change in prediction error when using $\Yh$ instead of $\s[\Yh]$ to predict $Y$. In other words, it measures the effect of predicting with a distribution and all its variability, vs the constant systematic part. This contrasts with intrinsic variance, $\V_\Yh[\Yh]$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\V_\Yh[\Yh] &amp;= \E_\Yh L(\Yh, \s[\Yh]) \\
\textrm{VE}[Y, \Yh] &amp;= \E_{Y,\Yh}[L(Y, \Yh) - L(Y, \s[\Yh])] \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;&lt;strong&gt;systematic effect&lt;/strong&gt;&lt;/span&gt;&lt;label for=&quot;5c8e80a5b3081b4f49de63bc6ff8c48d7b853f1f&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;5c8e80a5b3081b4f49de63bc6ff8c48d7b853f1f&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Not to be confused with systematic part. Not the terminology I would have gone with.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, $\textrm{SE}[Y, \Yh]$, is the expected change in prediction error when using $\s[\Yh]$ instead of $\s[Y]$ to predict $Y$. In other words, it measures the effect of predicting with the systematic part of the model, vs the systematic part of the data distribution. This contrasts with intrinsic bias squared, $\Bias[Y, \Yh]^2$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\Bias[Y, \Yh]^2 &amp;= L(\s[Y], \s[\Yh]) \\
\textrm{SE}[Y, \Yh] &amp;= \E_Y[L(Y, \s[\Yh]) - L(Y, \s[Y])] \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We may now state the generalized decomposition in terms of systematic and variance effect:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E_{Y,\Yh}L(Y, \Yh) = \textrm{SE}[Y, \Yh] + \textrm{VE}[Y, \Yh] + \V_Y[Y]&lt;/script&gt;

&lt;p&gt;Plugging in, we get,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\E_{Y,\Yh}L(Y, \Yh)
&amp;= \E_Y[L(Y, \s[\Yh]) - L(Y, \s[Y])] \\
&amp;\phantom{=}\, + \E_{Y,\Yh}[L(Y, \Yh) - L(Y, \s[\Yh])] \\
&amp;\phantom{=}\, + \E_YL(Y, \s[Y])
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Check out James et al. for some application examples of the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;generalized BV-decomposition&lt;/span&gt;&lt;label for=&quot;347f9f52dcd30e516865dc101e6c5330acc524e4&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;347f9f52dcd30e516865dc101e6c5330acc524e4&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;There exist more exotic BV-decompositions as well, e.g.
&lt;a href=&quot;https://math.stackexchange.com/questions/3017916/bias-variance-decomposition-for-kl-divergence&quot;&gt;KL-divergence&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-is-variance-anyway&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#what-is-variance-anyway&quot;&gt;What is variance anyway?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I was taught that variance is exactly equal to its ℓ2 &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;definition&lt;/a&gt;. I never questioned this until I had a heated argument with a friend who pointed out that variance was a collection of concepts before it was given a formula. The colloquial term &lt;em&gt;variance&lt;/em&gt; is defined as &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;“the fact or quality of being different, divergent, or inconsistent”&lt;/span&gt;&lt;label for=&quot;9bae4a6d7d13008870d9e32008ea0bc7daca6a45&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;9bae4a6d7d13008870d9e32008ea0bc7daca6a45&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;&lt;a href=&quot;https://www.google.com/search?q=define+variance&quot;&gt;https://www.google.com/search?q=define+variance&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. As it turns out, there are many ways to formalize aspects of this word, and one is not necessarily better than the other.&lt;/p&gt;

&lt;p&gt;Ultimately, the word &lt;em&gt;variance&lt;/em&gt;, when used in the context of statistics, will mean ℓ2 variance by convention. &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_dispersion&quot;&gt;Statistical dispersion&lt;/a&gt; is the technical name given to the umbrella of variance formulations. For example, &lt;a href=&quot;https://en.wikipedia.org/wiki/Median_absolute_deviation&quot;&gt;Median Absolute Deviation&lt;/a&gt; (MAD) is a not-so-obscure alternative to ℓ2 variance for all you &lt;a href=&quot;https://creativemaths.net/blog/median/&quot;&gt;median-lovers&lt;/a&gt; out there. &lt;a href=&quot;https://en.wikipedia.org/wiki/Entropy_(information_theory)&quot;&gt;Entropy&lt;/a&gt; is another one. Though not usually thought of as a measure of variance, entropy measures the spread of a distribution without distance to a reference point, which makes it particularly useful for &lt;a href=&quot;https://en.wikipedia.org/wiki/Categorical_variable&quot;&gt;categorical data&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I think formalizing concepts is a big deal, but I don’t take formalisms as absolute truth. They are more like pieces of code that can be connected together and modified. I am picky when it comes to the &lt;em&gt;API&lt;/em&gt; that a math definition uses, i.e. the objects that are abstracted away. I expect my formula to be general purpose, but there is not going to be a definition that captures all possible cases of an idea. My philosophy is to start with good math, and use it to understand the nuances of an idea. Then use intuition and creativity to think about the idea in new ways, and potentially invent a new formalism. The math you started with gives you a foundation for building and a sandbox for playing.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgments&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#acknowledgments&quot;&gt;Acknowledgments&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;I want to thank John Chung, Frazer Kirkman, Ivan Vendrov, and Roman Novak for their valuable discussions and feedback on this post. I give special thanks to Jeremy Nixon who inspired me to research this topic thoroughly and offered insights on the interaction between mathematics and informal ideas in research.&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Jul 2019 00:00:00 -0700</pubDate>
        <link>pragmanym.github.io/zhat/articles/bias-variance</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/bias-variance</guid>
        
        
        <category>primer</category>
        
      </item>
    
  </channel>
</rss>
