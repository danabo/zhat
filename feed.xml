<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Z-Hat</title>
    <description>A (we)blog devoted to finding better representations</description>
    <link>pragmanym.github.io/zhat/</link>
    <atom:link href="pragmanym.github.io/zhat/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 11 Jun 2020 17:14:37 -0700</pubDate>
    <lastBuildDate>Thu, 11 Jun 2020 17:14:37 -0700</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Notes: Dutch Book Argument</title>
        <description>&lt;!--more--&gt;

&lt;ul class=&quot;toc&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#axioms-of-probability&quot; id=&quot;markdown-toc-axioms-of-probability&quot;&gt;Axioms of probability&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#axioms-of-probability-for-propositional-logic&quot; id=&quot;markdown-toc-axioms-of-probability-for-propositional-logic&quot;&gt;Axioms of probability for propositional logic&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#i-if-not-bayesian--sure-loss-is-possible&quot; id=&quot;markdown-toc-i-if-not-bayesian--sure-loss-is-possible&quot;&gt;I. If not Bayesian ⟹ sure loss is possible&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#but-how-does-this-lead-to-bayes-rule&quot; id=&quot;markdown-toc-but-how-does-this-lead-to-bayes-rule&quot;&gt;But how does this lead to Bayes rule?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#but-is-real-life-a-series-of-bets&quot; id=&quot;markdown-toc-but-is-real-life-a-series-of-bets&quot;&gt;But is real-life a series of bets?&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ii-if-bayesian--sure-loss-is-not-possible&quot; id=&quot;markdown-toc-ii-if-bayesian--sure-loss-is-not-possible&quot;&gt;II. If Bayesian ⟹ sure loss is not possible&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Main source: &lt;a href=&quot;https://plato.stanford.edu/entries/dutch-book/&quot;&gt;Dutch Book Arguments (SEP)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Dutch Book Theorem:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Given a set of betting quotients that fails to satisfy the probability axioms, there is a set of bets with those quotients that guarantees a net loss to one side.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Converse Dutch Book Theorem:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;For a set of betting quotients that obeys the probability axioms, there is no set of bets (with those quotients) that guarantees a sure loss (win) to one side.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://www.stat.berkeley.edu/~census/dutchdef.pdf&quot;&gt;https://www.stat.berkeley.edu/~census/dutchdef.pdf&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Dutch book cannot be made against a Bayesian bookie.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I. If not Bayesian ⟹ sure loss is possible&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://link.springer.com/chapter/10.1007%2F978-1-4612-0919-5_10&quot;&gt;Foresight: Its Logical Laws, Its Subjective Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;II. If Bayesian ⟹ sure loss is not possible&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jstor.org/stable/2268221?seq=1&quot;&gt;On Confirmation and Rational Betting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jstor.org/stable/2268222&quot;&gt;Fair Bets and Inductive Probabilities&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Counter-arguments:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://link.springer.com/article/10.1023/A:1004996226545&quot;&gt;Hidden Assumptions in the Dutch Book Argument&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;axioms-of-probability&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#axioms-of-probability&quot;&gt;Axioms of probability&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;The axioms of probability:&lt;/strong&gt;&lt;br /&gt;
Let $(\Omega, \mathcal{E}, P)$ be a measure space, where $\Omega$ is the sample set (mutually exclusive outcomes), $\mathcal{E}$ is the event set (set of measurable subsets of $\Omega$), and $P$ is the probability measure ($P(E),\ \forall E \in \mathcal{E}$ is well defined).&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$P(E) \geq 0,\ \forall E \in \mathcal{E}$&lt;/li&gt;
  &lt;li&gt;$P(\Omega) = 1$&lt;/li&gt;
  &lt;li&gt;$P(E_1 \cup E_2) = P(E_1) + P(E_2) \iff E_1 \cap E_2 = \emptyset,\ \forall E_1,E_2 \in \mathcal{E}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that $P(E) \leq 1,\ \forall E \in \mathcal{E}$ follows directly from the axioms.&lt;/p&gt;

&lt;h2 id=&quot;axioms-of-probability-for-propositional-logic&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#axioms-of-probability-for-propositional-logic&quot;&gt;Axioms of probability for propositional logic&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;We can define probability over propositional statements. The sample set $\Omega$ is the set of all truth values of the primitives. If $(A_1, A_2, \ldots)$ is the set of all primitive propositions, then $\Omega = {(\mathrm{False}, \mathrm{False}, \ldots), (\mathrm{True}, \mathrm{False}, \ldots), (\mathrm{False}, \mathrm{True}, \ldots), (\mathrm{True}, \mathrm{True}, \ldots), \ldots}$ is every possible truth assignment for $(A_1, A_2, \ldots)$. This is assuming that we don’t know the truth value of any primitive. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Logical_connective&quot;&gt;logical connectives&lt;/a&gt;, $\wedge, \vee, \neg,$ etc., are all shorthands for constructing events (sets of truth assignments for $(A_1, A_2, \ldots)$). In other words, $P(H)$ is shorthand for the probability that proposition $H$ is true, where $H$ denotes an event $E$ containing exactly every truth assignment for $(A_1, A_2, \ldots)$ which makes $H$ true.&lt;/p&gt;

&lt;p&gt;Note that when there are finitely many $A_i$, there will be finitely many possible events. However, there are infinitely many logical propositions over finitely many primitives $A_i$. This is because most logical propositions are equivalent to others. In other words, we are making equivalence class over the set of propositions using the sets of primitive assignments that make them true. The set of equivalence classes over propositions is finite for finitely many primitives.&lt;/p&gt;

&lt;p&gt;Now the axioms for probability over propositional logical are just a special case of the general axioms:&lt;br /&gt;
Let $\mathcal{H}$ be the set of all logical propositions.&lt;br /&gt;
Let $\mathrm{True}$ be the proposition &lt;em&gt;True&lt;/em&gt;, which is satisfied by all truth assignments of the primitives (i.e. the event containing all samples).&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$P(H)\geq 0,\ \forall H \in \mathcal{H}$&lt;/li&gt;
  &lt;li&gt;$P(\mathrm{True}) = 1$&lt;/li&gt;
  &lt;li&gt;$P(H_1 \vee H_2) = P(H_1) + P(H_2) \iff \neg (H_1 \wedge H_2),\ \forall H_1, H_2 \in \mathcal{H}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Axiom 3 states that the probability of $H_1$ or $H_2$ is the sum of their probabilities iff $H_1$ and $H_2$ cannot both be true at the same time. $H_1 \wedge H_2$ constructs the set of primitive assignments where both propositions are true, which is just the intersection of their respective events, $E_1 \cap E_2$. However, $H_1 \wedge H_2 = \mathrm{False}$ where  $\mathrm{False}$ is the empty event is unconventional, so instead we write $\neg (H_1 \wedge H_2)$ which is equivalent to $\overline{E_1 \cap E_2} = \Omega$ (complement). One could also write $P(H_1 \vee H_2) = P(H_1) + P(H_2) \iff P(H_1 \wedge H_2) = 0$.&lt;/p&gt;

&lt;p&gt;We could also define probability over 1st order logic. Now $A(x)$ is a proposition on $x$, where $x$ is a non-proposition type (e.g. number). Let’s say $x$ is a natural number, then we have infinitely many primitive propositions $A(x)$ for each $x \in \mathbb{N}$.&lt;/p&gt;

&lt;p&gt;How does this inform the claims made in &lt;a href=&quot;https://meaningness.com/probability-and-logic&quot;&gt;https://meaningness.com/probability-and-logic&lt;/a&gt; ?&lt;/p&gt;

&lt;h1 id=&quot;i-if-not-bayesian--sure-loss-is-possible&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#i-if-not-bayesian--sure-loss-is-possible&quot;&gt;I. If not Bayesian ⟹ sure loss is possible&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;The Dutch book argument (DBA) uses traditional terminology around betting which I find to be confusing in this context, like &lt;em&gt;bookie&lt;/em&gt;, &lt;em&gt;agent&lt;/em&gt;, &lt;em&gt;making book&lt;/em&gt;, etc., so I will take care to clarify the meaning of all these things.&lt;/p&gt;

&lt;p&gt;Consider an asymmetric two-player betting game. Player 1 transacts with player 2 who sets prices.&lt;/p&gt;

&lt;p&gt;Player 1:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Called the &lt;em&gt;agent&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Chooses what bets to buy or sell to/from the bookie at the bookie’s prices.&lt;/li&gt;
  &lt;li&gt;Chooses the stakes.&lt;/li&gt;
  &lt;li&gt;Accepts the bookie’s prices.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Player 2:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Called the &lt;em&gt;bookie&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Chooses the prices for bets.&lt;/li&gt;
  &lt;li&gt;Must buy or sell any bets the agent requests, at whatever stakes the agent requests.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The DBA shows that the agent can take advantage of the bookie (make book against the bookie) iff the bookie’s bet prices do not conform the the axioms of probability. Here taking advantage of means transacting (buying/selling) a set of bets with the bookie to guarantee the agent wins money off the bookie in every scenario.&lt;/p&gt;

&lt;p&gt;We assume the bookie makes all prices on all possible bets known to the agent from the start, and the bookie cannot change these prices. The bookie wants to choose prices such that the agent cannot make book against him/her.&lt;/p&gt;

&lt;p&gt;A bet is defined by a stake $S$, betting quotient $q$, and target event $E$. When the agent buys a bet (bookie sells a bet) at stake $S$ with quotient $q$, the agent’s payoff is $S-qS$ if $E$ occurs, and $-qS$ otherwise. $S$ is only payed out to the agent (holder of the stake) when the betting target $H$ is true. $qS$ is paid to the seller regardless of outcome. This is a fee.&lt;/p&gt;

&lt;p&gt;The agent can also sell a bet to the bookie, which just negates the entries in the table. The bookie gives the agent a fee, and the agent pays out the stake to the bookie if $H$ is true.&lt;/p&gt;

&lt;p&gt;We saw how probability over logical propositions is a special case. I think it is easier to reason about DBA if we instead consider an arbitrary probability distribution over events $E \in \mathcal{E}$. These events are the possible targets of bets. The bookie must choose $q(E)$ for each event. $q(E)$ will end up being a probability measure over $\mathcal{E}$. We will now show that if $q(E)$ violates any of the axioms, the agent can make book against the bookie.&lt;/p&gt;

&lt;p&gt;Define a bet as a function $B : \Omega \to \mathbb{R}$ from samples to payoffs. A bet on event $E$ with stake $S$ and quotient $q$ has payoffs (w.r.t. buyer):&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
B_E(\omega) = \begin{cases}S-q(E)S &amp; \omega \in E \\ -q(E)S &amp; \omega \notin E\end{cases} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This can be represented as a table:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Result&lt;/th&gt;
      &lt;th&gt;Payoff&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$E$&lt;/td&gt;
      &lt;td&gt;$S-q(E)S$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\overline{E}$&lt;/td&gt;
      &lt;td&gt;$-q(E)S$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Assuming the stake $S$ is always the same (this argument is invariant to stake, as long as its positive), then a bet is represented by $B_E$. Since this game is zero-sum, from the seller’s perspective, the payoff is $-B_E$. Buying $-B_E$ is equivalent to selling $B_E$. We can also add bets like this&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\left(B_{E_1} + B_{E_2}\right)(\omega) = B_{E_1}(\omega) + B_{E_2}(\omega)\,,&lt;/script&gt;&lt;br /&gt;
to construct a more complicated multi-outcome bet, denoted as $B_{E_1} + B_{E_2}$.&lt;/p&gt;

&lt;p&gt;Now I am ready to outline why bets should conform to the three axioms:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Axiom 1:&lt;/strong&gt; $P(E) \geq 0,\ \forall E \in \mathcal{E}$.&lt;br /&gt;
Assume $q(E) &amp;lt; 0$ for some $E$.&lt;br /&gt;
Then the agent will buy $B_E$, which has a positive payoff in all cases.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Axiom 2:&lt;/strong&gt; $P(\Omega) = 1$.&lt;br /&gt;
Note that by definition event $\Omega$ always happens, and this is known to both the agent and bookie.&lt;br /&gt;
Assume $q(\Omega) &amp;lt; 1$.&lt;br /&gt;
Then the agent will buy $B_\Omega$ since the payoff is always $S-q(\Omega)S$ which is positive.&lt;br /&gt;
Assume $q(\Omega) &amp;gt; 1$.&lt;br /&gt;
Then the agent will buy -$B_\Omega$ (sell $B_\Omega$) since the payoff is always $-(S-q(\Omega)S)$ which is positive.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Axiom 3:&lt;/strong&gt; $P(E_1 \cup E_2) = P(E_1) + P(E_2) \iff E_1 \cap E_2 = \emptyset,\ \forall E_1,E_2 \in \mathcal{E}$.&lt;br /&gt;
Note that by definition $E_1 \cap E_2 = \emptyset$ implies $E_1$ and $E_2$ cannot happen simultaneously (by definition of the empty event), and this known to both the agent and bookie.&lt;br /&gt;
Assume $E_1 \cap E_2 = \emptyset$ for some $E_1, E_2$.&lt;br /&gt;
Assume $q(E_1 \cup E_2) &amp;gt; q(E_1) + q(E_2)$.&lt;br /&gt;
Then the agent will buy $B_{E_1} + B_{E_2} - B_{E_1 \cup E_2}$ which has payoff table (w.r.t. agent):&lt;br /&gt;
| Result | Payoff   | &lt;br /&gt;
| —— | ——– | &lt;br /&gt;
| $E_1$  | $-(q(E_1) + q(E_2) - q(E_1 \cup E_2))S$     | &lt;br /&gt;
| $E_2$  | $-(q(E_1) + q(E_2) - q(E_1 \cup E_2))S$     |&lt;br /&gt;
| $\overline{E_1 \cup E_2}$  | $-(q(E_1) + q(E_2) - q(E_1 \cup E_2))S$     |&lt;br /&gt;
The payoff is the same in all cases and $E_1 \cap E_2$ never occurs. $-(q(E_1) + q(E_2) - q(E_1 \cup E_2))S$ is positive.&lt;br /&gt;
Assume $q(E_1 \cup E_2) &amp;lt; q(E_1) + q(E_2)$.&lt;br /&gt;
Then the agent buys $-B_{E_1} - B_{E_2} + B_{E_1 \cup E_2}$, which is easy to show wins money for the agent in every scenario.&lt;/p&gt;

&lt;p&gt;Thus it is wise for the bookie to choose $q : \mathcal{E} \to \mathbb{R}$ s.t. it obeys the three axioms of probability.&lt;/p&gt;

&lt;p&gt;The DBA is ingenious because it does not assume any a priori probabilities over outcomes (i.e. objective probability), and it holds for 1-shot events (i.e. does not assume the game is repeatable).&lt;/p&gt;

&lt;h2 id=&quot;but-how-does-this-lead-to-bayes-rule&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#but-how-does-this-lead-to-bayes-rule&quot;&gt;But how does this lead to Bayes rule?&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Bayesian epistemology centers around using Bayes rule to compute a posterior from a prior. Where is the prior and posterior here?&lt;/p&gt;

&lt;p&gt;$q(E)$ is not a prior because $E$ is a datum, not a hypothesis. The DBA concludes that $q$ should be a valid probability measure. But how do we do all the fancy stuff that Bayesian inference requires like marginalizing over variables and computing conditional probabilities? To do that, we need at least two random variables, which we could define over our sample space. What would those two RVs be?&lt;/p&gt;

&lt;p&gt;In the framing of DBA, the world starts out completely unknown, and at the conclusion of the betting becomes completely known. There is no reason for a prior or posterior distribution. $q(E)$ is a likelihood distribution conditioned on nothing, i.e. probability of data without regard to hypotheses. There is nothing Bayesian about this because we are &lt;em&gt;literally not using Bayes’ rule because we only have one random variable!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;However, the DBA is clearly suggesting that $q(E)$ should encode the bookie’s beliefs about the outcomes, and is thus the prior. So then what is the posterior? We can get around this conundrum by supposing the bookie takes bets on the outcomes of some underlying process, i.e. time series, and updates $q(E)$ as time passes and outcomes are observed. Now we are computing a type of posterior: $q(E_t \mid E_{1:t-1})$ where $E_{1:t-1}$ is all previous observations. Hypotheses are technically not needed, but the bookie is free to secretly have a second RV over hypotheses under the hood (maybe the bookie is doing Solomonoff induction).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Are we confusing frequentist and Bayesian probability here?&lt;br /&gt;
In the Bayesian paradigm, hypotheses are themselves usually probability distributions, i.e. $p(X \mid H=h) = p_h(X)$ where $p_h(X)$ is a hypothesis labeled with $h$. What is the meaning of the probabilities in $p_h(X)$? Are these probabilities objective? If not, what does it mean for a hypothesis to be satisfied by data? We could consider likelihood to be a score, rather than an objective quantity, and a better hypothesis has a better score by definition (rather than thinking of the likelihood of data under the hypothesis as a frequentist prediction that can be tested through repeated experiment).&lt;/p&gt;

&lt;h2 id=&quot;but-is-real-life-a-series-of-bets&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#but-is-real-life-a-series-of-bets&quot;&gt;But is real-life a series of bets?&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The setup of the game described above ends up being isomorphic to probability theory.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Why does this isomorphism exist? Is there something intrinsic about betting that makes it conform to the rules of probability, or is this an artifact of the particular betting payout definition we are using?&lt;br /&gt;
This payout scheme is apparently economically justified and not arbitrarily chosen, i.e. bets (with predetermined payouts) traded on a market with have purchase prices that converge to the model above (assuming sufficient arbitrage). Note, IRL quotients are discretized and don’t sum exactly to 1, and there is a ask-bid spread which essentially ads a transaction cost to everything. Real world example: &lt;a href=&quot;https://www.predictit.org/markets/detail/3698&quot;&gt;https://www.predictit.org/markets/detail/3698&lt;/a&gt;. In economics, decisions under uncertainty are modeled in the same betting form. e.g. insurance (premium is the quotient, payout is the stake).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; During the course of everyday life, is the universe going to make book against us lest we conform to the rules of probability? Do the sorts of real-life bets we actually encounter and place have the same structure as the idealized betting above?&lt;/p&gt;

&lt;p&gt;Who is the bookie and agent? DBA says that you are the bookie over the course of your life, and you want to prevent the universe (or adversarial actors) from taking advantage of you. The problem is that you are often the one making decisions, i.e. deciding the bets to place. This is a different game, where the bookie chooses the quotients and the bets together. Also, real-life is not zero-sum. You will encounter win-win and lose-lose situations where you have to place a bet one way or the other and net win or lose. The universe is not an optimally rational agent either. I don’t expect the universe to spontaneously Dutch-book me. I don’t even expect people to Dutch-book me, because that would take work. In practice, everyone is not acting optimally.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What if outcomes are not binary? i.e. $\omega \in E$ and $\omega \notin E$ are not the only possibilities (i.e. don’t assume law of excluded middle, i.e. need to construct a proof for $\omega \in E$ or $\omega \notin E$).&lt;br /&gt;
For example, what if it is not always possible to determine whether an event occurred? This is the case in Solomonoff induction, which uses a semi-measure rather than a measure to get around this problem. In practice, with things like elections and trials there is a large vested interest in ensuring an outcome is determined. But over the course of everyday life, there are many ambiguities.&lt;/p&gt;

&lt;p&gt;In real life, you are more like the agent. You choose your bets (take actions) with pre-defined payoffs (the downstream results of your actions are not usually in your control). These payoff are not logically determined, but are the result of (often arbitrary) circumstance. It is very easy to Dutch book the universe! That’s generally how growth and progress happen.&lt;/p&gt;

&lt;p&gt;Presumably in a formal betting scenario the bookie’s probabilities are well-tuned, so that the bookie is indifferent to whether someone buys or sells a given bet. In everyday life, the payoffs of your decisions usually do not match your preferred betting quotients, so that there is one or a few best bets. The whole point of betting is that you believe the outcomes don’t match the “true” betting quotients. The DBA assumes that someone else might give you a series of bets which are locally in agreement with your quotients but globally a guaranteed loss. The problem is, you may not be compelled to take bets that agree with your expectations, but only take bets where the expected return is positive, i.e. disagreement.&lt;/p&gt;

&lt;h1 id=&quot;ii-if-bayesian--sure-loss-is-not-possible&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#ii-if-bayesian--sure-loss-is-not-possible&quot;&gt;II. If Bayesian ⟹ sure loss is not possible&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;TODO&lt;/p&gt;

</description>
        <pubDate>Thu, 11 Jun 2020 00:00:00 -0700</pubDate>
        <link>pragmanym.github.io/zhat/articles/notes-dutch-book-argument</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/notes-dutch-book-argument</guid>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>Notes: Complete Class Theorems</title>
        <description>&lt;!--more--&gt;

&lt;ul class=&quot;toc&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#results-to-understand-in-hoff&quot; id=&quot;markdown-toc-results-to-understand-in-hoff&quot;&gt;Results to understand in Hoff&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#notes&quot; id=&quot;markdown-toc-notes&quot;&gt;Notes&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#complete-class-theorem-i&quot; id=&quot;markdown-toc-complete-class-theorem-i&quot;&gt;Complete class theorem I&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#complete-class-theorem-ii&quot; id=&quot;markdown-toc-complete-class-theorem-ii&quot;&gt;Complete class theorem II&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#euclidean-parameter-spaces&quot; id=&quot;markdown-toc-euclidean-parameter-spaces&quot;&gt;Euclidean parameter spaces&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#complete-class-theorem-iii&quot; id=&quot;markdown-toc-complete-class-theorem-iii&quot;&gt;Complete class theorem III&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#interpretation-and-implications&quot; id=&quot;markdown-toc-interpretation-and-implications&quot;&gt;Interpretation and implications&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#discussion&quot; id=&quot;markdown-toc-discussion&quot;&gt;Discussion&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; I want to understand the complete class theorems because they are a common argument for Bayesian epistemology, a theory of knowledge that puts forward Bayesian posterior calculation as all you need. In order to properly evaluate whether “being Bayesian” is enough of a theoretical framework to build and explain intelligence, I need to understand arguments for Bayesian epistemology.&lt;/p&gt;

&lt;p&gt;The argument boils down to:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;If you agree with expected utility as your objective, then you have to be Bayesian.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In a nutshell: An strategy is inadmissible if there exists another strategy that is as good in all situations and strictly better in at least one. If you want your strategy to be admissible, it should be equivalent to a Bayes estimator.&lt;/p&gt;

&lt;p&gt;Complete class theorems: Only Bayes strategies are admissible, and admissible strategies are Bayes.&lt;/p&gt;

&lt;p&gt;I’m mainly following &lt;a href=&quot;https://www.stat.washington.edu/people/pdhoff/courses/581/LectureNotes/admiss.pdf&quot;&gt;Admissibility and complete classes - Peter Hoff&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Related study notes: &lt;a href=&quot;https://docs.google.com/document/d/1fCseo1fsPwJfjnehauAzOr4bf1GHHRfRW6cHwNQTNu4/edit&quot;&gt;Wald’s Complete Class Theorem(s) - study notes&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;results-to-understand-in-hoff&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#results-to-understand-in-hoff&quot;&gt;Results to understand in &lt;a href=&quot;https://www.stat.washington.edu/people/pdhoff/courses/581/LectureNotes/admiss.pdf&quot;&gt;Hoff&lt;/a&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Section 1&lt;/strong&gt;:&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/KSZ6PVb.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Section 2&lt;/strong&gt;:&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/F94ljVs.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/2QW8pcP.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Section 3&lt;/strong&gt;:&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/U2npCDa.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Section 4&lt;/strong&gt;:&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/XPqhZ4E.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/H8uda4H.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/fAvOCcu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Section 5&lt;/strong&gt; covers similar results for infinite parameter spaces (so far results are for finite parameter spaces).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Section 6&lt;/strong&gt;:&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/B9EDHSE.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/TQjLvpT.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/RRj8Mwi.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/XVFp6DY.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;notes&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#notes&quot;&gt;Notes&lt;/a&gt;&lt;/h1&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\newcommand{\bb}{\mathbb}
\newcommand{\mc}{\mathcal}
\newcommand{\d}{\delta}
\newcommand{\p}{\pi}
\newcommand{\t}{\theta}
\newcommand{\T}{\Theta}
\newcommand{\fa}{\forall}
\newcommand{\ex}{\exists}
\newcommand{\real}{\bb{R}}
\newcommand{\E}{\bb{E}}
\renewcommand{\D}[1]{\operatorname{d}\!{#1}}
\DeclareMathOperator*{\argmin}{argmin}&lt;/script&gt;

&lt;p&gt;Let $(\mc{X}, \mc{A}, P_\t)$ be a probability space for all $\t \in \T$.&lt;br /&gt;
$\mc{X}$ is the sample space.&lt;br /&gt;
$\T$ is the parameter space.&lt;br /&gt;
$\mc{P} = \{P_\t : \t \in \T\}$ is the &lt;em&gt;model&lt;/em&gt;, i.e. the set of all probability measures specified by the parameter space.&lt;/p&gt;

&lt;p&gt;We wish to estimate some unknown $g(\t)$ which depends in a known way on $\t$. The text does not tell us what type $g(\t)$ is, and it does not matter for the discussion since it will always be hidden behind our loss function. The text uses $g(\T)$ (the image of $g$) to denote the space of all such $g$, but I find it less confusing and more direct to use $G = g(\T)$.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;loss function&lt;/strong&gt; is a function $L : \T \times G \to \real^+$ which is always 0 for equivalent inputs, i.e.&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;L(\t, g(\t)) = 0,\ \fa \t \in \T\,.&lt;/script&gt;&lt;br /&gt;
Note that $L(\t_1, g(\t_2))$ may be 0 when $\t_1 \neq \t_2$.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;non-randomized estimator&lt;/strong&gt; for $g(\t)$ is a function $\d : \mc{X} \to G$ s.t. $x \mapsto L(\t, \d(x))$ is a measurable function (of $x$) for all $\t \in \T$. A &lt;a href=&quot;https://en.wikipedia.org/wiki/Measurable_function&quot;&gt;function is measurable&lt;/a&gt; if the preimage of any measurable set is measurable, i.e. it preserves measurability. Concretely in this case, $\{x : L(\t, \d(x)) \in B\} \in \mc{A}$ for all $B \in \mc{B}(\real)$, where $\mc{A}$ is our event space (set of all subsets of $\mc{X}$ which can be measured by $P_\t$), and $\mc{B}(\real)$ is the &lt;a href=&quot;https://mathworld.wolfram.com/BorelSet.html&quot;&gt;Borel $\sigma$-algebra&lt;/a&gt; over the reals, which is a standard definition of measurable sets of reals (unions and intersections of closed and open intervals are measurable). Presumably $\d$ is non-randomized because it only depends on the ground truth $x$.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;risk function&lt;/strong&gt; of estimator $\d$ is the expected loss:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;R(\t, \d) = \E_{x \sim X}\left[L(\t, \d(x)) \mid \t\right] = \int_\mc{X} L(\t, \d(x))P_\t(x) \D{x}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;randomized estimator&lt;/strong&gt; is a function $\d : \mc{X} \times [0, 1] \to G$ s.t. $(x, u) \mapsto L(\t, \d(x, u))$ is a measurable function (of $x$ and $u$) for all $\t \in \T$. Just like a non-randomized estimator, except it recieves noise from $U \sim \mathrm{uniform}([0, 1])$ as input. Non-randomized estimators are a special case (ignores the random input). Conversely, a randomized estimator can be viewed as a distribution over non-randomized estimators (which are parametrized by $u \in [0, 1]$).&lt;/p&gt;

&lt;p&gt;The risk function then integrates over $u$:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;R(\t, \d) = \E_{x \sim X, u \sim U}\left[L(\t, \d(x, u)) \mid \t\right] = \int_0^1 \int_\mc{X} L(\t, \d(x, u))P_\t(x) \D{x} \D{u}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;An estimator $\d_1$ &lt;strong&gt;dominates&lt;/strong&gt; another estimator $\d_2$ iff&lt;br /&gt;
\begin{align}&lt;br /&gt;
\fa \t \in \T,\ R(\t, \d_1) \leq R(\t, \d_2)\,, \&lt;br /&gt;
\ex \t \in \T,\ R(\t, \d_1) &amp;lt; R(\t, \d_2)\,.&lt;br /&gt;
\end{align}&lt;br /&gt;
$\d_1$ must be at least as good (same risk or less) as $\d_2$ in every situation, and must be strictly better (less risk) in at least one situation, for the descriptor &lt;em&gt;dominance&lt;/em&gt; to apply.&lt;/p&gt;

&lt;p&gt;An estimator $\d$ is &lt;strong&gt;admissible&lt;/strong&gt; if it is not dominated by any estimator.&lt;br /&gt;
Admissibility does not mean an estimator is any good, however, but any inadmissible estimator can be automatically ruled out.&lt;/p&gt;

&lt;p&gt;Let $\mc{D}$ be the set of all randomized estimators.&lt;br /&gt;
A &lt;strong&gt;class&lt;/strong&gt; (subset) of estimators $\mc{C} \subset \mc{D}$ is &lt;strong&gt;complete&lt;/strong&gt; iff $\fa \d’ \in \mc{C}^c,\ \ex \d \in \mc{C}$ that dominates $\d’$.&lt;br /&gt;
Here $(\cdot)^c$ is the compliment operator, i.e. $\mc{C}^c = \{\d’ \in \mc{D} : \d’ \notin \mc{C}\}$.&lt;/p&gt;

&lt;p&gt;Let $\p$ be a probability measure on $\T$ and $\d$ be an estimator (from here on it does not matter if $\d$ is randomized or not because the risk does not depend on the arguments of $\d$).&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Bayes risk&lt;/strong&gt; of $\d$ w.r.t. $\p$ is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(\p, \d) = \E_{\p(\t)}[R(\t, \d)] = \int_\T R(\t, \d) \p(\t) \D{\t}\,.&lt;/script&gt;

&lt;p&gt;This is the expected risk w.r.t. $\p(\t)$, which is called our &lt;strong&gt;prior&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Bayes risk allows us to compare estimators by comparing numbers rather than functions, but now we have a new problem, which is that we have to choose a prior.&lt;/p&gt;

&lt;p&gt;$\d$ is a &lt;strong&gt;Bayes estimator&lt;/strong&gt; w.r.t. $\p$ iff&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(\p, \d) \leq R(\p, \d'),\ \fa \d' \in \mc{D}\,.&lt;/script&gt;

&lt;p&gt;Note that a Bayes estimator $\t$ can be dominated if $\pi$ assigns measure 0 to some subsets of $\T$. It is easy to show that if $\t$ is dominated by $\t’$, then $\t’$ is also Bayes and $R(\p, \d) = R(\p, \d’)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt; (Bayes $\implies$ admissible): If prior $\pi(\theta)$ has exactly one Bayes estimator, then that estimator is admissible.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Thus the only thing that can dominate a Bayes estimator is another Bayes estimator. If there is only one Bayes estimator for a given prior, then it must be admissible.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Under what conditions is there more than one Bayes estimator for a given prior?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3&lt;/strong&gt; (Bayes $\implies$ admissible):&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/9H363wf.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;complete-class-theorem-i&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#complete-class-theorem-i&quot;&gt;Complete class theorem I&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;(admissible $\implies$ Bayes)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If $\d$ is admissible and $\T$ is finite, then $\d$ is Bayes (w.r.t some prior distribution).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;complete-class-theorem-ii&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#complete-class-theorem-ii&quot;&gt;Complete class theorem II&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Class of Bayes estimators is complete&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;If $\T$ is finite and $\mc{S}$ is closed then the class of Bayes rules is complete and the admissible rules form a minimal complete class.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;euclidean-parameter-spaces&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#euclidean-parameter-spaces&quot;&gt;Euclidean parameter spaces&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;TODO: generalized Bayes estimator&lt;br /&gt;
TODO: limiting Bayes estimator&lt;/p&gt;

&lt;p&gt;Bayes $\implies$ Admissible&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/BZJqBWn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Admissible $\implies$ Bayes&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/CGYqfbF.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;complete-class-theorem-iii&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#complete-class-theorem-iii&quot;&gt;Complete class theorem III&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Class of Bayes estimators is complete&lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/CFCCIMO.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;interpretation-and-implications&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#interpretation-and-implications&quot;&gt;Interpretation and implications&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What is the connection between &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayes_estimator#Definition&quot;&gt;Bayesian estimators&lt;/a&gt; and Bayesian posteriors?&lt;/p&gt;

&lt;p&gt;Answer: Bayes estimator predicts the mean posterior for L2 loss, median for L1 loss.  [credit: John Chung]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;:&lt;br /&gt;
If $\p(\t)$ is a given prior, then a corresponding Bayes estimator is $\d$ is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\d(x) = \argmin_{\hat{\t}} \E_{\t \sim p_\p(\t \mid x)}\left[L(\t, \hat{\t})\right] = \argmin_{\hat{\t}} \int_{\T} L(\t, \hat{\t}) p_\pi(\t \mid x) \D{\t}\,,&lt;/script&gt;

&lt;p&gt;where the posterior is $p_\pi(\t \mid x) = P_\t(x)\pi(\t)/p_\p(x)$ and marginal data distribution is $p_\p(x) = \int P_\t(x)\pi(\t) \D{\t}$.&lt;br /&gt;
In words, the Bayes estimator minimizes the posterior expected loss for every $x$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt;&lt;br /&gt;
&lt;br /&gt;(This proof is my own)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\min_{\hat{\d}}R(\p, \hat{\d}) &amp;= \min_{\hat{\d}} \int_\mc{X}\int_\T L(\t, \hat{\d}(x)) P_\t(x)\p(\t) \D{\t}\D{x} \\
  &amp;= \min_{\hat{\d}} \int_\mc{X}\left(\int_\T L(\t, \hat{\d}(x)) p_\pi(\t \mid x) \D{\t}\right) p_\p(x) \D{x} \\
  &amp;= \int_\mc{X}\left(\min_{\hat{\d}_x} \int_\T L(\t, \hat{\d}_x) p_\pi(\t \mid x) \D{\t}\right) p_\p(x) \D{x} \\
  &amp;= \E_{x \sim p_\p(x)}\left[\min_{\hat{\d}_x} \int_\T L(\t, \hat{\d}_x) p_\pi(\t \mid x) \D{\t}\right] \\
  &amp;= \E_{x \sim p_\p(x)}\left[\min_{\hat{\d}_x} \E_{\t \sim p_\p(\t \mid x)}\left[L(\t, \hat{\d}_x)\right] \right]\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;So the min Bayes risk is expected (w.r.t. data) minimum “posterior expected loss”.&lt;/p&gt;

&lt;p&gt;Thus if we define $\d(x) := \d^*_x,\ \forall x \in \mc{X}$, where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\d^*_x = \argmin_{\hat{\d}_x} \E_{\t \sim p_\p(\t \mid x)}\left[L(\t, \hat{\d}_x)\right]\,,&lt;/script&gt;

&lt;p&gt;then $\d = \argmin_\hat{\d} R(\p, \hat{\d})\,.$&lt;br /&gt;
&lt;em&gt;QED&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The general form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b^* = \argmin_b \E_A \left[L(A, b)\right]&lt;/script&gt;

&lt;p&gt;is called the &lt;em&gt;systematic part&lt;/em&gt; of random variable $A$. When $L$ is squared difference (i.e. $\ell^2$), then $b^*$ is the mean of $A$. When $L$ is absolute difference  (i.e. $\ell^1$), then $b^*$ is the median of $A$. When $L$ is the indicator loss (i.e. $\ell^0$), then $b^*$ is the mode of $A$. There are also losses corresponding to other distribution statistics like quantile loss. See the definition of &lt;em&gt;systematic part&lt;/em&gt; in my post on the &lt;a href=&quot;http://zhat.io/articles/bias-variance#bias-variance-decomposition-for-any-loss&quot;&gt;generalized bias-variance decomposition&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;$\d$ will be the mean, median, or mode of the posterior for $\ell^2$, $\ell^1$, $\ell^0$ losses respectively. To avoid confusion, here it is stated explicitly:&lt;/p&gt;

&lt;p&gt;If $L(\t, \hat{\t}) = (\t - \hat{\t})^2$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\d(x) = \mathrm{Mean}_{\t \sim p_\p(\t \mid x)}\left[\t\right] = \E_{\t \sim p_\p(\t \mid x)}\left[\t\right]\,.&lt;/script&gt;

&lt;p&gt;If $L(\t, \hat{\t}) = \lvert\t - \hat{\t}\rvert$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\d(x) = \mathrm{Median}_{\t \sim p_\p(\t \mid x)}\left[\t\right]\,.&lt;/script&gt;

&lt;p&gt;If $L(\t, \hat{\t}) = (\t - \hat{\t})^0$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\d(x) = \mathrm{Mode}_{\t \sim p_\p(\t \mid x)}\left[\t\right]\,.&lt;/script&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
L(\t, \hat{\t}) = \begin{cases}\tau\cdot(\t - \hat{\t}) &amp; \t - \hat{\t} \geq 0 \\ (\tau-1)\cdot(\t - \hat{\t}) &amp; \mathrm{otherwise}\end{cases}, %]]&gt;&lt;/script&gt;     then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\d(x) = \mathrm{Quantile}\{\tau\}_{\t \sim p_\p(\t \mid x)}\left[\t\right]\,,&lt;/script&gt;

&lt;p&gt;and $\tau=\frac{1}{2}$ gives the median.&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#discussion&quot;&gt;Discussion&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Do the complete class theorems prove the necessity of Bayesian epistemology (assuming you wish to be rational)?&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Complete class theorems assume the data has a well defined probability distribution. If we use CCTs to justify Bayesian epistemology (i.e. usage of probability for outcomes which do not repeat, have a frequency or occurrence, or any well defined objective notion of probability) then this argument is circular. It depends on frequentist probability being a thing, and Bayesian probability is enticing over frequentist probability because frequentist probability only makes sense in limited circumstances where events have well defined frequencies of occurrence.&lt;/li&gt;
  &lt;li&gt;Enforcing admissibility may be inconsequential. This framework is silent on how to define the hypothesis space and choose a prior, which matters quite a lot for 1 shot prediction, but doesn’t matter at infinite data limit. In practice we don’t care about the infinite data limit. In practice, picking the wrong hypothesis space or a bad prior may impact your utility much more than being admissible.&lt;/li&gt;
  &lt;li&gt;The result above shows that only the &lt;em&gt;systematic part&lt;/em&gt; (e.g. mean) of the posterior matters for minimizing Bayes risk.&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 11 Jun 2020 00:00:00 -0700</pubDate>
        <link>pragmanym.github.io/zhat/articles/notes-complete-class-theorems</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/notes-complete-class-theorems</guid>
        
        
        <category>notes</category>
        
      </item>
    
      <item>
        <title>Primer to Shannon's Information Theory</title>
        <description>&lt;p&gt;Shannon’s theory of information is usually just called &lt;em&gt;information theory&lt;/em&gt;, but is it deserving of that title? Does Shannon’s theory completely capture every possible meaning of the word &lt;em&gt;information&lt;/em&gt;? In the grand quests to creating AI and understanding the rules of the universe (i.e. grand unified theory) information may be key. Intelligent agents search for information and manipulate it. Particle interactions in physics may be viewed as information transfer. The physics of information may be key to interpreting quantum mechanics and resolving the measurement problem.&lt;/p&gt;

&lt;p&gt;If you endeavor to answer these hard questions, it is prudent to understand existing so-called theories of information so you can evaluate whether they are powerful enough and to take inspiration from them.&lt;/p&gt;

&lt;p&gt;Shannon’s information theory is a hard nut to crack. Hopefully this primer gets you far enough along to be able to read a textbook like &lt;em&gt;Elements of Information Theory&lt;/em&gt;. At the end I start to explore the question of whether Shannon’s theory is a complete theory of information, and where it might be lacking.&lt;/p&gt;

&lt;p&gt;This post is long. That is because Shannon’s information theory is a framework of thought. That framework has a vocabulary which is needed to appreciate the whole. I attempt to gradually build up this vocabulary, stopping along the way to build intuition. With this vocabulary in hand, you will be ready to explore the big questions at the end of this post.&lt;/p&gt;

&lt;!--more--&gt;

&lt;ul class=&quot;toc&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#self-information&quot; id=&quot;markdown-toc-self-information&quot;&gt;Self-Information&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#regarding-notation&quot; id=&quot;markdown-toc-regarding-notation&quot;&gt;Regarding notation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bits-not-bits&quot; id=&quot;markdown-toc-bits-not-bits&quot;&gt;&lt;em&gt;Bits&lt;/em&gt;, not bits&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#stepping-back&quot; id=&quot;markdown-toc-stepping-back&quot;&gt;Stepping back&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#entropy&quot; id=&quot;markdown-toc-entropy&quot;&gt;Entropy&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#regarding-notation-1&quot; id=&quot;markdown-toc-regarding-notation-1&quot;&gt;Regarding notation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#conditional-entropy&quot; id=&quot;markdown-toc-conditional-entropy&quot;&gt;Conditional Entropy&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#mutual-information&quot; id=&quot;markdown-toc-mutual-information&quot;&gt;Mutual Information&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#pointwise-mutual-information&quot; id=&quot;markdown-toc-pointwise-mutual-information&quot;&gt;Pointwise Mutual Information&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#properties-of-pmi&quot; id=&quot;markdown-toc-properties-of-pmi&quot;&gt;Properties of PMI&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#special-values&quot; id=&quot;markdown-toc-special-values&quot;&gt;Special Values&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#expected-mutual-information&quot; id=&quot;markdown-toc-expected-mutual-information&quot;&gt;Expected Mutual Information&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#channel-capacity&quot; id=&quot;markdown-toc-channel-capacity&quot;&gt;Channel capacity&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#shannon-information-for-continuous-distributions&quot; id=&quot;markdown-toc-shannon-information-for-continuous-distributions&quot;&gt;Shannon Information For Continuous Distributions&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#proof-that-mi-is-fininte-for-continuous-distributions&quot; id=&quot;markdown-toc-proof-that-mi-is-fininte-for-continuous-distributions&quot;&gt;Proof that MI is fininte for continuous distributions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#problems-with-shannon-information&quot; id=&quot;markdown-toc-problems-with-shannon-information&quot;&gt;Problems With Shannon Information&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1-tv-static-problem&quot; id=&quot;markdown-toc-1-tv-static-problem&quot;&gt;1. TV Static Problem&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-shannon-information-is-blind-to-scrambling&quot; id=&quot;markdown-toc-2-shannon-information-is-blind-to-scrambling&quot;&gt;2. Shannon Information is Blind to Scrambling&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3-deterministic-information&quot; id=&quot;markdown-toc-3-deterministic-information&quot;&gt;3. Deterministic information&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4-if-the-universe-is-continuous-everything-contains-infinite-information&quot; id=&quot;markdown-toc-4-if-the-universe-is-continuous-everything-contains-infinite-information&quot;&gt;4. If the universe is continuous everything contains infinite information&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5-shannon-information-ignores-the-meaning-of-messages&quot; id=&quot;markdown-toc-5-shannon-information-ignores-the-meaning-of-messages&quot;&gt;5. Shannon information ignores the meaning of messages&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#6-probability-distributions-are-not-objective&quot; id=&quot;markdown-toc-6-probability-distributions-are-not-objective&quot;&gt;6. Probability distributions are not objective&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#appendix&quot; id=&quot;markdown-toc-appendix&quot;&gt;Appendix&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#properties-of-conditional-entropy&quot; id=&quot;markdown-toc-properties-of-conditional-entropy&quot;&gt;Properties of Conditional Entropy&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bayes-rule&quot; id=&quot;markdown-toc-bayes-rule&quot;&gt;Bayes’ Rule&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cross-entropy-and-kl-divergence&quot; id=&quot;markdown-toc-cross-entropy-and-kl-divergence&quot;&gt;Cross Entropy and KL-Divergence&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#acknowledgments&quot; id=&quot;markdown-toc-acknowledgments&quot;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;self-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#self-information&quot;&gt;Self-Information&lt;/a&gt;&lt;/h1&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\newcommand{\and}{\wedge}
\newcommand{\or}{\vee}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bm}{\boldsymbol}
\newcommand{\rX}{\bm{X}}
\newcommand{\rY}{\bm{Y}}
\newcommand{\rZ}{\bm{Z}}
\newcommand{\rC}{\bm{C}}
\newcommand{\diff}[1]{\mathop{\mathrm{d}#1}}&lt;/script&gt;

&lt;p&gt;I’m going to use non-standard notation which I believe avoids some confusion and ambiguities.&lt;/p&gt;

&lt;p&gt;Shannon defines information indirectly by defining quantity of information contained in a message/event. This is analogous how physics defines mass and energy in terms of their quantities.&lt;/p&gt;

&lt;p&gt;Let’s define $x$ to be any mathematical object from a set of possibilities $X$. We typically call $x$ a &lt;em&gt;message&lt;/em&gt;, but it can also be referred to as an &lt;em&gt;outcome&lt;/em&gt;, &lt;em&gt;state&lt;/em&gt;, or &lt;em&gt;event&lt;/em&gt; depending on the context.&lt;/p&gt;

&lt;p&gt;Define &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;$h(x)$&lt;/span&gt;&lt;label for=&quot;ad226219b7413f6c19c43a404b5a9d36708aa40e&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ad226219b7413f6c19c43a404b5a9d36708aa40e&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The standard notation is $I(x)$, but this is easy to confuse with mutual information &lt;a href=&quot;#expected-mutual-information&quot;&gt;below&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to be the &lt;strong&gt;self-information&lt;/strong&gt; of $x$, which is the amount of information gained by &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;receiving&lt;/span&gt;&lt;label for=&quot;5aecb422be00894a86508ac09f6366924a33ad33&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;5aecb422be00894a86508ac09f6366924a33ad33&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Receiving here can mean, (1) sampling an outcome from a distribution, (2) storing in memory &lt;em&gt;one&lt;/em&gt; of its possible states, or (3) viewing with the mind or knowing to be the case one out of the possible cases.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; $x$. We will see how a natural definition of $h(x)$ arises from combining these two principles:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Quantity of information is a function only of probability of occurrence.&lt;/li&gt;
  &lt;li&gt;Quantity of information acts like quantity of bits when applied to computer memory.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Principle (1) constrains $h$ to the form $h(x) = f(p_X(x))$, and we do not yet know what $f$ should be.&lt;/p&gt;

&lt;p&gt;To see why, let’s unpack (1): it implies that messages/events must always come from a distribution, which is what provides the probabilities. Say you receive a message $x$ sampled from probability distribution (function) $p_X : X \to [0, 1]$ over a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;discrete&lt;/span&gt;&lt;label for=&quot;36d6168a1655cd352523cce0e3a2ac045fc1621d&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;36d6168a1655cd352523cce0e3a2ac045fc1621d&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Assume all distributions are discrete until the &lt;a href=&quot;#shannon-information-for-continuous-distributions&quot;&gt;continuous section&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; set $X$. Then (1) is saying that $h$ should only &lt;em&gt;look at&lt;/em&gt; the probability $p_X(x)$ and not $x$ itself. This is a reasonable requirement, since we want to define information irrespective of the kind of object that $x$ is.&lt;/p&gt;

&lt;p&gt;Principle (2) constrains what $f$ should be: &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;$f(p) = -\log_2 p$&lt;/span&gt;&lt;label for=&quot;ff9302aebed53145e61c362eae63cb172e24e20a&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ff9302aebed53145e61c362eae63cb172e24e20a&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Though we assume a uniform discrete probability distribution to derive this, we will use this definition of $f$ to generalize the same logic to all probability distributions, which is how we arrive at the final definition of $h$.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, where $p \in [0, 1]$ is a probability value.&lt;/p&gt;

&lt;p&gt;To understand (2), consider computer memory. With $N$ bits of memory there are $2^N$ distinguishable states, and only one is the case at one time. Increasing the number of bits exponentially increases the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;number of counterfactual states&lt;/span&gt;&lt;label for=&quot;b6ce5dab64f8e17412d6f1eecdbc565ff4506d08&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;b6ce5dab64f8e17412d6f1eecdbc565ff4506d08&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Number of states you could have stored but didn’t.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. In memory terms, receiving a “message” of $N$ bits of memory simply means finding out the state those bits are in. Attaching equal weight to each possibility (i.e. memory state) gives us a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;special case of the probability distribution we used above&lt;/span&gt;&lt;label for=&quot;89a786f47bc58cb026e252161192577d84784488&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;89a786f47bc58cb026e252161192577d84784488&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;To see the equivalence between these two notions of information, i.e. more rare equals more informative vs number of counterfactual states (or memory capacity), it is useful to think of the probability distribution as a weighted possibility space, and of the memory states as possibilities.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to define $h$: the &lt;em&gt;uniform&lt;/em&gt; distribution, where there are $2^N$ possible states and the weight of single state is $\frac{1}{2^N} = 2^{-N}$.&lt;/p&gt;

&lt;!--We intuitively think of the quantity of information stored in memory as the number of bits it has. We have $f(p)$ return $N$ when every possible state has an equal weight of $p=2^{-N}$, because we assume a uniform distribution over $2^N$ states, which is equivalent to how we conceive of computer memory with $N$ bits.--&gt;

&lt;p&gt;Composing $f(p) = -\log_2 p$ with $h(x) = f(p_X(x))$ gives us the full definition of self-information:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x) = -\log_2 p_X(x)\,.&lt;/script&gt;

&lt;!--*Now the magic happens.* Given that we defined self-information as $h(x) = f(p_X(x))$, and given that we've pinned down $f(p) = -\log_2 p$ for a special case, we've done all the work we need to do to define $h(x)$ for all probability distributions, because nothing in our definition of $f(p)$ actually depends on the particular distribution we used.--&gt;

&lt;h2 id=&quot;regarding-notation&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#regarding-notation&quot;&gt;Regarding notation&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;From here on, I will use $h(x)$ as a function of message $x$, without specifying the type of $x$. It can be anything: a number, a binary sequence, a string, etc. $f(p)$ is a function of probabilities, rather than messages. So:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;$h : X \to \R^+$ maps from messages to information,&lt;br /&gt; 
and $f : [0, 1] \to \R^+$ maps from probabilities to information;&lt;/p&gt;
&lt;p&gt;and keep in mind that $h(x) = f(p_X(x))$, so &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;$h$ implicitly assumes&lt;/span&gt;&lt;label for=&quot;ac81d19ce662d5360601c37e5741410b398a15da&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ac81d19ce662d5360601c37e5741410b398a15da&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;I may sometimes write $h_X$ to make explicit the dependency of $h$ on $p_X$.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; we have a probability distribution over $x$ defined somewhere.&lt;/p&gt;

&lt;p&gt;In some places below &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;I’ve written equations in terms of $f$ rather than $h$&lt;/span&gt;&lt;label for=&quot;407f53214d4b89bb8df9485bfcc2f05a23b9bf92&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;407f53214d4b89bb8df9485bfcc2f05a23b9bf92&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Allow me the slight verbosity now, as you’d probably have had to pore over verbose definitions if I hadn’t.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; where I felt it would allow you to grasp things just by looking at the shape of the equation.&lt;/p&gt;

&lt;h2 id=&quot;bits-not-bits&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#bits-not-bits&quot;&gt;&lt;em&gt;Bits&lt;/em&gt;, not bits&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;You can get through the above exposition by thinking in terms of computer bits. Now we part ways from the computer bits intuition. Note that this departure occurs when $p_X(x)$ is not a (negative) integer power of two. $h(x)$ will be non-integer, and very likely irrational. What does it mean to have a fraction of a bit? From here on out, it’s better to think of $h$ as a quantity of information, rather than a count of physical objects. We will continue to call the unit of $h(x)$ a &lt;em&gt;bit&lt;/em&gt;, but this is a mere convention. The word &lt;em&gt;bit&lt;/em&gt; should no long be thought of a physical medium that stores two distinguishable states (usually labeled “0” and “1”), but is instead a unit. Like the kilogram and Joule, this unit is undefined, but its usage gives it semantic meaning.&lt;/p&gt;

&lt;p&gt;So then how is $h$ to be understood? What is the intuition behind this quantity? In short, Shannon bits are an &lt;a href=&quot;https://en.wikipedia.org/wiki/Analytic_continuation&quot;&gt;analytic continuation&lt;/a&gt; of computer bits. Just like how the &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_function&quot;&gt;gamma function&lt;/a&gt; extends factorial to continuous values, Shannon bits extend the computer bit to &lt;strong&gt;non-uniform distributions&lt;/strong&gt; over a &lt;strong&gt;non-power-of-2&lt;/strong&gt; number of counterfactuals. Let me explain these two phrases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;non-power-of-2&lt;/strong&gt;: We have memory that can store one out of $M$ possibilities, where $M \neq 2^N$. For example, I draw a card from a deck of 52. That card holds $-\log_2 \frac{1}{52} = \log_2 52 \approx 5.70044\ldots$ bits of information. A fractional bit can represent a non-power-of-2 possibility space, and quantifies the log-base conversion factor into base $M$. In this case $-log_{52} x = -\frac{\log_2 x}{\log_2 52}$. Note that it is actually common to use units of information other than base-2. For example a &lt;a href=&quot;https://en.wikipedia.org/wiki/Nat_(unit)&quot;&gt;&lt;em&gt;nat&lt;/em&gt;&lt;/a&gt; is log-base-e, a &lt;a href=&quot;https://en.wikipedia.org/wiki/Ternary_numeral_system&quot;&gt;&lt;em&gt;trit&lt;/em&gt;&lt;/a&gt; is base-3, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Hartley_(unit)&quot;&gt;&lt;em&gt;dit&lt;/em&gt; or &lt;em&gt;ban&lt;/em&gt;&lt;/a&gt; is base-10.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;non-uniform distributions&lt;/strong&gt;: Using the deck of cards example, let’s say we draw from a sub-deck containing all cards with the hearts suit. We’ve reduced the possibility space to a subset of a super-space, in this case size 13, and have reduced the information contained in a given card, $-\log_2 \frac{1}{13} \approx 3.70044\ldots$ bits. You can think of this as assigning a weight to each card: 0 for cards we exclude, and $\frac{1}{13}$ for cards we include. If we make the non-zero weights non-uniform, we now have an interpretational issue: what is the physical meaning of these weights? Thinking of this weight as a probability of occurrence is one way to recover physical meaning, but this is &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;not a requirement&lt;/span&gt;&lt;label for=&quot;02ee609808a0f8d62fa190ce247f9b35f70dd990&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;02ee609808a0f8d62fa190ce247f9b35f70dd990&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;And probability may not even be an objective property of physical systems in general.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. However, I will &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;call these weights probabilities&lt;/span&gt;&lt;label for=&quot;03b6daf7c965c2773f771111cb006d8764629617&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;03b6daf7c965c2773f771111cb006d8764629617&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The reason we wish to hold sum of weights fixed to 1 is so that we can consider the information contained in compound events which are sets of elementary events. In other words, think of the card drawn from the sub-deck of 13 as a card from &lt;em&gt;any suit&lt;/em&gt;, i.e. the set of 4 cards with the same number. The card represents an equivalence class over card number.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, and the weighted-possibility-spaces distributions, as that is the convention. But keep in mind that these weights do not necessarily represent frequencies of occurrence nor uncertainties. The meaning of probability itself is a subject of debate.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The reason we wish to hold sum of weights fixed to 1 is so that we can consider the information contained in compound events which are sets of elementary events. In other words, think of the card drawn from the sub-deck of 13 as a card from &lt;em&gt;any suit&lt;/em&gt;, i.e. the set of 4 cards with the same number. The card represents an equivalence class over card number.&lt;/p&gt;

&lt;p&gt;Let’s examine some of the properties of $h$ to build further intuition.&lt;/p&gt;

&lt;p&gt;First notice that $f(1) = 0$. An event with a probability of 1 contains no information. If $x$ is certain to occur, $x$ is uninformative. Likewise, $f(p) \to \infty$ as $p \to 0$. If $x$ is impossible, it contains infinite information! In general, $h(x)$ goes up as $p_X(x)$ goes down. The less likely an event, the more information it contains. Hopefully this sounds to you like a reasonable property of information.&lt;/p&gt;

&lt;p&gt;Next, we can be more specific about how $h$ goes up as $p_X$ goes down. Recall that $f(p) = -\log_2 p$ and $h(x) = f(p_X(x))$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(p/2) = f(p) + 1\,.&lt;/script&gt;

&lt;p&gt;If we halve the probability of an event, we add one bit of information to it. That is a nice way to think about our new unit of information. The &lt;em&gt;bit&lt;/em&gt; is a halving of probability. Other units can be defined in this way, e.g. the &lt;em&gt;nat&lt;/em&gt; is dividing of probability by Euler’s constant e, the &lt;em&gt;trit&lt;/em&gt; is a thirding of probability, etc.&lt;/p&gt;

&lt;p&gt;Finally, notice that $f(pq) = f(p) + f(q)$. Or to write it another way: $h(x \and y) = h(x) + h(y)$ iff $x$ and $y$ are independent events, because&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
h(x \and y) &amp;= -\log_2 p_{X,Y}(x \and y) \\
  &amp;= -\log_2 p_X(x)\cdot p_Y(y) \\
  &amp;= -\log_2 p_X(x) - \log_2 p_Y(y)\,,
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $x \and y$ indicates the composite event &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;“$x$ and $y$”&lt;/span&gt;&lt;label for=&quot;03c34b59dc7fea1cd51e8dbb51bdcfc9754145fd&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;03c34b59dc7fea1cd51e8dbb51bdcfc9754145fd&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;We could either think of $x$ and $y$ as composite events themselves from the same distribution, i.e. $x$ and $y$ are sets of &lt;a href=&quot;https://en.wikipedia.org/wiki/Elementary_event&quot;&gt;elementary events&lt;/a&gt;, or as elementary events from two different random variables which have a joint distribution, i.e, $(x, y) \sim (\rX, \rY)$. I will consider the latter case from here on out, because it is conceptually simpler.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. Hopefully this is also intuitive. If two events are dependent, i.e. they causally affect each other, it makes sense that they might contain redundant information, meaning that you can predict part of one from the other, and so their combined information is less than the sum of their individual information. You may be surprised to learn that the opposite can also be true. The combined information of two events can be greater than the sum of their individual information! This is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Interaction_information#Example_of_negative_interaction_information&quot;&gt;&lt;em&gt;synergy&lt;/em&gt;&lt;/a&gt;. More on that in the &lt;a href=&quot;#pointwise-mutual-information&quot;&gt;pointwise mutual information&lt;/a&gt; section.&lt;/p&gt;

&lt;p&gt;In short, we can derive $f(p) = -\log_2 p$ from (1) additivity of information, $f(pq) = f(p) + f(q)$, and (2) a choice of unit, $f(½) = 1$. &lt;a href=&quot;https://en.wikipedia.org/wiki/Entropy_(information_theory)#Rationale&quot;&gt;Proof&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;recap&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#recap&quot;&gt;Recap&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;To make the full analogy: a weighting over possibilities is like a continuous relaxation of a set. An element is or is not in a set, while adding weights to elements (in a larger set) allows their member ship to have degrees, i.e. the &lt;em&gt;“is element”&lt;/em&gt; relation becomes a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;fuzzy value between 0 and 1&lt;/span&gt;&lt;label for=&quot;9c3ecf3b9785719c56a55d9ff267adf0112b2c75&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;9c3ecf3b9785719c56a55d9ff267adf0112b2c75&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;We recover regular sets by setting all weights to either 0 or uniform non-zero weights.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. With a weighted possibility space we have a lot more freedom to work with extra information beyond just merely which possibilities are in the set. Probability distributions are more expressive than mere sets.&lt;/p&gt;

&lt;h2 id=&quot;stepping-back&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#stepping-back&quot;&gt;Stepping back&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The unit &lt;em&gt;bit&lt;/em&gt; that we’ve defined is connected to computer bits only because they both convert multiplication to addition.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Computer bits: $(2^N\cdot2^M)$ states $\Longrightarrow$ $(N+M)$ bits.&lt;/li&gt;
  &lt;li&gt;Shannon bits: $(p\cdot q)$ probability $\Longrightarrow$ $(-\log_2 p - \log_2 q)$ bits.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The way I’ve motivated $h$ is a departure from Shannon’s original motivation for defining self-information, which was to describe the theoretically optimal lossless compression for messages being sent over a communication channel. Under this viewpoint, $h(x)$ quantifies the theoretically minimum possible length (in physical bits) to encode message $x$ in computer memory without loss of information. Under this view, $h(x)$ should be thought of as the asymptotic average bit-length for the optimal encoding of $x$ in an infinite sequence of messages drawn from $p_X$. Hence why it makes sense for $h(x)$ to be a continuous value. For more details, see &lt;a href=&quot;https://en.wikipedia.org/wiki/Arithmetic_coding#Connections_with_other_compression_methods&quot;&gt;arithmetic coding&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We are now flipping Shannon’s original motivation on its head, and using the theoretically optimal encoding length in bits as the definition of information content. In the following discussion, we don’t care how messages/events are actually represented physically. Our definition of information only cares about probability of occurrence, and is in fact &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;blind to the contents of messages&lt;/span&gt;&lt;label for=&quot;e4d483f3b81edaec6cd9e4f482319dd7556b1855&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;e4d483f3b81edaec6cd9e4f482319dd7556b1855&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Something that could be seen as either a flaw or a virtue, which I discuss &lt;a href=&quot;#5-shannon-information-ignores-the-meaning-of-messages&quot;&gt;below&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. The connection of probability to optimal physical encoding is one of the beautiful results that propelled Shannon’s framework into its lofty position as &lt;em&gt;information theory&lt;/em&gt;. However, for our purposes, we simply care about defining quantity of information, and do not care at all about how best to compress or store data for practical purposes.&lt;/p&gt;

&lt;p&gt;To be clear, when I talk about the self-information of a message, I am not saying anything about how the message is physically encoded or transmitted, and indeed it need not be encoded with an optimal number of computer bits. I am merely referring to a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;quantified&lt;/span&gt;&lt;label for=&quot;a3caf3737e12ebd497b9d5b99c399b7cd6e84ec9&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;a3caf3737e12ebd497b9d5b99c399b7cd6e84ec9&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Hopefully this quantity is objective and measurable in principle - something I discuss &lt;a href=&quot;#6-probability-distributions-are-not-objective&quot;&gt;below&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; property of the message, i.e. it’s information content. The number of computer bits a message is encoded with need not equal the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;number of Shannon bits it contains!&lt;/span&gt;&lt;label for=&quot;a6072afeef1e2f90ee72e61877bd97c2b4450eeb&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;a6072afeef1e2f90ee72e61877bd97c2b4450eeb&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;In short, physical encoding length and probability of occurrence need not be linked.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;entropy&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#entropy&quot;&gt;Entropy&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;In the last section I said that under the view of optimal lossless compression, $h(x)$ is the bit length of the optimal encoding for $x$ averaged over an infinite sample from random variable $\rX$, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Arithmetic_coding#Connections_with_other_compression_methods&quot;&gt;arithmetic coding&lt;/a&gt; can approach this limit. We could also consider the average bit length per message from $\rX$ (averaged across all messages). That is the &lt;strong&gt;entropy&lt;/strong&gt; of random variable $\rX$, which is the expected self-information,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
H[\rX] &amp;= \E_{x\sim \rX}[h(x)] \\
       &amp;= \E_{x\sim \rX}[-\log_2\,p_X(x)]\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the quantifying information view, think of entropy $H[\rX]$ as the number of bits you expect to gain by observing an event sampled from $p_X(x)$. In that sense it is a measure of uncertainty, i.e. how much information I do not have, i.e. quantifying what is unknown.&lt;/p&gt;

&lt;p&gt;Let’s build our intuition of entropy. A good way to view entropy is as a measure of how spread out a distribution is. Entropy is actually a type of &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_dispersion&quot;&gt;statistical dispersion&lt;/a&gt; of $p_X$, meaning you could use it as an &lt;a href=&quot;http://zhat.io/articles/19/bias-variance#what-is-variance-anyway&quot;&gt;alternative to statistical variance&lt;/a&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/posts/primer-shannon-information/bimodal.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;For example, a bi-modal distribution can have arbitrarily high variance by moving the modes far apart, but the overall spread-out-ness (entropy) will not necessarily change.&lt;/p&gt;

&lt;p&gt;The more spread out a distribution is, the higher its entropy. For bounded &lt;a href=&quot;https://en.wikipedia.org/wiki/Support_(mathematics)#Support_of_a_distribution&quot;&gt;support&lt;/a&gt;, the uniform distribution has highest entropy (&lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution#Other_examples.&quot;&gt;other max-entropy distributions&lt;/a&gt;). The &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;minimum possible entropy is 0&lt;/span&gt;&lt;label for=&quot;61d3d043a10d74a21176f1255203a29c26b0f6f4&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;61d3d043a10d74a21176f1255203a29c26b0f6f4&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Note that in the expectation, 0-probability outcomes have infinite self-information, so we have to use the convention that $p_X(x)\cdot h(x) = 0\cdot\infty = 0$.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, which indicates a deterministic distribution, i.e. $p_X(x) \in \{0, 1\}$ for all $x \in X$.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/2/22/Binary_entropy_plot.svg&quot; alt=&quot;Credit: &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/Binary_entropy_function&amp;quot;&amp;gt;https://en.wikipedia.org/wiki/Binary_entropy_function&amp;lt;/a&amp;gt;&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;Credit: &lt;a href=&quot;https://en.wikipedia.org/wiki/Binary_entropy_function&quot;&gt;https://en.wikipedia.org/wiki/Binary_entropy_function&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Though Shannon calls his new idea entropy, the connection to physical entropy is nontrivial. If there is a connection, that is more of a coincidence. Apparently Shannon’s decision to call it entropy was made by a suggestion by von Neumann at a party:  http://www.eoht.info/page/Neumann-Shannon+anecdote&lt;br /&gt;
[credit: Mark Moon]&lt;/p&gt;

&lt;p&gt;There are connections between information entropy and thermodynamics entropy (see https://plato.stanford.edu/entries/information-entropy/), but I do not yet understand them well enough to give an overview here - perhaps in a future post. Some physicists consider information to have a physical nature, and even a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;conservation law&lt;/span&gt;&lt;label for=&quot;4cd9a9099f83eeb0f5a534d111b0875861d2c3ec&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;4cd9a9099f83eeb0f5a534d111b0875861d2c3ec&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;In the sense that requiring physics be time-symmetric is equivalent to requiring information to be conserved.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;! Further reading: &lt;a href=&quot;https://theoreticalminimum.com/courses/statistical-mechanics/2013/spring/lecture-1&quot;&gt;The Theoretical Minimum - Entropy and conservation of information&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/No-hiding_theorem&quot;&gt;no-hiding theorem&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: Why expected self-information?&lt;br /&gt;
We could have used median or something else. Expectation is a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;default go-to operation over distributions&lt;/span&gt;&lt;label for=&quot;e3be61e72d21ae86483befb994084998b611b8df&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;e3be61e72d21ae86483befb994084998b611b8df&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;See my previous post: &lt;a href=&quot;http://zhat.io/articles/bias-variance#bias-variance-decomposition-for-any-loss&quot;&gt;http://zhat.io/articles/bias-variance#bias-variance-decomposition-for-any-loss&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; because of its nice properties, but ultimately it is an arbitrary choice. However, as we will see, one huge benefit in our case is that expectation is linear.&lt;/p&gt;

&lt;h3 id=&quot;regarding-notation-1&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#regarding-notation-1&quot;&gt;Regarding notation&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;From here on out, I will drop the subscript $X$ from $p_X(x)$ when $p(x)$ unambiguously refers to the probability of $x$. This is a common thing to do, but it can also lead to ambiguity if I want to write $p(0)$, the probability that $x$ is 0. A possible resolution is to use random variable notation, $p(\rX = 0)$, which I use in some places. However, there is the same issue for self-information. For example, quantities $h(x), h(y), h(x\and y), h(y \mid x)$. I will add subscripts to $h$ when it would be ambiguous otherwise, for example $h_X(0), h_Y(0), h_{X,Y}(x\and y), h_{Y\mid X}(0 \mid 0)$ .&lt;/p&gt;

&lt;h2 id=&quot;conditional-entropy&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#conditional-entropy&quot;&gt;Conditional Entropy&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Conditional self-information, defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
h(y \mid x) &amp;= -\log_2\,p(y \mid x)\\
  &amp;= -\log_2(p(y \and x) / p(x)) \\
  &amp;= h(x \and y) - h(x)\,,
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;is the information you stand to gain by observing $y$ given that you already observed $x$.  I let $x \and y$ denote the observation of $x$ and $y$ together (I could write $(x, y)$, but then $p((y, x))$ would look awkward).&lt;/p&gt;

&lt;p&gt;If $x$ and $y$ are independent events, $h(y \mid x) = h(y)$. Otherwise, $h(y \mid x)$ can be greater or less than $h(y)$. It may seem counterintuitive that $h(y \mid x) &amp;gt; h(y)$ can happen, because this implies you gain more from $y$ by just simply knowing something else, $x$. However, this reflects the fact that you are unlikely to see $x, y$ together. Likewise, if $h(y \mid x) &amp;lt; h(y)$ you are likely to see $x, y$ together. More on this in the next section.&lt;/p&gt;

&lt;p&gt;Confusingly, conditional entropy can refer to two different things.&lt;/p&gt;

&lt;p&gt;First is expected conditional self-information,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
H[\rY \mid \rX = x] &amp;= \E_{y\sim \rY \mid \rX=x}[h(y \mid x)] \\
  &amp;= \E_{y\sim \rY \mid \rX=x}[\log_2\left(\frac{p(x)}{p(x, y)}\right)] \\
  &amp;= \sum\limits_{y \in Y} p(y \mid x) \log_2\left(\frac{p(x)}{p(x, y)}\right)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The other is what is most often referred to as &lt;strong&gt;conditional entropy&lt;/strong&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
H[\rY \mid \rX] &amp;= \E_{x,y \sim \rX,\rY}[h(y \mid x)] \\
  &amp;= \E_{x,y \sim \rX,\rY}[\log_2\left(\frac{p(x)}{p(x, y)}\right)] \\
  &amp;= \E_{x\sim \rX} H[\rY \mid \rX = x]\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The intuition behind $H[\rY \mid \rX = x]$ will be the same as of entropy, $H[\rY]$, which we covered in the last section. Let’s gain some intuition for $H[\rY \mid \rX]$. If $H[\rY]$ measures uncertainty of $\rY$, then $H[\rY \mid \rX = x]$ measures conditional uncertainty given $x$, and $H[\rY \mid \rX]$ measures average conditional uncertainty w.r.t. $\rX$.&lt;/p&gt;

&lt;p&gt;The maximum value of $H[\rY \mid \rX]$ is $H[\rY]$, which is achieved when $\rX$ and $\rY$ are independent random variables. This should make sense, as recieving a message from $\rX$ does not tell you anything about $\rY$, so your state of uncertainty does not decrease.&lt;/p&gt;

&lt;p&gt;The minimum value of $H[\rY \mid \rX]$ is 0, which is achieved when $p_{\rY \mid \rX}(\rY \mid \rX = x)$ is deterministic for all $x$. In other words, you can define a function $g : X \rightarrow Y$ to map from $X$ to $Y$. This wouldn’t otherwise be the case when $\rY \mid \rX$ is stochastic.&lt;/p&gt;

&lt;p&gt;$H[\rY \mid \rX]$ is useful because it takes all $x \in X$ into consideration. You might have, for example, $H[\rY \mid \rX = x_1] = 0$ for $x_1$, but $H[\rY \mid \rX] &amp;gt; 0$, which means $y$ cannot always be deterministically decided from $x$. In the section on mutual information we will see how to think of $H[\rY \mid \rX]$ as a property of a stochastic function from $X$ to $Y$.&lt;/p&gt;

&lt;p&gt;Because of linearity of expectation, all identities that hold for self-information hold for their entropy counterparts. For example,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
h(y \mid x) &amp;= h(x \and y) - h(x) \\
\Longrightarrow H[\rY \mid \rX] &amp;= H[(\rX, \rY)] - H[\rX]\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;This is a nice result. This equation says that the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;average uncertainty about $\rY$ given $\rX$&lt;/span&gt;&lt;label for=&quot;40b324ca64f32166d198598020cd16fd3a369058&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;40b324ca64f32166d198598020cd16fd3a369058&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Amount of information left to observe in $\rY$ on average.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; equals the total expected information in their joint distribution, $(\rX, \rY)$, minus the average information in $\rX$. In other words, conditional entropy is the total information in $x \and y$ minus information in what you have, $x$, all averaged over all the possible $(x, y)$ you can have.&lt;/p&gt;

&lt;h1 id=&quot;mutual-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#mutual-information&quot;&gt;Mutual Information&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;In my view, mutual information is what holds promise as a definition of information. This it the most important topic to understand for tackling the &lt;a href=&quot;#problems-with-shannon-information&quot;&gt;problems with Shannon information&lt;/a&gt; section below.&lt;/p&gt;

&lt;h2 id=&quot;pointwise-mutual-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#pointwise-mutual-information&quot;&gt;Pointwise Mutual Information&lt;/a&gt;&lt;/h2&gt;

&lt;!-- Intuitively, if two events are causally connected, i.e. dependent, they contain redundant information combined. meaning that their combined information would be less than the sum of their information. It may  also be the case that their combined information could be greater than the sum of their information! This is called *synergy*. We will see examples of this later. --&gt;

&lt;p&gt;When two events $x$ and $y$ are dependent, how do we compute their total information? Previously we said that $h(x \and y) = h(x) + h(y)$ iff $p_X(x \and y) = p_X(x)p_X(y)$. However, the general case is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x \and y) = h(x) + h(y) - i(x, y)\,,&lt;/script&gt;

&lt;p&gt;where I am defining $i(x, y)$ such that this equation holds. Rearranging we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
i(x, y) &amp;= h(x) + h(y) - h(x \and y) \\
        &amp;= -\log_2(p_X(x)) - \log_2(p_X(y)) + \log_2(p_X(x \and y)) \\
        &amp;= \log_2\left(\frac{p_X(x, y)}{p_X(x)p_X(y)}\right)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$i(x, y)$ is called &lt;em&gt;pointwise mutual information&lt;/em&gt; (PMI). Informally, PMI measures the amount of bits shared by two events. To say that another way, it measures how much information I have about one event given I only observe the other. Notice that PMI is symmetric, $i(x, y) = i(y, x)$, so any two events contain the same information about each other.&lt;/p&gt;

&lt;p&gt;$i(x, y)$ is a difference in information. Positive $i(x, y)$ indicates &lt;em&gt;redundancy&lt;/em&gt;, i.e. total information is &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;less than the sum of the parts&lt;/span&gt;&lt;label for=&quot;03c34736803c8e2efd1383baf17104684eeab01a&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;03c34736803c8e2efd1383baf17104684eeab01a&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;If you object that it doesn’t make sense to lose information by observing $x$ and $y$ together over observing them separately, it is important to note that $h(x) + h(y)$ is not a physically meaningful quantity, unless they are independent. Technically, you would have $h(x) + h(y \mid x)$ in total. $h(x)$ and $h(y)$ are both the amounts of information to gain by observing either $x$ or $y$ &lt;strong&gt;first&lt;/strong&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;: $h(x, y) &amp;lt; h(x) + h(y)$. However, it may also be the case that $i(x, y)$ is negative so that $h(x, y) &amp;gt; h(x) + h(y)$. &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;This is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Synergy#Information_theory&quot;&gt;&lt;em&gt;synergy&lt;/em&gt;&lt;/a&gt;.&lt;/span&gt;&lt;label for=&quot;4ce5fcaeebe9d13d3fd8d49c2de4ba62a623e205&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;4ce5fcaeebe9d13d3fd8d49c2de4ba62a623e205&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The word &lt;em&gt;synergy&lt;/em&gt; is conventionally used in the context of expected mutual information, and I am running the risk of conflating two distinct phenomenon under the same word. There is no synergy among two random variables under expected mutual information, and this type of synergy only appears among 3 or more random variables.  See &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_mutual_information#Synergy_and_redundancy&quot;&gt;https://en.wikipedia.org/wiki/Multivariate_mutual_information#Synergy_and_redundancy&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This is highly speculative, but synergy (either the pointwise-MI or expected-MI kind) may be a fundamental insight that could explain &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;emergence&lt;/span&gt;&lt;label for=&quot;c55207622e4799bd3610c39427659c03f63135f5&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;c55207622e4799bd3610c39427659c03f63135f5&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Emergence is a concept in philosophy. See &lt;a href=&quot;https://en.wikipedia.org/wiki/Emergence&quot;&gt;https://en.wikipedia.org/wiki/Emergence&lt;/a&gt; and &lt;a href=&quot;https://plato.stanford.edu/entries/properties-emergent/&quot;&gt;https://plato.stanford.edu/entries/properties-emergent/&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and possible limitations of reductionism in illuminating reality. See &lt;a href=&quot;https://www.scottaaronson.com/blog/?p=3294&quot;&gt;Higher-level causation exists (but I wish it didn’t)&lt;/a&gt;.&lt;/p&gt;

&lt;!--
Let's look at (an admittedly contrived) example of synergy. Suppose our [sample space](https://en.wikipedia.org/wiki/Sample_space) is $\\{a, b, c\\}$ (composed of [coutcomes](https://en.wikipedia.org/wiki/Sample_space#Conditions_of_a_sample_space) or [elementary events](https://en.wikipedia.org/wiki/Elementary_event)), and we have two events $x = \\{a, b\\}$ and $y = \\{b, c\\}$. $x, y$ co-occur if we draw outcome $b$. If $p(a) = 7/16, p(b) = 1/8, p(c) = 7/16$, then $p(x) = 9/16$, $p(y) = 9/16$, $p(x \and y) = 1/8$. $h(x) = h(y) \approx 0.83$ and $h(x \and y) = 3$, so $i(x, y) = 2\cdot0.83 - 3 \approx -1.34$ bits.


That may seem like a contrived example, because I was working with composite events instead of elementary events. The same phenomenon can happen for joint distributions of sample spaces.  &lt;font color=&quot;red&quot;&gt;TODO: explain the difference between the example above and a joint distribution.&lt;/font&gt;
--&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;br /&gt;
Let $X = \{0, 1\}$ and $Y = \{0, 1\}$, then the joint sample space is the cartesian product $X \times Y$. $p_X(x), p_Y(y)$ denote marginal probabilities, and $p_{X,Y}(x, y)$ is their joint probability. The joint probability table:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$x$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$y$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$p_{X,Y}(x, y)$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7/16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7/16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1/16&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We have&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$h_X(0) = -\log_2 p_X(0) = -\log_2 1/2 = 1$&lt;/li&gt;
  &lt;li&gt;$h_Y(0) = -\log_2 p_Y(0) = -\log_2 1/2 = 1$&lt;/li&gt;
  &lt;li&gt;$h_{X,Y}(0 \and 0) = -\log_2 p_{X,Y}(0, 0) = -\log_2 1/16 = 4$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$i(0,0) = h_X(0) + h_Y(0) - h_{X,Y}(0 \and 0) = -2$, and so $(0,0)$ is synergistic. On the other hand, $i(0,1) \approx 0.80735$, indicating $(0,1)$ is redundant.&lt;/p&gt;

&lt;h2 id=&quot;properties-of-pmi&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#properties-of-pmi&quot;&gt;Properties of PMI&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Let’s explore some of the properties of PMI. From here on out, I will consider sampling elementary events from a joint distribution, $(x, y) \sim \bm{X}\times\bm{Y}$, where $\bm{X}, \bm{Y}$ are unspecified discrete (possibly infinite) random variables. For notational simplicity I’ll drop the subscripts from distributions, so $p(x), p(y), p(x, y)$ denote the marginals of $\bm{X}, \bm{Y}$ and the joint $\bm{X}\times\bm{Y}$.&lt;/p&gt;

&lt;p&gt;To recap, PMI measures the difference in bits between the product of marginals $p(x)p(y)$ and the joint $p(x, y)$, as evidenced by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
i(x, y) &amp;= \log_2\left(\frac{p(x, y)}{p(x)p(y)}\right) \\
        &amp;= h(x) + h(y) - h(x \and y)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Negative PMI implies synergy, while positive PMI implies redundancy.&lt;/p&gt;

&lt;p&gt;Another way to think about PMI is as a measure of how much $p(y \mid x)$ differs from $p(y)$ (and vice versa). Suppose an oracle sampled $(x, y) \sim \rX\times\rY$, but the outcome $(x, y)$ remains hidden from you. $p(y)$ is the information you stand to gain by having $y$ revealed to you. However, $p(y \mid x)$ is what you stand to gain from seeing $y$ if $x$ is already revealed. You do not know how much information $x$ contains about $y$ without seeing $y$. Only the oracle knows this. However, if you know $p(y \mid x)$, then you can compute your expected information gain (conditional uncertainty), $H[\rY \mid \rX=x]$.&lt;/p&gt;

&lt;p&gt;PMI measures the change in information you will gain about $y$ (from the oracle’s perspective) before and after $x$ is revealed (and vice versa). In this view, it makes sense to rewrite PMI as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
i(x, y) &amp;= \log_2\left(\frac{p(y \mid x)}{p(y)}\right) \\
        &amp;= -\log_2\,p(y) + \log_2\,p(y \mid x) \\
        &amp;= h(y) - h(y \mid x)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;special-values&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#special-values&quot;&gt;Special Values&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;By definition, $i(x, y) = 0$ iff $\bm{X}, \bm{Y}$ are independent. Verifying, we see that,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
i(x, y) &amp;= \log_2\left(\frac{p(x)p(y)}{p(x)p(y)}\right) \\
        &amp;= 0\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The maximum possible PMI happens when $x$ and $y$ are perfectly associated, i,e. $p(y \mid x) = 1$ or $p(x \mid y) = 1$. So $h(y \mid x) = 0$ or vice versa, meaning you know everything about $y$ if you have $x$. Then $i(x, y) = h(y) - h(y \mid x) = h(y)$. In general, the maximum possible PMI is $\min\{h(x), h(y)\}$.&lt;/p&gt;

&lt;p&gt;PMI has no minimum, and goes to $-\infty$ if $x$ and $y$ can never occur together but can occur separately, i.e. $p(x, y) = 0$ while $p(x), p(y) &amp;gt; 0$. We can see that $p(y \mid x) = p(x, y)/p(x) = 0$ so long as $p(x) &amp;gt; 0$. So $h(y \mid x) \to \infty$, and we have $i(x, y) = h(y) - h(y \mid x) \to -\infty$ if $h(y) &amp;gt; 0$.&lt;/p&gt;

&lt;p&gt;While redundancy is bounded, synergy is infinite. This should make sense, as $h(x), h(y)$ are bounded so there is a maximum amount of information to redundantly share. On the other hand, synergy measures how rare the co-occurrence of $(x,y)$ together are, relative to their marginal probabilities, where lower $p(x, y)$ means their co-occurrence is more special. So if $(x,y)$ can never occur, then their co-occurrence is infinitely special.&lt;/p&gt;

&lt;h2 id=&quot;expected-mutual-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#expected-mutual-information&quot;&gt;Expected Mutual Information&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Expected mutual information, also just called mutual information (MI), is given as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
I[\rX, \rY] &amp;= \E_{x\sim X, y\sim Y}[i(x, y)] \\
        &amp;= \E_{x\sim X, y\sim Y}\left[\log_2\left(\frac{p(x, y)}{p(x)p(y)}\right)\right]\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$I$ is to correlation as $H$ is to variance. While correlation measures to what extent $\rX$ and $\rY$ have a &lt;a href=&quot;https://en.wikipedia.org/wiki/Correlation_and_dependence&quot;&gt;linear relationship&lt;/a&gt;, $I$ measures the strength of their statistical dependency. While variance measures average distance from some critical point, $H$ is distance agnostic, i.e. it measures unordered dispersion. Similarly, while statistical correlation measures deviation of the mapping between $\rX$ and $\rY$ from perfectly linear, $I$ is shape agnostic, i.e. it measures unordered causal dependence.&lt;/p&gt;

&lt;p&gt;First off, it is important to point out that $I$ is always non-negative, unlike its pointwise counterpart (proof &lt;a href=&quot;https://math.stackexchange.com/a/159544&quot;&gt;here&lt;/a&gt;). You can see this intuitively by trying to construct an anti-dependent relationship between $\rX$ and $\rY$. On average, $p(x, y)$ would have to be less than the product of their marginals. You can construct individual cases where this is true for a particular $(x, y)$, but to do that, you will have to fill most of the probability table (for 2D joint) with p-mass to compensate. This is reflected in Jensen’s inequality. A direct consequence is $H[\rY] \geq H[\rY \mid \rX]$.&lt;/p&gt;

&lt;p&gt;$I$ being non-negative means you can safely think about it as a measure of information content. In this sense, information is stored in the relationship between $\rX$ and $\rY$.&lt;/p&gt;

&lt;p&gt;Note that by remembering that expectation is linear, some useful identities pop out of the definition above,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
I[\rX, \rY] &amp;= H[\rX] + H[\rY] - H[(\rX,\rY)] \\
  &amp;= H[\rX] - H[\rX \mid \rY] \\
  &amp;= H[\rY] - H[\rY \mid \rX]\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;An intuitive way to think about $I$ is as a continuous measure of &lt;em&gt;bijectivity&lt;/em&gt; of the stochastic function, $g(x) \sim p(\rY \mid \rX = x)$, where $g : X \rightarrow Y$. This is easier to see if we write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I[\rX, \rY] = H[\rY] - H[\rY \mid \rX]\,.&lt;/script&gt;

&lt;p&gt;$H[\rY]$ measures &lt;em&gt;surjectivity&lt;/em&gt;, i.e. how much $g$ spreads out over $Y$ (marginalized over $\rX$). &lt;em&gt;surjective&lt;/em&gt; (a.k.a. onto) in the set-theory sense means that $g$ maps to every element in $Y$. In the statistical sense, $g$ may map to every element in $Y$ with some probability, but to some elements much more frequently than others. We would say $p(y)$ is &lt;em&gt;peaky&lt;/em&gt;, the opposite of spread out. Recall that $H$ measures statistical dispersion. Larger $H[\rY]$ means more even spread of probability mass across all the elements in $Y$. In that sense, it measures how surjective $g$ is.&lt;/p&gt;

&lt;p&gt;$H[\rY \mid \rX]$ measures &lt;em&gt;anti-injectivity&lt;/em&gt;. &lt;em&gt;injective&lt;/em&gt; (a.k.a. one-to-one) in the set-theory sense means that $g$ maps every element in $X$ to a unique element in $Y$. There is no sharing, and you know which $x \in X$ was the input for any $y \in Y$ in the image of $g(X)$. In the statistical sense, $g$ may map a given $x$ to many elements in $Y$, each with some probability, i.e. fan-out. Anti-injective is like a reversal of injective, which is about fan-in. The more $g$ fans-out, the more anti-injective it is. Recall that $H[\rY \mid \rX]$ measures averge uncertainty about $\rY$ given an observation from $\rX$. This is, in a sense, the average statistical fan-out of $g$. Lower $H[\rY \mid \rX]$ means $g$’s output is more concentrated (peaky) on average for a given $x$, and higher means its output is more uniformly spread on average.&lt;/p&gt;

&lt;p&gt;For a function to be a bijection, is needs to be both injective and surjective. $H[\rY]$ may seem like a good continuous proxy for surjectivity, but $H[\rY \mid \rX]$ seems to measure something different from injectivity. Notice that $H[\rY \mid \rX]$ is affected by the injectivity of $g^{-1}$. If $g^{-1}$ maps many $y$s to the same $x$, then we are uncertain about what $g(x)$ should be.&lt;/p&gt;

&lt;p&gt;In general, I claim that $I[\rX, \rY]$ measures how bijective $g$ is. $I[\rX, \rY]$ is maximized when $H[\rY]$ is maximized and $H[\rY \mid \rX]$ is minimized (i.e. 0). That is, when $g$ is maximally surjective and minimally anti-injective, implying it is maximally injective. Higher $I[\rX, \rY]$ actually does indicate that $g$ is more invertible because $I$ is symmetric. It measures how much information can flow through $g$ in either direction.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg&quot; alt=&quot;Useful diagram for keeping track of the relationships between these concepts.&amp;lt;br/&amp;gt;Credit: &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/Mutual_information&amp;quot;&amp;gt;https://en.wikipedia.org/wiki/Mutual_information&amp;lt;/a&amp;gt;&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;Useful diagram for keeping track of the relationships between these concepts.&lt;br /&gt;Credit: &lt;a href=&quot;https://en.wikipedia.org/wiki/Mutual_information&quot;&gt;https://en.wikipedia.org/wiki/Mutual_information&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Useful diagram for keeping track of the relationships between these concepts&lt;/p&gt;

&lt;h3 id=&quot;channel-capacity&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#channel-capacity&quot;&gt;Channel capacity&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;$I$ is determined by $p(x)$ just as much as $p(y \mid x)$, but $g$ has ostensibly nothing to do with $p(x)$. If we want $I$ to measure properties of $g$ in isolation, it should not care about the distribution over its inputs. One solution to this issue is to use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Channel_capacity#Formal_definition&quot;&gt;&lt;strong&gt;capacity&lt;/strong&gt;&lt;/a&gt; of $g$, defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
C[g] &amp;= \sup_{p_X(x)} I[\rX, \rY] \\
  &amp;= \sup_{p_X(x)} \E_{y\sim p_{g(x)}, x \sim p_X}[i(x, y)] \\
  &amp;= \sup_{p_X(x)} \E_{y\sim p_{Y \mid X=x}, x \sim p_X}[-h(y \mid x) + h(x)]\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;In other words, if you don’t have a preference for $p(x)$, choose $p(x)$ which maximizes $I[\rX, \rY]$.&lt;/p&gt;

&lt;h1 id=&quot;shannon-information-for-continuous-distributions&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#shannon-information-for-continuous-distributions&quot;&gt;Shannon Information For Continuous Distributions&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;Up to now we’ve only considered discrete distributions. Describing the information content in continuous distributions and their events is tricky business, and a bit more nuanced than usually portrayed. Let’s explore this.&lt;/p&gt;

&lt;p&gt;For this discussion, let’s consider a random variable $\rX$ with &lt;a href=&quot;https://en.wikipedia.org/wiki/Support_(mathematics)#Support_of_a_distribution&quot;&gt;support&lt;/a&gt; over $\R$. Let $f(x)$ be the probability density function (pdf) of $\rX$.&lt;/p&gt;

&lt;p&gt;Elementary events $x \in \rX$ &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;do not have probabilities perse&lt;/span&gt;&lt;label for=&quot;81ebfb9082ef06c6091bdf79ef229fe6555037fd&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;81ebfb9082ef06c6091bdf79ef229fe6555037fd&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;you could say their probability mass is 0 in the limit&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. Self-information is a function of probability mass, so we should instead compute self-info of events that are intervals (or measurable sets) over $\R$. For example,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
h(a &lt; x &lt; b) &amp;= -\log_2\,p(a &lt; \rX &lt; b)\\
  &amp;= -\log_2\left(\int_a^b f(x) \diff x\right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Conjecture: The entropy of any distribution with uncountable support is infinite. This should make sense, as we now have uncountably many possible outcomes. One observation rules out infinitely many alternatives, so it should contain infinite information. We can see this clearly because the entropy of a uniform distribution over $N$ possibilities is $\log_2 N$ which grows to infinity as $N$ does. On the other hand, a one-hot distribution over $N$ possibilities has 0 entropy, because you will &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;always observe&lt;/span&gt;&lt;label for=&quot;2b312968d2ec3765f14ce161dd47e1212e3f03cc&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;2b312968d2ec3765f14ce161dd47e1212e3f03cc&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Unless you observe an impossible outcome, in which case you gain infinite information!&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; the probability-1 outcome and gain 0 information. So we expect the Dirac-delta distribution to have 0 entropy.&lt;/p&gt;

&lt;p&gt;But wait, the Gaussian distribution is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution&quot;&gt;maximum-entropy&lt;/a&gt; distribution, implying that the entropy of continuous distributions can be numerically compared! People talk about entropy of continuous distributions all the time! What people normally call entropy for continuous distributions is actually &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_entropy&quot;&gt;differential entropy&lt;/a&gt;, which is not the same thing as the $H$ we’ve been working with.&lt;/p&gt;

&lt;p&gt;I’ll show that $H[\rX]$ is infinite when the distribution has continuous support, following a similar proof in &lt;a href=&quot;https://www.crmarsh.com/static/pdf/Charles_Marsh_Continuous_Entropy.pdf&quot;&gt;Introduction to Continuous Entropy&lt;/a&gt;.  To do that, let’s take a &lt;a href=&quot;https://en.wikipedia.org/wiki/Riemann_sum&quot;&gt;Riemann sum&lt;/a&gt; of $f(x)$. Let $\{x_i\}_{i=-\infty}^\infty$ be a set of points equally spaced by intervals of $\Delta$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
% \def\u{\Delta x}
\def\u{\Delta}
\begin{align}
H[\rX] &amp;= -\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u \log_2\left(f(x_i) \u\right) \\
  &amp;= -\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u \log_2\left(f(x_i)\right) - \lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u \log_2\left(\u\right)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The left term is just the Riemann integral of $f(x)\log_2(f(x))$, which I will define as &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;&lt;strong&gt;differential entropy&lt;/strong&gt;&lt;/span&gt;&lt;label for=&quot;642b79d7e276fc9865eef223339f4eb4b1a72b14&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;642b79d7e276fc9865eef223339f4eb4b1a72b14&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Typically $h$ is used to denote differential entropy, but I’ve already used it for self-information, so I’m using $\eta$ instead.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta[f] := -\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u \log_2\left(f(x_i)\right) = -\int_{-\infty}^\infty f(x) \log_2\left(f(x)\right) \diff{x}\,.&lt;/script&gt;

&lt;p&gt;The right term can be simplified using the &lt;a href=&quot;https://tutorial.math.lamar.edu/Classes/CalcI/LimitsProperties.aspx&quot;&gt;limit product rule&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u \log_2\left(\u\right) = -\left(\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u\right)\cdot\left(\lim\limits_{\u \to 0}\log_2\left(\u\right)\right)\,.&lt;/script&gt;

&lt;p&gt;Note that&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty f(x_i) \u = \int_{-\infty}^\infty f(x) \diff{x} = 1\,,&lt;/script&gt;&lt;br /&gt;
because $f(x)$ is a p.d.f.&lt;/p&gt;

&lt;p&gt;Putting it all together we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H[\rX] = \eta[f] - \lim\limits_{\u \to 0}\log_2\left(\u\right)\,.&lt;/script&gt;

&lt;p&gt;$\log_2(\u) \to -\infty$ as $\u \to 0$, so $H[\rX]$ explodes to infinity when $\eta[f]$ is finite, which it is for most well-behaved functions.&lt;/p&gt;

&lt;p&gt;A simple proof that $H$ is finite for continuous distributions with support over an finite set: the Riemann sum above will only have at most finitely many non-zero terms as $\Delta \to \infty$.&lt;/p&gt;

&lt;p&gt;Differential entropy is very different from entropy. It can be unboundedly negative. For example, the differential entropy of a Gaussian distribution with variance $\sigma^2$ is $\frac{1}{2}\ln(2\pi e \sigma^2)$. Taking the limit as $\sigma \to 0$, we see the differential entropy of the &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;Dirac-delta distribution is $-\infty$&lt;/span&gt;&lt;label for=&quot;db0f66f8ebc12d35b9801d084e4ee4afbd893dc5&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;db0f66f8ebc12d35b9801d084e4ee4afbd893dc5&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Plugging $\eta[f] = -\infty$ into our relation $H[\rX] = \eta[f] - \lim\limits_{\u \to 0}\log_2\left(\u\right)$, we see why entropy of $\delta(x)$ would be 0.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. A notable problem with differential entropy is that its not invariant to change of coordinates, and there is a proposed fix for that: &lt;a href=&quot;https://en.wikipedia.org/wiki/Limiting_density_of_discrete_points&quot;&gt;https://en.wikipedia.org/wiki/Limiting_density_of_discrete_points&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;proof-that-mi-is-fininte-for-continuous-distributions&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#proof-that-mi-is-fininte-for-continuous-distributions&quot;&gt;Proof that MI is fininte for continuous distributions&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;A very nice result is that expected mutual information is finite where entropy would be infinite, so long as there is some amount of noise between the two random variables. This implies that even if physical processes are continuous and contain infinite information, we can only get finite information out of them, because measurement requires establishing a statistical relation between the measurement device and that system which is always noisy in reality. MI is agnostic to discrete or continuous universes! As long as there is some amount of noise in between a system and your measurement, your measurement will contain finite information about the system.&lt;/p&gt;

&lt;p&gt;The proof follows the same Riemann sum approach from the previous section. I will show that mutual information and differential mutual information are equivalent. Since differential mutual information finite for well behaved functions, so is mutual information!&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
I[\rX, \rY] &amp;= -\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty \sum\limits_{j=-\infty}^\infty f_{XY}(x_i, y_j) \u^2 \log_2\left(\frac{f_{XY}(x_i, y_j)\u^2}{f_X(x_i)\u f_Y(y_i)\u} \right) \\
  &amp;= -\lim\limits_{\u \to 0} \sum\limits_{i=-\infty}^\infty \sum\limits_{j=-\infty}^\infty f_{XY}(x_i, y_j) \u^2 \log_2\left(\frac{f_{XY}(x_i, y_j)}{f_X(x_i)f_Y(y_i)} \right) \\
  &amp;= \int_{-\infty}^\infty \int_{-\infty}^\infty f_{XY}(x_i, y_j) \log_2\left(\frac{f_{XY}(x_i, y_j)}{f_X(x_i)f_Y(y_i)} \right) \diff{y}\diff{x}\,
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;because the $\Delta$s cancel inside the log.&lt;/p&gt;

&lt;p&gt;If $p(\rY \mid \rX = x)$ is a Dirac-delta for all $x$, and $p(\rY)$ has continuous support, then $I[\rX, \rY]= H[\rY] - H[\rY \mid \rX] = \infty$ because $H[\rY]=\infty$ and $H[\rY \mid \rX]=0$. Thus some noise between $\rX$ and $\rY$ is required to make the MI finite. It follows that $I[\rX, \rX] = H[\rX] = \infty$ when $\rX$ has continuous support.&lt;/p&gt;

&lt;h1 id=&quot;problems-with-shannon-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#problems-with-shannon-information&quot;&gt;Problems With Shannon Information&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; Do the concepts just outlined capture our colloquial understanding of information? Are there situations where they behave differently from how we expect information to behave? I’ll go through some fairly immediate objections to this Shannon’s definition of information, and some remedies.&lt;/p&gt;

&lt;h2 id=&quot;1-tv-static-problem&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#1-tv-static-problem&quot;&gt;1. TV Static Problem&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Imagine a TV displaying static noise. If we assume a fairly uniform distribution over all “static noise” images, we know that the entropy of the TV visuals will be high, because probability mass is spread fairly evenly across all possible images. Each image on average has a very low probability of occurring. According to Shannon, each image then contains a large amount of information.&lt;/p&gt;

&lt;p&gt;That may sound absurd. &lt;a href=&quot;https://en.wikipedia.org/wiki/Noise_(signal_processing)&quot;&gt;Noise&lt;/a&gt;, by some definitions, carries no useful information. Noise is uninformative.  To a human looking at TV static, the information gained is that the TV is not displaying anything. This is a very high level piece of information, but much less than the supposedly high information content of the static itself.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/posts/primer-shannon-information/tv-static.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The resolution here is to define what it means for a human to obtain information. I propose looking at the mutual information between the TV and the viewer’s brain. Let $\rX$ be a random variable over TV images, and $\rZ$ be a random variable over the viewer’s brain states. The support of $\rX$ is the space of all possible TV screens, so static and SpongeBob are just different distributions over the same space. Now, the state of the viewer’s brain is causally connected to what is on the TV screen, but the nature of their visual encoder (visual cortex) determines $p(\rZ \mid \rX)$, and thus $p(\rZ, \rX)$. I would guess that any person who says TV static is uninformative does not retain much detail about the patterns in the static. Basically, that person would just remember that they saw static. What we have here is a region of large fan-in. Many static images are collapsed to a single output for their visual encoder, namely the label “TV noise”. So the information contained in TV static is low to a human, because $I[\rX, \rZ]$ is low when $\rX$ is the distribution of TV static.&lt;/p&gt;

&lt;p&gt;Note that the signal, “TV noise”, is still rather informative, if you consider the space of all possible labels you could assign to the TV screen, e.g. “SpongeBob” or “sitcom”. Further, that you are looking at a TV and not anything else is information.&lt;/p&gt;

&lt;h2 id=&quot;2-shannon-information-is-blind-to-scrambling&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#2-shannon-information-is-blind-to-scrambling&quot;&gt;2. Shannon Information is Blind to Scrambling&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Encryption scrambles information to make it inaccessible to prying eyes. Encryption is usually lossless, meaning the original message is fully recoverable. If $\rX$ is a distribution over messages, then the encryption function Enc should preserve that distribution. To Shannon information, $\rX$ and $\text{Enc}(\rX)$ contain the same information. Shannon information is therefore blind to operations like scrambling which do something interesting to the information present, i.e. like making it accessible or inaccessible.&lt;/p&gt;

&lt;p&gt;The resolution is again mutual information. While permuting message space (or any bijective transformation) does not change information content under Shannon, it changes the useful information content. A human looking at (or otherwise perceiving) a message is creating a casual link between the message and a representation in the brain. This link has mutual information. Likewise, any measurement apparatus establishes a link between physical state and a representation of that state (the measurement result), again establishing mutual information.&lt;/p&gt;

&lt;p&gt;Information in a message becomes inaccessible or useless when the representation of the message cannot distinguish between two messages. Encryption maps the part of message space that human brains can discriminate, i.e. meaningful English sentences (or other such meaningful content) to a part of message space that humans cannot discriminate, i.e. apparently arbitrary character strings. These arbitrary strings appear to be meaningless because they are all mapped to the same or similar representation in our heads, namely the “junk text” label. In short, mutual information between plain text and brain states is much higher than mutual information between encrypted text and brain states.&lt;/p&gt;

&lt;h2 id=&quot;3-deterministic-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#3-deterministic-information&quot;&gt;3. Deterministic information&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;How is data on disk contain information if it is fixed and known? How does the output of a deterministic computer program contain information? How do math proofs contain information? All these things do not have an inherent probability distribution. If there is uncertainty, we might call it &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;logical uncertainty&lt;/span&gt;&lt;label for=&quot;0e18784002a64f3be42ab79f74a4282e0d11c58f&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;0e18784002a64f3be42ab79f74a4282e0d11c58f&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;See &lt;a href=&quot;https://intelligence.org/2016/04/21/two-new-papers-uniform/&quot;&gt;New papers dividing logical uncertainty into two subproblems&lt;/a&gt;&lt;br /&gt;and &lt;a href=&quot;https://golem.ph.utexas.edu/category/2016/09/logical_uncertainty_and_logica.html&quot;&gt;Logical Uncertainty and Logical Induction&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. It is an open question whether logical uncertainty and empirical uncertainty should be conflated, and both brought under the umbrella of probability theory.&lt;/p&gt;

&lt;p&gt;This is similar to asking, how does Shannon information account for what I already know? When I observe a message I didn’t already know it is informative, but what about the information contained in messages I currently have?  It is also an open question whether probability should be considered objective or subjective, and whether quantities of information are objective or subjective. Perhaps you regard a message you have to be informative, because you are implicitly modeling its information w.r.t. some other receiver who has not yet received it.&lt;/p&gt;

&lt;h2 id=&quot;4-if-the-universe-is-continuous-everything-contains-infinite-information&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#4-if-the-universe-is-continuous-everything-contains-infinite-information&quot;&gt;4. If the universe is continuous everything contains infinite information&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;This one is resolved by the discussion above about mutual information of continuous distributions being finite, so long as there is noise between the two random variables. Thus, in a universe where all measurements are noisy, mutual information is always finite regardless of the underlying meta-physics (whether objects contain finite or infinite information in an absolute sense).&lt;/p&gt;

&lt;h2 id=&quot;5-shannon-information-ignores-the-meaning-of-messages&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#5-shannon-information-ignores-the-meaning-of-messages&quot;&gt;5. Shannon information ignores the meaning of messages&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;There is a competing information theory, &lt;a href=&quot;https://en.wikipedia.org/wiki/Algorithmic_information_theory&quot;&gt;algorithmic information theory&lt;/a&gt; which uses the length of the shortest program that can output a message $x$ as the information measure of $x$, called &lt;a href=&quot;https://en.wikipedia.org/wiki/Kolmogorov_complexity&quot;&gt;Kolmogorov complexity&lt;/a&gt;. If $x$ is less compressible, it contains more information. This is analogous to low $p_X(x)$ leading to its optimal &lt;a href=&quot;https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding&quot;&gt;Shannon-Fano&lt;/a&gt; being longer, and thus containing more information.&lt;/p&gt;

&lt;p&gt;Algorithmic information theory addresses the criticism that $h(x)$ depends only on the probability of $x$, rather than the meaning of $x$. If $x$ is a word, sentence, or even a book, the information content of $x$ supposedly does not depend on what the text is! Algorithmic information theory defines information as a property of the content of $x$ as a string, and drops the dependency on probability.&lt;/p&gt;

&lt;p&gt;I think this criticism does not consider what &lt;em&gt;meaning&lt;/em&gt; is. A steel-man’ed Shannon information at least seems self-consistent to me. Again, the right approach is to use mutual information. I propose that the meaning of a piece of text is ultimately due to the brain state it invokes in you when you read it. Your &lt;a href=&quot;https://www.deeplearningbook.org/contents/representation.html&quot;&gt;representation&lt;/a&gt; of the text shares information with the text. So while yes the probability of $x$ in the void may be meaningless, the joint probability of $(x, z)$ where $z$ is your brain state is what gives $x$ meaning. Shannon information being blind to what we are calling the contents of a message can be seen as a virtue. In other words, Shannon is blind to &lt;em&gt;preconceived&lt;/em&gt; meaning. While statistical variance cares about the Euclidean distance between points in $\mathbb{R}^n$, entropy does not and should not if the mathematical representation of these points as vectors is not important. Shannon does not care what you label your points! Their meaning comes solely from their co-occurrence with other random variables.&lt;/p&gt;

&lt;p&gt;I think condensing a string of text, like a book, into one random variable $\rX$ is very misleading, because this distribution factors! A book is a single outcome from a distribution over all strings of characters, and we write this distribution as $p(\rC_i \mid \rC_{i-1}, \ldots, \rC_2, \rC_1)$ where $\rC_i$ is the random variable for the $i$-th character in the book. In this way, each character position contains semantic information in its probability distribution conditioned on the previous character choices. The premise of &lt;a href=&quot;https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf&quot;&gt;language modeling&lt;/a&gt; in machine learning is that the statistical relationships between words (their frequencies of co-occurrence) in a corpus of text &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;determine their meaning&lt;/span&gt;&lt;label for=&quot;420e4613ec913eb3ebc7f105b4ba7df0378bbd7b&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;420e4613ec913eb3ebc7f105b4ba7df0378bbd7b&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The theory goes that a computer which can estimate frequencies of words very precisely would implicitly have to create internal representations of those words which encode their meaning, and so beefed up language modeling is all that is needed for intelligence.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-probability-distributions-are-not-objective&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#6-probability-distributions-are-not-objective&quot;&gt;6. Probability distributions are not objective&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;I touched on this already. Probability has two interpretations: frequentist (objective) and Bayesian (subjective). It is unclear if frequentist probability is an objective property of matter. For repeatable controlled experiments, a frequentist description is reasonable, like in games of chance, and in statistical mechanics and quantum mechanics. When probability is extended to systems that don’t repeat in any meaningful sense, like the stock market or historical events, the objectiveness is dubious. There is a camp that argues probability should reflect the state of belief of an observer, and is more a measurement of the brain doing the observing than the thing being observed.&lt;/p&gt;

&lt;p&gt;So then this leads to an interesting question: is Shannon information a property of a system being observed, or a property of the observer in relation to it (or both together)? Is information objective in the sense that multiple independent parties can do measurements to verify a quantity of information, or is it subjective in the sense that it depends on the beliefs of the person doing the calculating? I am not aware of any answer or consensus on this question for information in general,&lt;/p&gt;

&lt;h1 id=&quot;appendix&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#appendix&quot;&gt;Appendix&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;properties-of-conditional-entropy&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#properties-of-conditional-entropy&quot;&gt;Properties of Conditional Entropy&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Source: https://en.wikipedia.org/wiki/Conditional_entropy#Properties&lt;/p&gt;

&lt;p&gt;$H[\rY \mid \rX] = H[(\rX, \rY)] - H[\rX]$&lt;/p&gt;

&lt;p&gt;Bayes’ rule of conditional entropy:&lt;br /&gt;
$H[\rY \mid \rX] = H[\rX \mid \rY] - H[\rX] + H[\rY]$&lt;/p&gt;

&lt;p&gt;Minimum value:&lt;br /&gt;
$H[\rY \mid \rX] = 0$ when $p(y \mid x)$ is always deterministic, i.e. one-hot, i.e. $p(y \mid x) \in \{0, 1\}$ for all $(x, y) \in X \times Y$.&lt;/p&gt;

&lt;p&gt;Maximum value:&lt;br /&gt;
$H[\rY \mid \rX] = H[\rY]$ when $\rX, \rY$ are independent.&lt;/p&gt;

&lt;h2 id=&quot;bayes-rule&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#bayes-rule&quot;&gt;Bayes’ Rule&lt;/a&gt;&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y \mid x) = p(x \mid y)p(y)/p(x)&lt;/script&gt;

&lt;p&gt;can be rewritten in terms of self-information:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(y \mid x) = h(x \mid y) + h(y) - h(x)\,.&lt;/script&gt;

&lt;p&gt;The information contained in $y$ given $x$ is proportional to the information contained in $x$ given $y$ plus the information contained in $y$. This is just Bayes’ rule in log-space, but makes it a bit easier to reason about what Bayes’ rule is doing. Whether $y$ is likely in its own right and whether $x$ is likely given $y$ both contribute to the total information.&lt;/p&gt;

&lt;h2 id=&quot;cross-entropy-and-kl-divergence&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#cross-entropy-and-kl-divergence&quot;&gt;Cross Entropy and KL-Divergence&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Unlikely everything we’ve seen so far, these are necessarily functions of probability functions, rather than random variables. Further, these are both comparisons of probability functions over the same support.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(P,Q) = -\sum_x P(x)\log Q(x)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{K}\left(P\,||\,Q\right) = \sum_{x} P(x)\log
{\frac{P(x)}{Q(x)}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{K}\left(P\,||\,Q\right) = H(P,Q) - H(P)&lt;/script&gt;

&lt;p&gt;Sources:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://stats.stackexchange.com/questions/111445/analysis-of-kullback-leibler-divergence&lt;/li&gt;
  &lt;li&gt;https://stats.stackexchange.com/questions/357963/what-is-the-difference-cross-entropy-and-kl-divergence&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Mutual information can be &lt;a href=&quot;https://en.wikipedia.org/wiki/Mutual_information#Relation_to_Kullback%E2%80%93Leibler_divergence&quot;&gt;written in terms of KL-divergence&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I[\rX, \rY] = \mathbb{K}\left(p_{X,Y}\,||\,p_X \cdot p_Y\right)&lt;/script&gt;

&lt;p&gt;where $(p_X \cdot p_Y)(x, y) \mapsto p_X(x) \cdot p_Y(x)$.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgments&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#acknowledgments&quot;&gt;Acknowledgments&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;I would like to thank John Chung for extensive and Aneesh Mulye for excruciating feedback on the structure and language of this post.&lt;/p&gt;

</description>
        <pubDate>Tue, 09 Jun 2020 00:00:00 -0700</pubDate>
        <link>pragmanym.github.io/zhat/articles/primer-shannon-information</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/primer-shannon-information</guid>
        
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Wallace: Emergence of particles from QFT - Study Notes</title>
        <description>
</description>
        <pubDate>Fri, 10 Jan 2020 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/wallace-particles-qft</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/wallace-particles-qft</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Visualizing Quantum Field States - Study Notes</title>
        <description>
</description>
        <pubDate>Fri, 10 Jan 2020 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/visualizing-quantum-fields</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/visualizing-quantum-fields</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Solomonoff Induction - Study Notes</title>
        <description>
</description>
        <pubDate>Tue, 31 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/solomonoff-induction</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/solomonoff-induction</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Weak Measurement (Quantum Mechanics) - Study Notes</title>
        <description>
</description>
        <pubDate>Tue, 24 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/weak-measurement</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/weak-measurement</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Topology: Sphere &amp; Torus - Study Notes</title>
        <description>
</description>
        <pubDate>Mon, 23 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/topology-sphere-torus</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/topology-sphere-torus</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Cox's Theorem - Study Notes</title>
        <description>
</description>
        <pubDate>Mon, 23 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/coxs-theorem</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/coxs-theorem</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Quantum State</title>
        <description>&lt;p&gt;The two views of quantum state:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Quantum states are $L^2$-normalized complex-valued functions over classical configuration space.&lt;/li&gt;
  &lt;li&gt;Quantum states are unit vectors residing in a complex Hilbert space, $\mathcal{H}$.&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\newcommand{\bm}{\boldsymbol}
\newcommand{\diff}[1]{\mathop{\mathrm{d}#1}} 
\newcommand{\bra}[1]{\langle#1\rvert}
\newcommand{\ket}[1]{\lvert#1\rangle}
\newcommand{\braket}[2]{\langle#1\vert#2\rangle}&lt;/script&gt;

&lt;!--more--&gt;

&lt;ul class=&quot;toc&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-quantum-states-are-functions&quot; id=&quot;markdown-toc-1-quantum-states-are-functions&quot;&gt;1) Quantum States Are Functions&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#punchline-quantum-states-are-probability-distributions&quot; id=&quot;markdown-toc-punchline-quantum-states-are-probability-distributions&quot;&gt;Punchline: quantum states are probability distributions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-quantum-states-are-vectors&quot; id=&quot;markdown-toc-2-quantum-states-are-vectors&quot;&gt;2) Quantum States Are Vectors&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#measurement-bases&quot; id=&quot;markdown-toc-measurement-bases&quot;&gt;Measurement Bases&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#a-proper-basis&quot; id=&quot;markdown-toc-a-proper-basis&quot;&gt;A Proper Basis&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#punchline-uncertainty-principle-from-change-of-basis&quot; id=&quot;markdown-toc-punchline-uncertainty-principle-from-change-of-basis&quot;&gt;Punchline: Uncertainty Principle From Change Of Basis&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a very short primer on quantum mechanics, and the bare minimum that needs to be explained in order to understand what QM is about. My goal is to provide some conceptual punchlines with minimal prereqs. The formalism of QM is filled with technicalities and complications, which I try to point out where possible. Keep in mind I am glossing over a ton of details.&lt;/p&gt;

&lt;p&gt;Note, throughout I am setting $\hbar = 1$.&lt;/p&gt;

&lt;h1 id=&quot;1-quantum-states-are-functions&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#1-quantum-states-are-functions&quot;&gt;1) Quantum States Are Functions&lt;/a&gt;&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Quantum states are $L^2$-normalized complex-valued functions over classical configuration space.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For our purposes, classical configuration space is position space. A system with $N$ particles in 3D space has $3N$ dimensional configuration space. For simplicity, I’ll mostly talk about one particle in 1D space, i.e. 1D configuration space.&lt;/p&gt;

&lt;p&gt;So to restate, a quantum state is a function,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi : \mathbb{R}^n \rightarrow \mathbb{C}\,.&lt;/script&gt;

&lt;p&gt;for $n$-dimensional configuration space such that $\lvert\psi\rvert = 1$, i.e. $\psi$ is $L^2$-normalized.&lt;/p&gt;

&lt;p&gt;The $L^2$-norm of a complex-valued function is given as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert\psi\rvert = \sqrt{\int_{\mathbb{R}^n} \psi(x)\bar{\psi}(x)\diff{x
^n}}\,,&lt;/script&gt;

&lt;p&gt;where $\bar{f}(x) = \text{Re}[f(x)] - i\,\text{Im}[f(x)]$ is the complex conjugate.&lt;/p&gt;

&lt;h2 id=&quot;punchline-quantum-states-are-probability-distributions&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#punchline-quantum-states-are-probability-distributions&quot;&gt;Punchline: quantum states are probability distributions&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;You probably already guessed that $\psi$ encodes the probability of finding particles in space. The probability of observing system $\psi$ to be in &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;some region of configuration space&lt;/span&gt;&lt;label for=&quot;ec51e042e3a91e03088eeebfa8e927d02ca43c07&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;ec51e042e3a91e03088eeebfa8e927d02ca43c07&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Remember that $n$ is the total number of spatial coordinates that describes the system. We are calculating the probability for all the coordinates together to be in some range.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; $D \subseteq \mathbb{R}^n$ is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_D  \psi(x) \bar{\psi}(x)\diff{x^n}\,.&lt;/script&gt;

&lt;p&gt;$\psi(x) \bar{\psi}(x) = \lvert\psi(x)\rvert^2$ is the absolute value squared of the complex value $\psi(x)$ (not to be confused with the function-norm $\lvert\psi\rvert$). In other words, $\lvert\psi(x)\rvert^2$ is a probability density. $\psi(x)$ is called a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;&lt;em&gt;probability amplitude&lt;/em&gt;&lt;/span&gt;&lt;label for=&quot;8fd9acc0f5baccc0d4f8236ac8210c8d7555b801&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;8fd9acc0f5baccc0d4f8236ac8210c8d7555b801&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;“Probability amplitude” refers specifically to a complex number whose square is a probability. I don’t think the word “amplitude” makes this apparent and it is rather ambiguous, but it’s handy to have a term to refer to these not-quite-probabilities and that is the convention.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-quantum-states-are-vectors&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#2-quantum-states-are-vectors&quot;&gt;2) Quantum States Are Vectors&lt;/a&gt;&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Quantum states are unit vectors residing in a complex Hilbert space, $\mathcal{H}$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;&lt;em&gt;Hilbert space&lt;/em&gt;&lt;/span&gt;&lt;label for=&quot;da5780db3d541999f81e9889c2af05f261b22854&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;da5780db3d541999f81e9889c2af05f261b22854&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The definition of Hilbert space is surprisingly ambiguous or too technical in most sources. For a while I was confused about what was special about Hilbert spaces in contrast with typical vector spaces. The axioms of linear algebra &lt;a href=&quot;https://math.stackexchange.com/a/28876&quot;&gt;allow for complex-valued scalars&lt;/a&gt;. The other notable feature of Hilbert space is that it allows for infinite dimensions, but that doesn’t make it unique. There are many ways to construct infinite dimensional vector spaces, e.g. &lt;a href=&quot;https://math.stackexchange.com/a/466741&quot;&gt;this answer&lt;/a&gt;. The distinguishing feature of Hilbert space is that it is &lt;a href=&quot;https://en.wikipedia.org/wiki/Complete_metric_space&quot;&gt;&lt;em&gt;complete&lt;/em&gt;&lt;/a&gt;, meaning that all &lt;a href=&quot;https://en.wikipedia.org/wiki/Cauchy_sequence&quot;&gt;Cauchy sequences&lt;/a&gt; of vectors in Hilbert space converge to vectors in Hilbert space. This ensures we can do calculus. It is true that all finite dimensional inner product spaces are Hilbert spaces, and they are simultaneously many other types of spaces, e.g. Banach spaces. Finite dimensional vector spaces do show up in quantum mechanics, and they are still called Hilbert spaces for consistency. Further reading: &lt;a href=&quot;https://www.quantiki.org/wiki/hilbert-spaces&quot;&gt;A&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Hilbert_space#History&quot;&gt;B&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is a generalization of Euclidean vector space to real or complex-valued vector spaces, with finite or infinite dimensions. The generalization of the Euclidean inner-product for functions is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle f, g \rangle = \int_{\mathbb{R}^n} f(x)\bar{g}(x)\diff{x^n}\,.&lt;/script&gt;

&lt;p&gt;Let $\mathcal{H}$ be a Hilbert space of $L^2$-normalized complex-valued functions. Then any $\psi \in \mathcal{H}$ is a unit-vector, since $\lvert\psi\rvert^2 = \langle \psi, \psi \rangle = 1$.&lt;/p&gt;

&lt;h2 id=&quot;measurement-bases&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#measurement-bases&quot;&gt;Measurement Bases&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Given that the elements of $\mathcal{H}$ are functions over $x$, one straightforward way to choose a basis for $\mathcal{H}$ is to think of $\psi \in \mathcal{H}$ as a vector with uncountably many entries, one for each input $x$. This is achieved with a basis of &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function&quot;&gt;Dirac delta functions&lt;/a&gt;&lt;/span&gt;&lt;label for=&quot;68625e91f114b5de914c4a1ae96b612e8a90302e&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;68625e91f114b5de914c4a1ae96b612e8a90302e&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;0 everywhere except for the origin. Defined so that its total area is 1. Technically they are not functions, but &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function#As_a_distribution&quot;&gt;distributions&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. Writing $\psi$ in this basis,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\psi &amp;= \int_{\mathbb{R}^n} \psi(\chi)\cdot\left[ x \mapsto \delta(x-\chi)\right]\diff{\chi^n} \\
     &amp;= x \mapsto \int_{\mathbb{R}^n} \psi(\chi)\delta(x-\chi)\diff{\chi^n} \\
     &amp;= x \mapsto \psi(x)\,.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;This integral is an uncountable linear combination, where &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;$x \mapsto \delta(x-\chi)$&lt;/span&gt;&lt;label for=&quot;6e2c209cbdb6c596f74638233310e8ffd60cc39e&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;6e2c209cbdb6c596f74638233310e8ffd60cc39e&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Note I am using non-standard notation. “$x \mapsto f(x)$” instantiates a function object in-line, just as “$\{f(y) \mid \text{condition}[y]\}$” instantiates a set in-line.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the $\chi$-th basis element and $\psi(\chi)$ is the corresponding coefficient.&lt;/p&gt;

&lt;p&gt;This basis of Dirac deltas corresponds to position measurement. Particles and systems of particles have other measurable properties, like momentum and total energy, which have their own bases. More on that below.&lt;/p&gt;

&lt;h2 id=&quot;a-proper-basis&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#a-proper-basis&quot;&gt;A Proper Basis&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;There is a problem here. $b_\chi = x \mapsto \delta(x-\chi)$ is not $L^2$-normalizable, because $\lvert b_\chi\rvert^2 = \int_{\mathbb{R}^n} \delta(x-\chi)^2\diff{x^n}$ is undefined. Such a basis that is not actually contained in our vector space is sometimes called an &lt;em&gt;improper basis&lt;/em&gt;. This can be remedied by augmenting $\mathcal{H}$ to include objects like Dirac deltas. The resulting construction is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Rigged_Hilbert_space&quot;&gt;rigged Hilbert space&lt;/a&gt;. However, this is not an ideal solution because &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;now we have quantum states&lt;/span&gt;&lt;label for=&quot;0f554cb904b48367e4f303e35eba33200371deb9&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;0f554cb904b48367e4f303e35eba33200371deb9&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;We will soon see that we want our Hilbert space to be closed under Fourier transform, so if Dirac deltas are included, then so should sin waves of the form $x \mapsto e^{ikx}$, which are also not $L^2$-normalizable.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; which cannot be interpreted as probability distributions.&lt;/p&gt;

&lt;p&gt;There are plenty of valid orthonormal bases for the Hilbert space of &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;unit vectors&lt;/span&gt;&lt;label for=&quot;a407fae48b39bd45ead1aeb29b1a295c009b4471&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;a407fae48b39bd45ead1aeb29b1a295c009b4471&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;And also $L^2$-normalized functions.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. You may be surprised to learn that such bases are necessarily countable. One of the most well known is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_harmonic_oscillator#Hamiltonian_and_energy_eigenstates&quot;&gt;measurement basis for the total energy of the harmonic oscillator&lt;/a&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{B} = \{x \mapsto(-1)^n(2^nn!\sqrt{\pi})^{-\frac{1}{2}}e^{\frac{x^2}{2}}\frac{\diff{^n}}{\diff{x^n}}\left[e^{-x^2}\right]\,\mid\, n \in \mathbb{N}\}\,.&lt;/script&gt;

&lt;p&gt;Symbolically these functions look quite complicated, but you can get a sense for the pattern of what these functions look like as $n$ increases:&lt;br /&gt;
&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Herm5.svg/900px-Herm5.svg.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
*Plot of first 6 basis functions: 0 (black), 1 (red), 2 (blue), 3 (yellow), 4 (green), and 5 (magenta). Note that these are real-valued functions. Source: &lt;a href=&quot;https://en.wikipedia.org/wiki/Hermite_polynomials#Hermite_functions&quot;&gt;https://en.wikipedia.org/wiki/Hermite_polynomials#Hermite_functions&lt;/a&gt;.&lt;br /&gt;
 *&lt;/p&gt;

&lt;p&gt;It’s amazing that can write any function in $\mathcal{H}$ as a linear combination of the functions in $\mathcal{B}$, i.e. any continuous complex-valued $L^2$-normalized function can be written as a weighted sum of these functions. We can also use $\mathcal{B}$ to explicitly construct our $L^2$-normalized Hilbert space,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{H} = \{c_0H_0(x) + c_1H_1(x) + c_2H_2(x) + \ldots \,|\, c_0,c_1,c_2,\ldots\in\mathbb{C}\}\,,&lt;/script&gt;

&lt;p&gt;where $H_n(x) \in \mathcal{B}$ is the $n$-th &lt;a href=&quot;https://en.wikipedia.org/wiki/Hermite_polynomials#Definition&quot;&gt;Hermite function&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;punchline-uncertainty-principle-from-change-of-basis&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#punchline-uncertainty-principle-from-change-of-basis&quot;&gt;Punchline: Uncertainty Principle From Change Of Basis&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Under the vector view, the coefficients of a quantum state in some measurement basis are &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;probability amplitudes&lt;/span&gt;&lt;label for=&quot;da0f2680d7f9c26d8d21b75aee55f3fa4e0dbc0d&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;da0f2680d7f9c26d8d21b75aee55f3fa4e0dbc0d&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Their absolute squares are probabilities or probability densities.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. The basis vectors are called &lt;em&gt;definite&lt;/em&gt; states, and their linear combinations are indefinite. So if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi = c_1 B_1 + c_2 B_2 + \ldots\,&lt;/script&gt;

&lt;p&gt;for measurement basis $\{B_1, B_2, \ldots\}$, when the corresponding property of $\psi$ is measured, the system will instantaneously jump to one of $B_i$ with probability $\lvert c_i\rvert^2$. This instantaneous jump is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Wave_function_collapse&quot;&gt;&lt;em&gt;wave-function collapse&lt;/em&gt;&lt;/a&gt;. The experimenter will also know which of the basis states the system is in because there is a corresponding measurement readout for each (unless the measurement is degenerate). In the position basis, we recover the probability interpretation of states as functions.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Superposition&lt;/em&gt; is the name given to the physical phenomenon of a system being in an indefinite state, meaning that when you observe some property the outcome is probabilistic. Mathematically superposition is modeled by linear combinations of definite states. A key facet of quantum mechanics is that multiple measurable properties of a system cannot be &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;simultaneously definite&lt;/span&gt;&lt;label for=&quot;7066f1de8a35a976b9f072db9e312f0a6ba14845&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;7066f1de8a35a976b9f072db9e312f0a6ba14845&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Aside from so-called &lt;a href=&quot;https://en.wikipedia.org/wiki/Complete_set_of_commuting_observables&quot;&gt;commuting observables&lt;/a&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. We get this phenomenon for free out of the framework of linear algebra, by representing properties to be measured as orthonormal bases in Hilbert space.&lt;/p&gt;

&lt;p&gt;The famous example is position and momentum. Both these properties of a particle cannot be known at the same time. We saw that the position (improper) basis for one spatial coordinate is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{B}_\text{x} = \{x \mapsto \delta(x - \chi) \mid \chi \in \mathbb{R}\}\,.&lt;/script&gt;

&lt;p&gt;The momentum basis is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Position_and_momentum_space#Relation_between_space_and_reciprocal_space&quot;&gt;Fourier conjugate&lt;/a&gt; of the position basis, given as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{B}_\text{p} = \{x \mapsto \frac{1}{\sqrt{2\pi}}e^{ipx} \mid p \in \mathbb{R}\}\,.&lt;/script&gt;

&lt;p&gt;The real part of these functions are sin waves. An element $[x \mapsto \delta(x - \chi)] \in \mathcal{B}_\text{x}$ of the position basis can be written as a linear combination of momentum basis elements,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
[x \mapsto \delta(x - \chi)] 
  &amp;= \int_\mathbb{R} \frac{1}{\sqrt{2\pi}}e^{-ip\chi} \cdot \vec{b}_p \diff{p} \\
  &amp;= \int_\mathbb{R} \frac{1}{\sqrt{2\pi}}e^{-ip\chi} \cdot \left[x \mapsto \frac{1}{\sqrt{2\pi}}e^{ipx}\right] \diff{p} \\
  &amp;= x \mapsto \int_\mathbb{R} \frac{1}{\sqrt{2\pi}}e^{-ip\chi} \frac{1}{\sqrt{2\pi}}e^{ipx} \diff{p}
\end{align}\,. %]]&gt;&lt;/script&gt;

&lt;p&gt;In the momentum basis, a position basis element becomes a sine wave, i.e. the coefficient of the $p$-th momentum basis element is $\frac{1}{\sqrt{2\pi}}e^{-ip\chi}$.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Heisenberg uncertainty principle&lt;/em&gt;, informally, states that a quantum state which is more localized in space (peakier distribution in the position basis) is necessarily less localized in momentum space (spread out distribution in the momentum basis), and vice versa. This uncertainty principle is not an extra assertion on top of quantum mechanics, but is a &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;consequence of the Fourier transform&lt;/span&gt;&lt;label for=&quot;a49269f8c12334e2ed9b8c18b484bc3255faf867&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;a49269f8c12334e2ed9b8c18b484bc3255faf867&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;The same uncertainty principle is a problem when converting audio recordings to sheet music, as the frequency resolution of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectrogram&quot;&gt;spectrogram&lt;/a&gt; is inversely proportional to the time resolution. In other words, you cannot know the exact frequency of a fast changing sound due to the same uncertainty principle. Here time plays the role of space. &lt;a href=&quot;https://lts2.epfl.ch/blog/gbr/2015/05/08/uncertainty-principle-in-quantum-physics-and-signal-processing/&quot;&gt;Further reading.&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. More generally, there is an uncertainty principle between any two measurable properties with &lt;span class=&quot;marginnote-outer&quot;&gt;&lt;span class=&quot;marginnote-ref&quot;&gt;different measurement bases&lt;/span&gt;&lt;label for=&quot;9acabeda9899d14b3b917f753be9cfa784d0a28c&quot; class=&quot;margin-toggle&quot;&gt; ⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;9acabeda9899d14b3b917f753be9cfa784d0a28c&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;span class=&quot;marginnote-inner&quot;&gt;Technically this is not true, as some properties can be &lt;a href=&quot;https://en.wikipedia.org/wiki/Complete_set_of_commuting_observables&quot;&gt;observed simultaneously&lt;/a&gt;. How this happens is out of the scope of this post, but essentially, when measurement readouts correspond to multiple basis elements the system will collapse to a superposition of those elements, i.e. it will be projected onto their span. A second measurement of a “compatible” property will further collapse the system to a single basis element.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;This definition of quantum state is very helpful for visualizing what your system looks like in physical space. The vector definition is unfortunately more abstract and can often obscure the connection to physical space. The vector definition has its own benefits. In my opinion, knowing how to go between both definitions is optimal for understanding.&lt;/p&gt;

&lt;p&gt;I didn’t touch on how quantization comes into play, i.e. discrete measurement outcomes. In short, discrete measurement bases result in discrete measurement readouts. For example, total energy will be discrete for particles in potential wells, and continuous for free particles. What determines whether the measurement basis will be continuous or discrete, and how to derive these bases all together, is a complicated matter that gets into &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_theory&quot;&gt;spectral theory of linear operators&lt;/a&gt;. Maybe a topic of a future post.&lt;/p&gt;

&lt;p&gt;It’s important to point out is that &lt;a href=&quot;https://en.wikipedia.org/wiki/Measurement_in_quantum_mechanics&quot;&gt;measurement in quantum mechanics&lt;/a&gt; is not explained, but &lt;a href=&quot;https://en.wikipedia.org/wiki/Wave_function_collapse#History_and_context&quot;&gt;taken as given&lt;/a&gt;. The non-determinism of measurement outcomes is part of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac%E2%80%93von_Neumann_axioms&quot;&gt;axioms of quantum mechanics&lt;/a&gt;, at least under the Copenhagen interpretation. There are alternative interpretations which deny wave-function collapse, notably &lt;a href=&quot;https://en.wikipedia.org/wiki/Many-worlds_interpretation&quot;&gt;pilot-wave&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Many-worlds_interpretation&quot;&gt;many-worlds&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Sun, 22 Dec 2019 00:00:00 -0800</pubDate>
        <link>pragmanym.github.io/zhat/articles/quantum-minimal-conceptual-unit</link>
        <guid isPermaLink="true">pragmanym.github.io/zhat/articles/quantum-minimal-conceptual-unit</guid>
        
        
        <category>post</category>
        
      </item>
    
  </channel>
</rss>
